{"cells":[{"cell_type":"markdown","metadata":{},"source":["# MobileNetV3 YOLOv3 on ICDAR 2015\n","\n","A text detection model with a MobileNetV3 backbone, trained according to the YOLOv3 paradigm on the (too small and horrible, yet great to showcase things with) ICDAR 2015 dataset.<br>\n","The model gets one-shot pruned and quantized for potential deployment on edge-devices.<br>"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:21.050641Z","iopub.status.busy":"2024-10-03T21:10:21.050117Z","iopub.status.idle":"2024-10-03T21:10:24.052955Z","shell.execute_reply":"2024-10-03T21:10:24.052036Z","shell.execute_reply.started":"2024-10-03T21:10:21.050583Z"},"trusted":true},"outputs":[],"source":["import gc\n","import csv\n","import math\n","import torch\n","import random\n","import numpy as np\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import torch_optimizer as optim\n","import matplotlib.patches as patches\n","import torch.nn.utils.prune as prune\n","import torchvision.transforms.functional as TF\n","\n","from PIL import Image\n","from pathlib import Path\n","from torchvision import transforms\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.lr_scheduler import OneCycleLR\n","from mobileyolov3 import MobileYOLOv3, DSConv, Resizer\n","\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.054910Z","iopub.status.busy":"2024-10-03T21:10:24.054406Z","iopub.status.idle":"2024-10-03T21:10:24.094925Z","shell.execute_reply":"2024-10-03T21:10:24.093802Z","shell.execute_reply.started":"2024-10-03T21:10:24.054872Z"},"trusted":true},"outputs":[],"source":["torch.manual_seed(42)\n","torch.cuda.manual_seed_all(42)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using training device: {device}\")\n","\n","batch_size = 128\n","num_workers = 4\n","num_classes = 1\n","learning_rate = 4e-4\n","num_epochs = 800\n","lr_warmup = num_epochs * 0.1\n","weight_decay = 1e-4\n","optim_k = 5\n","optim_alpha = 0.3\n","warmup_start = 0.35\n","warmup_epochs = lr_warmup\n","scheduler_t0 = warmup_epochs\n","scheduler_tmult = 2\n","prune_amount = 0.2\n","dropout_rate = 0.4\n","target_architecture = 'cuda'\n","anchors = [(0.28, 0.35), (0.43, 0.58), (0.62, 0.78)]\n","num_anchors = len(anchors)\n","\n","# https://www.kaggle.com/datasets/bestofbests9/icdar2015\n","dataset_path = Path(\"./icdar2015/\")\n","train_path = dataset_path / 'ch4_training_images'\n","train_labels = dataset_path / 'ch4_training_localization_transcription_gt'\n","test_path = dataset_path / 'ch4_test_images'\n","test_labels = dataset_path / 'ch4_test_localization_transcription_gt'\n","\n","model_path = 'mobileyolov3_icdar2015.pth'"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Activation Functions"]},{"cell_type":"markdown","metadata":{},"source":["We will need to incorporate the correct activation functions and place them throughout our model.<br>\n","If you expect a value to only ever be $[0;1]$, then using LeakyReLU might not be as computationally effective (projects into $[0;\\infty]$) as Sigmoid (projects into $[0;1]$).\n","\n","Plotting some activation function candidates helps in making sure that you really select the most fitting activation for your setting:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# This is interesting too: https://pat.chormai.org/blog/2020-relu-softplus (considered, but didn't end up using it)\n","# For Swish: https://arxiv.org/pdf/1710.05941.pdf (esp pp. 5-6) (considered, but didn't end up using it)\n","\n","x = np.linspace(-10, 10, 400)\n","y_relu = np.maximum(0, x)\n","y_leaky_relu_0_1 = np.where(x > 0, x, 0.1 * x)\n","y_leaky_relu_0_02 = np.where(x > 0, x, 0.02 * x)\n","y_sigmoid = 1 / (1 + np.exp(-x))\n","y_softplus_beta_0_5 = (1 / 0.5) * np.log(1 + np.exp(0.5 * x))\n","y_softplus_beta_1 = (1 / 1) * np.log(1 + np.exp(1 * x))\n","y_swish_derivative = (x * y_sigmoid) + y_sigmoid * (1 - (x * y_sigmoid))\n","y_elu = np.where(x > 0, x, np.exp(x) - 1)\n","\n","plt.figure(figsize=(8, 4))\n","plt.plot(x, y_relu, label=\"ReLU\", linewidth=2, color='black')\n","plt.plot(x, y_sigmoid, label=\"Sigmoid\", linewidth=2, color='red')\n","plt.plot(x, y_elu, label=\"ELU\", linewidth=2, linestyle='dashed', color='brown')\n","plt.plot(x, y_leaky_relu_0_1, label=\"Leaky ReLU (α=0.1)\", linewidth=2, linestyle='dotted', color='green')\n","plt.plot(x, y_leaky_relu_0_02, label=\"Leaky ReLU (α=0.02)\", linewidth=2, linestyle='dotted', color='gray')\n","plt.plot(x, y_softplus_beta_0_5, label=\"Softplus (β=0.5)\", linewidth=2, color='blue')\n","plt.plot(x, y_softplus_beta_1, label=\"Softplus (β=1)\", linewidth=2, linestyle='dashed', color='orange')\n","plt.plot(x, y_swish_derivative, label=\"Swish Derivative\", linewidth=2, color='purple')\n","\n","plt.title('Compared Activation Functions', fontsize=14)\n","plt.xlabel('x', fontsize=12)\n","plt.ylabel('activation(x)', fontsize=12)\n","plt.legend(loc='upper left')\n","plt.axhline(0, color='black', linewidth=0.5)\n","plt.axvline(0, color='black', linewidth=0.5)\n","plt.grid(True)\n","plt.xlim(-7, 7)\n","plt.ylim(-1, 3)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Pruning & Quantization Definition"]},{"cell_type":"markdown","metadata":{},"source":["Pruning and quantization will get applied after model training concluded.\n","\n","Pruning picks out tiny/non-contributing weights and removes them from the model structure all together.<br>\n","This frees up memory and processing resources at little to no cost in accuracy. Impact on accuracy varies per use-case though.<br>\n","Quantization is an additional step that, considering weights, doesn't look at the value itself, but the difference between required and actually granted numeric precision.<br>\n","If a weight can be represented little informational loss through a coarser numeric precision, this could decrease memory and computation demands."]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.110103Z","iopub.status.busy":"2024-10-03T21:10:24.109729Z","iopub.status.idle":"2024-10-03T21:10:24.125642Z","shell.execute_reply":"2024-10-03T21:10:24.124621Z","shell.execute_reply.started":"2024-10-03T21:10:24.110066Z"},"trusted":true},"outputs":[],"source":["def prune_model(model, amount):\n","    def prune_conv(conv, amount):\n","        prune.ln_structured(conv, name='weight', amount=amount, n=2, dim=0)\n","\n","    for _, module in model.named_modules():\n","        if isinstance(module, DSConv):\n","            prune_conv(module.depthwise, amount)\n","            prune_conv(module.pointwise, amount)\n","        elif isinstance(module, Resizer):\n","            if isinstance(module.conv, DSConv):\n","                prune_conv(module.conv.depthwise, amount)\n","                prune_conv(module.conv.pointwise, amount)\n","            else:\n","                prune_conv(module.conv, amount)\n","        elif isinstance(module, nn.Conv2d):\n","            prune_conv(module, amount)\n","\n","    parameters_to_prune = []\n","    for module in model.modules():\n","        if isinstance(module, nn.Conv2d):\n","            parameters_to_prune.append((module, 'weight'))\n","        elif isinstance(module, DSConv):\n","            parameters_to_prune.extend([(module.depthwise, 'weight'), (module.pointwise, 'weight')])\n","        elif isinstance(module, nn.Linear):\n","            parameters_to_prune.append((module, 'weight'))\n","    prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=amount)\n","    return model\n","\n","def quantize_model(model, device):\n","    \"\"\"\n","    Crunch numeric precision of weights and activations.\n","    \"\"\"\n","    model = model.cpu() # Works only on CPU\n","    quantized_model = torch.quantization.quantize_dynamic(model, {nn.Conv2d, nn.Linear}, dtype=torch.qint8, inplace=True) # inplace=True avoids deepcopy issues\n","    return quantized_model.to(device)\n","\n","def lift_pruning(model):\n","    for module in model.modules():\n","        if isinstance(module, (nn.Conv2d, nn.Linear)):\n","            if hasattr(module, 'weight_mask'):\n","                module.weight.data *= module.weight_mask\n","                prune.remove(module, 'weight')\n","            elif hasattr(module, 'weight_orig'):\n","                # If there's a weight_orig but no mask, it means pruning was applied but the mask was removed\n","                module.weight.data = module.weight_orig.data\n","                delattr(module, 'weight_orig')\n","        elif isinstance(module, DSConv):\n","            for conv in [module.depthwise, module.pointwise]:\n","                if hasattr(conv, 'weight_mask'):\n","                    conv.weight.data *= conv.weight_mask\n","                    prune.remove(conv, 'weight')\n","                elif hasattr(conv, 'weight_orig'):\n","                    conv.weight.data = conv.weight_orig.data\n","                    delattr(conv, 'weight_orig')\n","        elif isinstance(module, Resizer):\n","            if isinstance(module.conv, DSConv):\n","                for conv in [module.conv.depthwise, module.conv.pointwise]:\n","                    if hasattr(conv, 'weight_mask'):\n","                        conv.weight.data *= conv.weight_mask\n","                        prune.remove(conv, 'weight')\n","                    elif hasattr(conv, 'weight_orig'):\n","                        conv.weight.data = conv.weight_orig.data\n","                        delattr(conv, 'weight_orig')\n","            else:\n","                if hasattr(module.conv, 'weight_mask'):\n","                    module.conv.weight.data *= module.conv.weight_mask\n","                    prune.remove(module.conv, 'weight')\n","                elif hasattr(module.conv, 'weight_orig'):\n","                    module.conv.weight.data = module.conv.weight_orig.data\n","                    delattr(module.conv, 'weight_orig')\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Datasets"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.127518Z","iopub.status.busy":"2024-10-03T21:10:24.127141Z","iopub.status.idle":"2024-10-03T21:10:24.162452Z","shell.execute_reply":"2024-10-03T21:10:24.161267Z","shell.execute_reply.started":"2024-10-03T21:10:24.127481Z"},"trusted":true},"outputs":[],"source":["class ICDAR2015(Dataset):\n","    \"\"\"\n","    ICDAR2015 Dataset for YOLOv3 training.\n","    \"\"\"\n","    def __init__(self, input_path, label_path, num_classes=1, num_anchors=3, img_size=(224, 224), img_format='.jpg', anchors=anchors):\n","        self.input_path = Path(input_path)  # Path to images\n","        self.label_path = Path(label_path)  # Path to labels\n","        self.num_classes = num_classes      # Number of associable classes\n","        self.num_anchors = num_anchors      # Number of predictable distinct objects per grid tile\n","        self.img_size = img_size            # Image size\n","        self.batch_count = 0                # Batch counter\n","        self.anchors = anchors              # Anchor boxes (starter values)\n","        self.transform = transforms.Compose([\n","            transforms.Resize(img_size),\n","            transforms.ColorJitter(brightness=0.1, contrast=0.3, saturation=0.3, hue=0.1),\n","            transforms.RandomAdjustSharpness(sharpness_factor=3.0, p=0.9),\n","            transforms.ToTensor()\n","        ])\n","        self.files = self._assemble_files(img_format=img_format)\n","        self.labels = [self._parse_label(label, Image.open(img).size) for img, label in self.files]\n","\n","    def _assemble_files(self, img_format):\n","        image_files, data = list(self.input_path.glob(f'*{img_format}')), []\n","        for img_file in image_files:\n","            img_id = img_file.stem.split('_')[-1]\n","            label_file = self.label_path / f\"gt_img_{img_id}.txt\"\n","            if label_file.exists():\n","                data.append((img_file, label_file))\n","            else:\n","                print(f\"Warning: No matching label file found for {img_file.name}\")\n","        return data\n","\n","    def __len__(self):\n","        return len(self.files)\n","    \n","    def __getitem__(self, idx):\n","        img_path, _ = self.files[idx]\n","        img = Image.open(img_path)\n","        img = self.transform(img)\n","        label = self.labels[idx]\n","        return img, label\n","    \n","    def __iter__(self):\n","        self.index = 0\n","        return self\n","\n","    def __next__(self):\n","        if self.index >= len(self):\n","            raise StopIteration\n","        item = self[self.index]\n","        self.index += 1\n","        return item\n","\n","    def _calculate_iou(self, box1, box2):\n","        x1, y1 = max(box1[0], box2[0]), max(box1[1], box2[1])\n","        x2, y2 = min(box1[0] + box1[2], box2[0] + box2[2]), min(box1[1] + box1[3], box2[1] + box2[3])\n","        inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n","        box1_area, box2_area = box1[2] * box1[3], box2[2] * box2[3]\n","        union_area = box1_area + box2_area - inter_area\n","        return inter_area / union_area if union_area > 0 else 0\n","\n","    def _to_grid(self, grid, box):\n","        grid_h, grid_w = grid.size(0), grid.size(1)\n","        x, y, w, h, obj, _ = box\n","        grid_x, grid_y = int(x * grid_w), int(y * grid_h)\n","        x, y = x * grid_w - grid_x, y * grid_h - grid_y\n","        best_iou, best_anchor_idx = 0, -1\n","        for anchor_idx, (anchor_w, anchor_h) in enumerate(self.anchors):\n","            anchor_box = torch.tensor([x, y, w / anchor_w, h / anchor_h])\n","            iou = self._calculate_iou(anchor_box.numpy(), [x, y, w, h])\n","            if iou > best_iou:\n","                best_iou = iou\n","                best_anchor_idx = anchor_idx\n","        if best_anchor_idx >= 0:\n","            anchor_slice = slice(best_anchor_idx * (5 + self.num_classes), (best_anchor_idx + 1) * (5 + self.num_classes))\n","            grid[grid_y, grid_x, anchor_slice][:4] = torch.tensor([x, y, w / self.anchors[best_anchor_idx][0], h / self.anchors[best_anchor_idx][1]])\n","            grid[grid_y, grid_x, anchor_slice][4] = obj\n","\n","    def _parse_label(self, label_path, img_size):\n","        coarse_labels = torch.zeros((7, 7, self.num_anchors * (5 + self.num_classes)))\n","        medium_labels = torch.zeros((14, 14, self.num_anchors * (5 + self.num_classes)))\n","        fine_labels   = torch.zeros((28, 28, self.num_anchors * (5 + self.num_classes)))\n","\n","        with open(label_path, 'r', encoding='utf-8-sig') as file:\n","            reader = csv.reader(file, delimiter=',')\n","            for row in reader:\n","                row = torch.tensor([float(i) for i in row[:8]])\n","                x, y = (row[0::2].sum() / 4, row[1::2].sum() / 4)\n","                w = row[0::2].max() - row[0::2].min()\n","                h = row[1::2].max() - row[1::2].min()\n","                x, y = min(x, img_size[0] - 1e-3), min(y, img_size[1] - 1e-3)\n","                w, h = min(w, img_size[0] - 1e-3), min(h, img_size[1] - 1e-3)\n","                x, y = x / img_size[0], y / img_size[1]\n","                w, h = w / img_size[0], h / img_size[1]\n","                obj, cls = 1.0, 0.0\n","                box = torch.tensor([x, y, w, h, obj, cls])\n","                self._to_grid(coarse_labels, box)\n","                self._to_grid(medium_labels, box)\n","                self._to_grid(fine_labels, box)\n","        return torch.cat([coarse_labels.flatten(), medium_labels.flatten(), fine_labels.flatten()], dim=0)\n","\n","    def get_batch(self, batch_size, randomized=True):\n","        if randomized:\n","            indices = np.random.choice(len(self), batch_size, replace=False)\n","        else:\n","            indices = np.arange(self.batch_count, self.batch_count + batch_size) % len(self)\n","            self.batch_count += batch_size\n","        batch_images = torch.stack([self[i][0] for i in indices], dim=0)\n","        batch_labels = torch.stack([self[i][1] for i in indices], dim=0)\n","        return batch_images, batch_labels\n","\n","    @staticmethod\n","    def collate_fn(batch):\n","        images, labels = zip(*batch)\n","        images = torch.stack(images, dim=0)\n","        labels = torch.stack(labels, dim=0)\n","        return images, labels"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Loss"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["class YoLoss(nn.Module):\n","    \"\"\"\n","    YOLOv3 Custom Loss Function.\n","    \"\"\"\n","    def __init__(self, num_classes=1, num_anchors=3, lambda_coord=1.0, lambda_obj=3.0, lambda_noobj=0.1,\n","                 lambda_class=2.0, iou_threshold=0.5, focal_alpha=0.5, focal_gamma=1.0, label_smoothing=0.1, anchors=anchors):\n","        super(YoLoss, self).__init__()\n","        self.num_classes = num_classes\n","        self.num_anchors = num_anchors\n","        self.lambda_coord = lambda_coord\n","        self.lambda_obj = lambda_obj\n","        self.lambda_noobj = lambda_noobj\n","        self.lambda_class = lambda_class\n","        self.iou_threshold = iou_threshold\n","        self.focal_alpha = focal_alpha\n","        self.focal_gamma = focal_gamma\n","        self.label_smoothing = label_smoothing\n","        self.anchors = torch.tensor(anchors)\n","        self.eps = 1e-7\n","        self.mse_loss = nn.MSELoss(reduction='none')\n","        self.bce_loss = nn.BCEWithLogitsLoss(reduction='none')\n","    \n","    def focal_loss(self, pred, target):\n","        pred_prob = torch.sigmoid(pred)\n","        p_t = target * pred_prob + (1 - target) * (1 - pred_prob)\n","        alpha_factor = target * self.focal_alpha + (1 - target) * (1 - self.focal_alpha)\n","        modulating_factor = (1.0 - p_t).pow(self.focal_gamma)\n","        loss = self.bce_loss(pred, target)\n","        weight = torch.where(target == 1, torch.tensor(60.0).to(pred.device), torch.tensor(1.0).to(pred.device))\n","        return weight * (alpha_factor * modulating_factor * loss)\n","\n","    def bbox_iou(self, box1, box2, xywh=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-7):\n","        if xywh:\n","            (x1, y1, w1, h1), (x2, y2, w2, h2) = box1.chunk(4, -1), box2.chunk(4, -1)\n","            w1_, h1_, w2_, h2_ = w1 / 2, h1 / 2, w2 / 2, h2 / 2\n","            b1_x1, b1_x2, b1_y1, b1_y2 = x1 - w1_, x1 + w1_, y1 - h1_, y1 + h1_\n","            b2_x1, b2_x2, b2_y1, b2_y2 = x2 - w2_, x2 + w2_, y2 - h2_, y2 + h2_\n","        else:\n","            b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n","            b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n","            w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1\n","            w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1\n","        # Area of Bbox Intersection\n","        inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n","                (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n","        # Union Area\n","        union = (w1 * h1 + w2 * h2 - inter) + eps\n","        iou = inter / union\n","        if GIoU or DIoU or CIoU:\n","            cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)\n","            ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)\n","            if CIoU or DIoU:\n","                c2 = (cw ** 2 + ch ** 2) + eps\n","                rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4\n","                if DIoU:\n","                    return iou - rho2 / c2\n","                elif CIoU:\n","                    v = (4 / math.pi ** 2) * torch.pow(torch.atan(w2 / (h2 + eps)) - torch.atan(w1 / (h1 + eps)), 2)\n","                    with torch.no_grad():\n","                        alpha = v / (v - iou + (1 + eps))\n","                    return iou - (rho2 / c2 + v * alpha)\n","            else:\n","                c_area = cw * ch + eps\n","                return iou - (c_area - union) / c_area\n","        else:\n","            return iou\n","\n","    def get_box_loss(self, predictions, targets):\n","        box_loss = 0\n","        for pi, ti in zip(predictions, targets):\n","            mask = ti[..., 4] > 0\n","            p_boxes, t_boxes = pi[mask][..., :4], ti[mask][..., :4]\n","            if p_boxes.numel() > 0:\n","                iou = self.bbox_iou(p_boxes, t_boxes, CIoU=True)\n","                box_loss += torch.mean(1.0 - iou)\n","                box_loss += F.smooth_l1_loss(p_boxes, t_boxes, reduction='mean')\n","        return box_loss\n","\n","    def get_obj_loss(self, predictions, targets):\n","        obj_loss = 0.0\n","        noobj_loss = 0.0\n","        for pi, ti in zip(predictions, targets):\n","            pred_obj = pi[..., 4]\n","            target_obj = ti[..., 4]\n","            obj_loss += torch.mean(self.focal_loss(pred_obj, target_obj))\n","            noobj_mask = target_obj == 0\n","            if noobj_mask.any():\n","                noobj_loss += torch.mean(self.bce_loss(pred_obj[noobj_mask], target_obj[noobj_mask]))\n","        return self.lambda_obj * obj_loss + self.lambda_noobj * noobj_loss\n","\n","    def get_cls_loss(self, predictions, targets):\n","        cls_loss = 0\n","        if self.num_classes > 1:\n","            for pi, ti in zip(predictions, targets):\n","                pred_cls = pi[..., 5:]\n","                target_cls = ti[..., 5:]\n","                target_cls = (1 - self.label_smoothing) * target_cls + self.label_smoothing / self.num_classes\n","                cls_loss += torch.mean(self.focal_loss(pred_cls, target_cls))\n","        return cls_loss\n","\n","    def forward(self, predictions, targets):\n","        b_size = targets.size(0)\n","        coarse_size = 7 * 7 * self.num_anchors * (5 + self.num_classes)\n","        medium_size = 14 * 14 * self.num_anchors * (5 + self.num_classes)\n","        fine_size = 28 * 28 * self.num_anchors * (5 + self.num_classes)\n","\n","        flat_coarse, flat_medium, flat_fine = torch.split(targets, [coarse_size, medium_size, fine_size], dim=1)\n","        t_coarse = flat_coarse.view(b_size, 7, 7, self.num_anchors, (5 + self.num_classes))\n","        t_medium = flat_medium.view(b_size, 14, 14, self.num_anchors, (5 + self.num_classes))\n","        t_fine = flat_fine.view(b_size, 28, 28, self.num_anchors, (5 + self.num_classes))\n","\n","        targets_split = [t_coarse, t_medium, t_fine]\n","        \n","        box_loss = self.get_box_loss(predictions, targets_split)\n","        obj_loss = self.get_obj_loss(predictions, targets_split)\n","        cls_loss = self.get_cls_loss(predictions, targets_split)\n","                \n","        total_loss = self.lambda_coord * box_loss * obj_loss + self.lambda_class * cls_loss\n","\n","        if torch.isnan(total_loss):\n","            print(f'box_loss={box_loss}, obj_loss={obj_loss}, cls_loss={cls_loss}')\n","            total_loss = torch.where(torch.isnan(total_loss), torch.zeros_like(total_loss), total_loss)\n","\n","        return total_loss"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","### Data + Loss Sanity Check"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["train_dataset = ICDAR2015(train_path, train_labels, num_classes)\n","sanity_criterion = YoLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sanity_batch = 5\n","imgb, labelb = train_dataset.get_batch(sanity_batch)\n","\n","print('Image Batch:', imgb.shape, '\\tLabel:', labelb.shape)\n","print('Image:', imgb[0].shape, '\\tLabel:', labelb[0].shape)\n","\n","def get_loss(label_a, label_b, title='', grid_sizes=[(7, 7), (14, 14), (28, 28)]):\n","    coarse_size = grid_sizes[0][0] * grid_sizes[0][1] * num_anchors * (5 + num_classes)\n","    medium_size = grid_sizes[1][0] * grid_sizes[1][1] * num_anchors * (5 + num_classes)\n","    fine_size = grid_sizes[2][0] * grid_sizes[2][1] * num_anchors * (5 + num_classes)\n","    coarse_flat, medium_flat, fine_flat = torch.split(label_a, [coarse_size, medium_size, fine_size], dim=0)\n","    coarse = coarse_flat.view(grid_sizes[0][0], grid_sizes[0][1], num_anchors, (5 + num_classes))\n","    medium = medium_flat.view(grid_sizes[1][0], grid_sizes[1][1], num_anchors, (5 + num_classes))\n","    fine = fine_flat.view(grid_sizes[2][0], grid_sizes[2][1], num_anchors, (5 + num_classes))\n","    predictions = [coarse.unsqueeze(0), medium.unsqueeze(0), fine.unsqueeze(0)]\n","    print(title, sanity_criterion(predictions, label_b.unsqueeze(0)), '\\n', '-' * 50)\n","\n","def calculate_iou(box1, box2):\n","    x1 = max(box1[0], box2[0])\n","    y1 = max(box1[1], box2[1])\n","    x2 = min(box1[0] + box1[2], box2[0] + box2[2])\n","    y2 = min(box1[1] + box1[3], box2[1] + box2[3])\n","    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n","    box1_area, box2_area = box1[2] * box1[3], box2[2] * box2[3]\n","    union_area = box1_area + box2_area - inter_area\n","    return inter_area / union_area if union_area > 0 else 0\n","\n","def show_image_with_bboxes(img, label, num_anchors=3, num_classes=1, grid_sizes=[(7, 7), (14, 14), (28, 28)], anchors=anchors):\n","    img_np = TF.to_pil_image(img)\n","    _, ax = plt.subplots(1)\n","    ax.imshow(img_np)\n","    ax.set_xticks([])\n","    ax.set_yticks([])\n","    coarse_size = grid_sizes[0][0] * grid_sizes[0][1] * num_anchors * (5 + num_classes)\n","    medium_size = grid_sizes[1][0] * grid_sizes[1][1] * num_anchors * (5 + num_classes)\n","    fine_size = grid_sizes[2][0] * grid_sizes[2][1] * num_anchors * (5 + num_classes)\n","    coarse_flat, medium_flat, fine_flat = torch.split(label, [coarse_size, medium_size, fine_size], dim=0)\n","    coarse = coarse_flat.view(grid_sizes[0][0], grid_sizes[0][1], num_anchors, (5 + num_classes))\n","    medium = medium_flat.view(grid_sizes[1][0], grid_sizes[1][1], num_anchors, (5 + num_classes))\n","    fine = fine_flat.view(grid_sizes[2][0], grid_sizes[2][1], num_anchors, (5 + num_classes))\n","    def draw_boxes(grid, grid_w, grid_h, anchors):\n","        for y in range(grid_h):\n","            for x in range(grid_w):\n","                best_iou, best_anchor_idx = 0, -1\n","                for a in range(num_anchors):\n","                    box = grid[y, x, a]\n","                    if box[4] > 0:\n","                        anchor_w, anchor_h = anchors[a]\n","                        w = box[2].item() * anchor_w * img_np.width\n","                        h = box[3].item() * anchor_h * img_np.height\n","                        cx = (box[0].item() + x) / grid_w * img_np.width\n","                        cy = (box[1].item() + y) / grid_h * img_np.height\n","                        normalized_box = torch.tensor([cx - w / 2, cy - h / 2, w, h])\n","                        iou = calculate_iou(normalized_box.numpy(), [cx - w / 2, cy - h / 2, w, h])\n","                        if iou > best_iou:\n","                            best_iou = iou\n","                            best_anchor_idx = a\n","                if best_anchor_idx != -1:\n","                    best_box = grid[y, x, best_anchor_idx]\n","                    anchor_w, anchor_h = anchors[best_anchor_idx]\n","                    w_best = best_box[2].item() * anchor_w * img_np.width\n","                    h_best = best_box[3].item() * anchor_h * img_np.height\n","                    cx_best = (best_box[0].item() + x) / grid_w * img_np.width\n","                    cy_best = (best_box[1].item() + y) / grid_h * img_np.height\n","                    rect = patches.Rectangle((cx_best - w_best / 2, cy_best - h_best / 2), w_best, h_best,\n","                                             linewidth=1, edgecolor='g', facecolor='none')\n","                    ax.add_patch(rect)\n","    draw_boxes(coarse, grid_sizes[0][0], grid_sizes[0][1], anchors)\n","    draw_boxes(medium, grid_sizes[1][0], grid_sizes[1][1], anchors)\n","    draw_boxes(fine, grid_sizes[2][0], grid_sizes[2][1], anchors)\n","    plt.show()\n","\n","for i in range(imgb.shape[0]):\n","    show_image_with_bboxes(imgb[i], labelb[i])\n","    get_loss(labelb[i], labelb[i], title='Self Loss:')\n","    another_idx = random.choice([num for num in range(sanity_batch) if num != i])\n","    get_loss(labelb[i], labelb[another_idx], title='Random Comparison')"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Training"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.182684Z","iopub.status.busy":"2024-10-03T21:10:24.182334Z","iopub.status.idle":"2024-10-03T21:10:35.238599Z","shell.execute_reply":"2024-10-03T21:10:35.237680Z","shell.execute_reply.started":"2024-10-03T21:10:24.182648Z"},"trusted":true},"outputs":[],"source":["train_dataset = ICDAR2015(train_path, train_labels, num_classes)\n","train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n","                           num_workers=num_workers, collate_fn=ICDAR2015.collate_fn,\n","                           pin_memory=True)\n","\n","val_dataset = ICDAR2015(test_path, test_labels, num_classes)\n","val_loader  = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n","                         num_workers=num_workers, collate_fn=ICDAR2015.collate_fn,\n","                         pin_memory=True)"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://pytorch.org/tutorials/_images/pinmem.png\" alt=\"why pin_memory\" width=\"350\" height=\"auto\">"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def evaluate(model, criterion, data_loader, device):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():\n","        for images, targets in data_loader:\n","            images = images.to(device, non_blocking=True)\n","            targets = targets.to(device, non_blocking=True)\n","            with torch.amp.autocast(device_type=str(device)):\n","                outputs = model(images)\n","                loss = criterion(outputs, targets)\n","            total_loss += loss.item()\n","    return total_loss / len(data_loader)\n","\n","def mixup_data(x, y, alpha=0.1):\n","    if alpha > 0:\n","        lam = np.random.beta(alpha, alpha)\n","        batch_size = x.size(0)\n","        index = torch.randperm(batch_size).to(x.device)\n","        mixed_x = lam * x + (1 - lam) * x[index, :]\n","        y_a, y_b = y, y[index]\n","        return mixed_x, y_a, y_b, lam\n","    else:\n","        return x, y, y, 1"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:35.240698Z","iopub.status.busy":"2024-10-03T21:10:35.240174Z","iopub.status.idle":"2024-10-03T21:10:35.530099Z","shell.execute_reply":"2024-10-03T21:10:35.529042Z","shell.execute_reply.started":"2024-10-03T21:10:35.240641Z"},"trusted":true},"outputs":[],"source":["model = MobileYOLOv3(num_classes=num_classes, dropout_rate=dropout_rate, anchors=torch.tensor(anchors, dtype=torch.float32, device=device)).to(device)\n","criterion = YoLoss()\n","\n","# Adam with decoupled weight decay from gradient update\n","base_optimizer = torch.optim.AdamW([\n","    {'params': model.backbone.parameters(), 'lr': learning_rate * 0.8, 'weight_decay': weight_decay * 0.6},\n","    {'params': model.conv_7.parameters()},\n","    {'params': model.eca_7.parameters()},\n","    {'params': model.det1.parameters()},\n","    {'params': model.r_1024_128.parameters()},\n","    {'params': model.r_48_128.parameters()},\n","    {'params': model.conv_14.parameters()},\n","    {'params': model.eca_14.parameters()},\n","    {'params': model.det2.parameters()},\n","    {'params': model.r_512_64.parameters()},\n","    {'params': model.r_24_64.parameters()},\n","    {'params': model.conv_28.parameters()},\n","    {'params': model.eca_28.parameters()},\n","    {'params': model.det3.parameters()},\n","], lr=learning_rate, weight_decay=weight_decay)\n","\n","# Switching between providing 'fast weights' and 'slow weights' for AdamW optimizer update calculations\n","optimizer = optim.Lookahead(base_optimizer, k=optim_k, alpha=optim_alpha)\n","\n","# Gradually warm and then cool down LR over time\n","scheduler = OneCycleLR(optimizer, max_lr=learning_rate*2, epochs=num_epochs, steps_per_epoch=len(train_loader),\n","                       pct_start=0.3, anneal_strategy='cos', div_factor=10.0, final_div_factor=10000.0)\n","\n","# Helps avoiding numerical underflow/overflow through assigning mixed-precision by necessity rather than default\n","scaler = torch.amp.GradScaler(enabled=(str(device) != 'cpu'))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:35.532217Z","iopub.status.busy":"2024-10-03T21:10:35.531686Z","iopub.status.idle":"2024-10-03T21:22:36.998857Z","shell.execute_reply":"2024-10-03T21:22:36.997495Z","shell.execute_reply.started":"2024-10-03T21:10:35.532134Z"},"trusted":true},"outputs":[],"source":["lossi, losst = [], []\n","lowsi = float('inf')\n","\n","# Portraying the fine selection of hyperparameters\n","print(f'Batch Size:\\t {batch_size}\\n',\n","      f'Learning Rate:\\t {learning_rate}\\n',\n","      f'Weight Decay:\\t {weight_decay}\\n',\n","      f'Num Epochs:\\t {num_epochs}\\n',\n","      f'LR Warmup:\\t {lr_warmup}\\n',\n","      f'Optim K:\\t {optim_k}\\n',\n","      f'Optim Alpha:\\t {optim_alpha}\\n',\n","      f'Warmup Start:\\t {warmup_start}\\n',\n","      f'Scheduler T0:\\t {scheduler_t0}\\n',\n","      f'Scheduler Tmult:\\t {scheduler_tmult}\\n',\n","      f'Prune Amount:\\t {prune_amount}\\n',\n","      f'Target Architecture:\\t {target_architecture}\\n',\n","      f'Num Anchors:\\t {num_anchors}\\n',\n","      f'Dropout Rate:\\t {dropout_rate}\\n\\n')\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_loss = 0\n","    \n","    for files, targets in train_loader:\n","        files = files.to(device, non_blocking=True)\n","        targets = targets.to(device, non_blocking=True)\n","        mixed_files, targets_a, targets_b, lam = mixup_data(files, targets)\n","        optimizer.zero_grad()\n","        \n","        with torch.amp.autocast(device_type=str(device)):\n","            logits = model(mixed_files)\n","            loss_a = criterion(logits, targets_a)\n","            loss_b = criterion(logits, targets_b)\n","            loss = lam * loss_a + (1 - lam) * loss_b\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","        epoch_loss += loss.item()\n","        \n","    epoch_loss /= len(train_loader)\n","    lossi.append(epoch_loss)\n","    t_loss = evaluate(model, criterion, val_loader, device)\n","    losst.append(t_loss)\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    print(f'Epoch [{epoch+1:3}/{num_epochs}] | Train: {epoch_loss:8.6f} | Test: {t_loss:8.6f} | LR: {optimizer.param_groups[-1][\"lr\"]:.6f}')"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:22:37.001075Z","iopub.status.busy":"2024-10-03T21:22:37.000644Z","iopub.status.idle":"2024-10-03T21:22:37.315232Z","shell.execute_reply":"2024-10-03T21:22:37.314338Z","shell.execute_reply.started":"2024-10-03T21:22:37.001034Z"},"trusted":true},"outputs":[],"source":["# Save unaltered model\n","torch.save(model.state_dict(), f'solid_{model_path}')\n","\n","# Prune, Quantize\n","pruned_model = prune_model(model, amount=prune_amount)\n","lifted_model = lift_pruning(pruned_model)\n","quantized_model = quantize_model(lifted_model, device)\n","\n","# Save the quantized model\n","torch.save(quantized_model.state_dict(), model_path)"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Loss Graph"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:22:37.316866Z","iopub.status.busy":"2024-10-03T21:22:37.316534Z","iopub.status.idle":"2024-10-03T21:22:37.695052Z","shell.execute_reply":"2024-10-03T21:22:37.693981Z","shell.execute_reply.started":"2024-10-03T21:22:37.316831Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(range(num_epochs), lossi, label=\"Training Loss\", color='blue', marker='o', linestyle='-', markersize=3)\n","plt.plot(range(num_epochs), losst, label=\"Test Loss\", color=\"red\", marker='o', linestyle='-', markersize=3)\n","\n","plt.title('Loss Curves', fontsize=16)\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.grid(True)\n","plt.xticks(range(0, num_epochs, 50))\n","plt.legend(loc='upper right')\n","plt.show();"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Evaluate"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:30:41.780508Z","iopub.status.busy":"2024-10-03T21:30:41.779478Z","iopub.status.idle":"2024-10-03T21:30:41.790096Z","shell.execute_reply":"2024-10-03T21:30:41.788973Z","shell.execute_reply.started":"2024-10-03T21:30:41.780462Z"},"trusted":true},"outputs":[],"source":["def load_model(model_class, num_classes, model_path, device='cpu'):\n","    \"\"\"\n","    Load a PyTorch model for inference on the target device, regardless of where it was originally trained.\n","    \"\"\"\n","    if isinstance(device, str):\n","        device = torch.device(device)\n","\n","    # Load to CPU first\n","    state_dict = torch.load(model_path, map_location=device, weights_only=False)\n","    \n","    if isinstance(state_dict, dict) and 'model_state_dict' in state_dict:\n","        state_dict = state_dict['model_state_dict']\n","\n","    # Remove 'module.' prefix caused by SWA\n","    state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n","\n","    # Remove pruning-related keys\n","    new_state_dict = {}\n","    for key, value in state_dict.items():\n","        if 'weight_mask' not in key:\n","            new_key = key.replace('weight_orig', 'weight')\n","            new_state_dict[new_key] = value\n","    \n","    model = model_class(num_classes=num_classes, dropout_rate=dropout_rate, anchors=torch.tensor(anchors, dtype=torch.float32, device=device)).to(device)\n","    model.load_state_dict(new_state_dict, strict=False)\n","    model.eval()\n","    return model"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["model = load_model(MobileYOLOv3, num_classes, model_path, device='cuda')"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def visualize_inference(model, dataset, num_anchors=3, num_classes=1, grid_sizes=[(7, 7), (14, 14), (28, 28)], anchors=anchors):\n","    img, _ = dataset[random.randint(0, len(dataset) - 1)]\n","    prediction = model(img.unsqueeze(0).to(device))\n","    prediction = [p.squeeze(0) for p in prediction]\n","    _, ax = plt.subplots(1)\n","    img_np = TF.to_pil_image(img)\n","    ax.imshow(img_np)    \n","    ax.set_xticks([])\n","    ax.set_yticks([])\n","\n","    def draw_boxes(grid, grid_w, grid_h, anchors):\n","        for y in range(grid_h):\n","            for x in range(grid_w):\n","                best_iou, best_anchor_idx = 0, -1\n","                for a in range(num_anchors):\n","                    box = grid[y, x, a]\n","                    if box[4] > 0.1:\n","                        anchor_w, anchor_h = anchors[a]\n","                        w = box[2].item() * anchor_w * img_np.width  # Width\n","                        h = box[3].item() * anchor_h * img_np.height # Height\n","                        cx = (box[0].item() + x) / grid_w * img_np.width   # Center x\n","                        cy = (box[1].item() + y) / grid_h * img_np.height  # Center y\n","                        # Create a normalized bounding box for IoU calculation\n","                        normalized_box = torch.tensor([cx - w / 2, cy - h / 2, w, h])\n","                        # Calculate IoU with the current anchor\n","                        iou = calculate_iou(normalized_box.numpy(), [cx - w / 2, cy - h / 2, w, h])\n","                        # Update best IoU and corresponding anchor index\n","                        if iou > best_iou:\n","                            best_iou = iou\n","                            best_anchor_idx = a\n","                if best_anchor_idx != -1:\n","                    # Draw the bounding box with the best anchor\n","                    best_box = grid[y, x, best_anchor_idx]\n","                    anchor_w, anchor_h = anchors[best_anchor_idx]\n","                    w_best = best_box[2].item() * anchor_w * img_np.width  # Width using best anchor\n","                    h_best = best_box[3].item() * anchor_h * img_np.height # Height using best anchor\n","                    cx_best = (best_box[0].item() + x) / grid_w * img_np.width   # Center x using best anchor\n","                    cy_best = (best_box[1].item() + y) / grid_h * img_np.height  # Center y using best anchor\n","                    rect = patches.Rectangle((cx_best - w_best / 2, cy_best - h_best / 2), w_best, h_best,\n","                                             linewidth=1, edgecolor='g', facecolor='none')\n","                    ax.add_patch(rect)\n","\n","    draw_boxes(prediction[0], grid_sizes[0][0], grid_sizes[0][1], anchors)\n","    draw_boxes(prediction[1], grid_sizes[1][0], grid_sizes[1][1], anchors)\n","    draw_boxes(prediction[2], grid_sizes[2][0], grid_sizes[2][1], anchors)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["visualize_inference(model, train_dataset)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1150310,"sourceId":1928836,"sourceType":"datasetVersion"}],"dockerImageVersionId":30775,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"ai","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
