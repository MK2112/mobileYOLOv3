{"cells":[{"cell_type":"markdown","metadata":{},"source":["# MobileNetV3 YOLOv3 on ICDAR 2015\n","\n","A text detection model based on MobileNetV3 and YOLOv3.<br>\n","Pruned and quantized for deployment on edge devices."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:21.050641Z","iopub.status.busy":"2024-10-03T21:10:21.050117Z","iopub.status.idle":"2024-10-03T21:10:24.052955Z","shell.execute_reply":"2024-10-03T21:10:24.052036Z","shell.execute_reply.started":"2024-10-03T21:10:21.050583Z"},"trusted":true},"outputs":[],"source":["import gc\n","import csv\n","import math\n","import torch\n","import random\n","import numpy as np\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import torch_optimizer as optim\n","import matplotlib.patches as patches\n","import torch.nn.utils.prune as prune\n","import torchvision.transforms.functional as TF\n","\n","from PIL import Image, ImageDraw\n","from pathlib import Path\n","from torchvision import transforms\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.lr_scheduler import OneCycleLR\n","from mobileyolov3 import MobileYOLOv3, Conv, DSConv, Resizer\n","\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.054910Z","iopub.status.busy":"2024-10-03T21:10:24.054406Z","iopub.status.idle":"2024-10-03T21:10:24.094925Z","shell.execute_reply":"2024-10-03T21:10:24.093802Z","shell.execute_reply.started":"2024-10-03T21:10:24.054872Z"},"trusted":true},"outputs":[],"source":["torch.manual_seed(42)\n","torch.cuda.manual_seed_all(42)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using training device: {device}\")\n","\n","batch_size = 64\n","num_workers = 4\n","num_classes = 1\n","learning_rate = 1e-5\n","num_epochs = 10\n","lr_warmup = num_epochs * 0.2\n","weight_decay = 1e-6\n","optim_k = 5\n","optim_alpha = 0.3\n","warmup_start = 0.35\n","warmup_epochs = 10\n","scheduler_t0 = warmup_epochs\n","scheduler_tmult = 2\n","prune_amount = 0.4\n","target_architecture = 'cuda'\n","mixup_alpha = 0.2\n","num_anchors = 3\n","\n","# https://www.kaggle.com/datasets/bestofbests9/icdar2015\n","dataset_path = Path(\"./icdar2015/\")\n","train_path = dataset_path / 'ch4_training_images'\n","train_labels = dataset_path / 'ch4_training_localization_transcription_gt'\n","test_path = dataset_path / 'ch4_test_images'\n","test_labels = dataset_path / 'ch4_test_localization_transcription_gt'\n","\n","model_path = 'mobileyolov3_icdar2015.pth'"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Activation Functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# This is interesting too: https://pat.chormai.org/blog/2020-relu-softplus\n","\n","x = np.linspace(-10, 10, 400)\n","y_relu = np.maximum(0, x)\n","y_leaky_relu = np.where(x > 0, x, 0.1 * x)\n","y_sigmoid = 1 / (1 + np.exp(-x))\n","y_softplus_beta_0_5 = (1 / 0.5) * np.log(1 + np.exp(0.5 * x))\n","y_softplus_beta_1 = (1 / 1) * np.log(1 + np.exp(1 * x))\n","y_swish_derivative = (x * y_sigmoid) + y_sigmoid * (1 - (x * y_sigmoid))\n","y_elu = np.where(x > 0, x, np.exp(x) - 1)\n","\n","plt.figure(figsize=(8, 4))\n","plt.plot(x, y_relu, label=\"ReLU\", linewidth=2, color='black')\n","plt.plot(x, y_sigmoid, label=\"Sigmoid\", linewidth=2, color='red')\n","plt.plot(x, y_elu, label=\"ELU\", linewidth=2, color='brown')\n","plt.plot(x, y_leaky_relu, label=\"Leaky ReLU (α=0.1)\", linewidth=2, linestyle='dashed', color='green')\n","plt.plot(x, y_softplus_beta_0_5, label=\"Softplus (β=0.5)\", linewidth=2, color='blue')\n","plt.plot(x, y_softplus_beta_1, label=\"Softplus (β=1)\", linewidth=2, linestyle='dashed', color='orange')\n","plt.plot(x, y_swish_derivative, label=\"Swish Derivative\", linewidth=2, color='purple')\n","\n","plt.title('Activation Functions Comparison', fontsize=14)\n","plt.xlabel('x', fontsize=12)\n","plt.ylabel('activation(x)', fontsize=12)\n","plt.legend(loc='upper left')\n","plt.axhline(0, color='black', linewidth=0.5)\n","plt.axvline(0, color='black', linewidth=0.5)\n","plt.grid(True)\n","plt.xlim(-7, 7)\n","plt.ylim(-1, 3)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Pruning & Quantization Definition"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.110103Z","iopub.status.busy":"2024-10-03T21:10:24.109729Z","iopub.status.idle":"2024-10-03T21:10:24.125642Z","shell.execute_reply":"2024-10-03T21:10:24.124621Z","shell.execute_reply.started":"2024-10-03T21:10:24.110066Z"},"trusted":true},"outputs":[],"source":["def prune_model(model, amount):\n","    def prune_conv(conv, amount):\n","        prune.ln_structured(conv, name='weight', amount=amount, n=2, dim=0)\n","\n","    for _, module in model.named_modules():\n","        if isinstance(module, DSConv):\n","            prune_conv(module.depthwise, amount)\n","            prune_conv(module.pointwise, amount)\n","        elif isinstance(module, Conv):\n","            prune_conv(module.conv, amount)\n","        elif isinstance(module, Resizer):\n","            if isinstance(module.conv, DSConv):\n","                prune_conv(module.conv.depthwise, amount)\n","                prune_conv(module.conv.pointwise, amount)\n","            else:\n","                prune_conv(module.conv, amount)\n","        elif isinstance(module, nn.Conv2d):\n","            prune_conv(module, amount)\n","\n","    # Global pruning\n","    parameters_to_prune = []\n","    for module in model.modules():\n","        if isinstance(module, nn.Conv2d):\n","            parameters_to_prune.append((module, 'weight'))\n","        elif isinstance(module, DSConv):\n","            parameters_to_prune.extend([(module.depthwise, 'weight'), (module.pointwise, 'weight')])\n","        elif isinstance(module, nn.Linear):\n","            parameters_to_prune.append((module, 'weight'))\n","    prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=amount)\n","    return model\n","\n","def quantize_model(model, device):\n","    # Crunch numeric precision of weights and activations.\n","    model = model.cpu() # Only works when done on CPU\n","    quantized_model = torch.quantization.quantize_dynamic(model, {nn.Conv2d, nn.Linear}, dtype=torch.qint8, inplace=True) # inplace=True avoids deepcopy issues\n","    return quantized_model.to(device)\n","\n","def lift_pruning(model):\n","    for module in model.modules():\n","        if isinstance(module, (nn.Conv2d, nn.Linear)):\n","            if hasattr(module, 'weight_mask'):\n","                module.weight.data *= module.weight_mask\n","                prune.remove(module, 'weight')\n","            elif hasattr(module, 'weight_orig'):\n","                # If there's a weight_orig but no mask, it means pruning was applied but the mask was removed\n","                module.weight.data = module.weight_orig.data\n","                delattr(module, 'weight_orig')\n","        elif isinstance(module, DSConv):\n","            for conv in [module.depthwise, module.pointwise]:\n","                if hasattr(conv, 'weight_mask'):\n","                    conv.weight.data *= conv.weight_mask\n","                    prune.remove(conv, 'weight')\n","                elif hasattr(conv, 'weight_orig'):\n","                    conv.weight.data = conv.weight_orig.data\n","                    delattr(conv, 'weight_orig')\n","        elif isinstance(module, Conv):\n","            if hasattr(module.conv, 'weight_mask'):\n","                module.conv.weight.data *= module.conv.weight_mask\n","                prune.remove(module.conv, 'weight')\n","            elif hasattr(module.conv, 'weight_orig'):\n","                module.conv.weight.data = module.conv.weight_orig.data\n","                delattr(module.conv, 'weight_orig')\n","        elif isinstance(module, Resizer):\n","            if isinstance(module.conv, DSConv):\n","                for conv in [module.conv.depthwise, module.conv.pointwise]:\n","                    if hasattr(conv, 'weight_mask'):\n","                        conv.weight.data *= conv.weight_mask\n","                        prune.remove(conv, 'weight')\n","                    elif hasattr(conv, 'weight_orig'):\n","                        conv.weight.data = conv.weight_orig.data\n","                        delattr(conv, 'weight_orig')\n","            else:\n","                if hasattr(module.conv, 'weight_mask'):\n","                    module.conv.weight.data *= module.conv.weight_mask\n","                    prune.remove(module.conv, 'weight')\n","                elif hasattr(module.conv, 'weight_orig'):\n","                    module.conv.weight.data = module.conv.weight_orig.data\n","                    delattr(module.conv, 'weight_orig')\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Datasets"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.127518Z","iopub.status.busy":"2024-10-03T21:10:24.127141Z","iopub.status.idle":"2024-10-03T21:10:24.162452Z","shell.execute_reply":"2024-10-03T21:10:24.161267Z","shell.execute_reply.started":"2024-10-03T21:10:24.127481Z"},"trusted":true},"outputs":[],"source":["class ICDAR2015(Dataset):\n","    \"\"\"\n","    ICDAR2015 Dataset for YOLOv3 training.\n","    \"\"\"\n","    def __init__(self, input_path, label_path, num_classes=1, num_anchors=num_anchors, img_size=(224, 224), img_format='.jpg', augment=True):\n","        self.input_path = Path(input_path)\n","        self.label_path = Path(label_path)\n","        self.num_classes = num_classes\n","        self.num_anchors = num_anchors\n","        self.img_size = img_size\n","        self.augment = augment\n","        self.img_format = img_format\n","        self.batch_count = 0\n","        self.transform = transforms.Compose([\n","            transforms.Resize(img_size),\n","            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n","            transforms.ToTensor()\n","        ])\n","        self.anchors = torch.tensor([\n","            [[1.25, 1.625], [2.0, 3.75], [4.125, 2.875]],           # of coarse (7x7 grid)\n","            [[1.875, 3.8125], [3.875, 2.8125], [3.6875, 7.4375]],   # of medium (14x14 grid)\n","            [[3.625, 2.8125], [4.875, 6.1875], [11.65625, 10.1875]] # of fine (28x28 grid)\n","        ])\n","        self.anchor_sets = {\n","            (7, 7): self.anchors[0],\n","            (14, 14): self.anchors[1],\n","            (28, 28): self.anchors[2]\n","        }\n","        self.eps = 1e-7\n","        self.files = self._assemble_files(img_format=self.img_format)\n","        self.labels = [self._parse_label(label_path, Image.open(img).size) for img, label_path in self.files]\n","\n","    def _assemble_files(self, img_format):\n","        image_files, data = list(self.input_path.glob(f'*{img_format}')), []\n","        for img_file in image_files:\n","            img_id = img_file.stem.split('_')[-1]\n","            label_file = self.label_path / f\"gt_img_{img_id}.txt\"\n","            if label_file.exists():\n","                data.append((img_file, label_file))\n","            else:\n","                print(f\"Warning: No matching label file found for {img_file.name}\")\n","        return data\n","\n","    def __len__(self):\n","        return len(self.files)\n","    \n","    def __getitem__(self, idx):\n","        img_path, _ = self.files[idx]\n","        img = Image.open(img_path).convert(\"RGB\")\n","        label = self.labels[idx]\n","        if self.augment:\n","            img, label = self._apply_augmentation(img, label)\n","        img = self.transform(img)\n","        return img, label\n","    \n","    def _apply_augmentation(self, img, label):\n","        # Random horizontal flip\n","        if random.random() > 0.5:\n","            img = TF.hflip(img)\n","            label = self._flip_boxes(label)\n","        # Random rotation (up to 15 degrees)\n","        if random.random() > 0.5:\n","            angle = random.uniform(-15, 15)\n","            img = TF.rotate(img, angle)\n","            label = self._rotate_boxes(label, angle)\n","        return img, label\n","    \n","    def _flip_boxes(self, label):\n","        # Flip bounding boxes horizontally\n","        flipped_label = label.clone()\n","        for grid_size in [(7, 7), (14, 14), (28, 28)]:\n","            grid_h, grid_w = grid_size\n","            for y in range(grid_h):\n","                for x in range(grid_w):\n","                    for a in range(self.num_anchors):\n","                        box_index = (y * grid_w + x) * self.num_anchors + a\n","                        box = flipped_label[box_index * (5 + self.num_classes): (box_index + 1) * (5 + self.num_classes)]\n","                        if box[4] > 0:  # If there's an object\n","                            box[0] = 1 - box[0]  # Flip x-coordinate\n","        return flipped_label\n","\n","    def __iter__(self):\n","        self.index = 0\n","        return self\n","\n","    def __next__(self):\n","        if self.index >= len(self):\n","            raise StopIteration\n","        item = self[self.index]\n","        self.index += 1\n","        return item\n","    \n","    def _rotate_boxes(self, label, angle):\n","        # Rotate bounding boxes\n","        rotated_label = label.clone()\n","        angle_rad = math.radians(angle)\n","        cos_angle, sin_angle = math.cos(angle_rad), math.sin(angle_rad)\n","        for grid_size in [(7, 7), (14, 14), (28, 28)]:\n","            grid_h, grid_w = grid_size\n","            for y in range(grid_h):\n","                for x in range(grid_w):\n","                    for a in range(self.num_anchors):\n","                        box_index = (y * grid_w + x) * self.num_anchors + a\n","                        box = rotated_label[box_index * (5 + self.num_classes): (box_index + 1) * (5 + self.num_classes)]\n","                        if box[4] > 0:  # If there's an object\n","                            cx, cy = box[0], box[1]\n","                            new_cx = cx * cos_angle - cy * sin_angle + 0.5 * (1 - cos_angle + sin_angle)\n","                            new_cy = cx * sin_angle + cy * cos_angle + 0.5 * (1 - sin_angle - cos_angle)\n","                            box[0], box[1] = new_cx, new_cy\n","        return rotated_label\n","    \n","    def _fill_anchor(self, anchors, x, y, w, h, obj, cls, grid_size):\n","        # Calculate the center of the box relative to the grid size\n","        anchor_x = min(int(x * grid_size[0]), grid_size[0] - 1)\n","        anchor_y = min(int(y * grid_size[1]), grid_size[1] - 1)\n","        gt_box = torch.tensor([x, y, w, h])\n","        \n","        try:\n","            anchor_set = self.anchor_sets[grid_size]\n","        except KeyError:\n","            raise ValueError(\"Invalid grid size?!\")\n","        \n","        # Vectorize IOU calculation\n","        anchor_boxes = torch.tensor([[0, 0, anchor_w, anchor_h] for anchor_w, anchor_h in anchor_set])\n","        ious = self.bbox_iou(gt_box.unsqueeze(0), anchor_boxes)\n","        best_iou, best_anchor_idx = torch.max(ious, dim=0)\n","        obj = best_iou.item() if best_iou > 0.5 else 0\n","        anchors[anchor_x, anchor_y, best_anchor_idx] = torch.tensor([x, y, w, h, obj, cls])\n","        return anchors\n","\n","    def bbox_iou(self, box1, box2, DIoU=False, CIoU=True):\n","        b1_x1, b1_x2 = box1[..., 0] - box1[..., 2] / 2.0, box1[..., 0] + box1[..., 2] / 2.0\n","        b1_y1, b1_y2 = box1[..., 1] - box1[..., 3] / 2.0, box1[..., 1] + box1[..., 3] / 2.0\n","        b2_x1, b2_x2 = box2[..., 0] - box2[..., 2] / 2.0, box2[..., 0] + box2[..., 2] / 2.0\n","        b2_y1, b2_y2 = box2[..., 1] - box2[..., 3] / 2.0, box2[..., 1] + box2[..., 3] / 2.0\n","\n","        if torch.allclose(box1.float(), box2.float(), atol=self.eps):\n","            return torch.ones_like(box1[..., 0])\n","\n","        inter = torch.clamp((torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)), min=0) * \\\n","                torch.clamp((torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)), min=0)\n","\n","        w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1\n","        w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1\n","        union = (w1 * h1 + w2 * h2 - inter) + self.eps\n","        iou = inter / union\n","\n","        if CIoU or DIoU:\n","            cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)\n","            ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)\n","            c2 = (cw ** 2 + ch ** 2) + self.eps\n","            rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4\n","            if DIoU:\n","                return iou - rho2 / c2\n","            elif CIoU:\n","                v = (4 / (math.pi ** 2)) * torch.pow(torch.atan(w2 / (h2 + self.eps)) - torch.atan(w1 / (h1 + self.eps)), 2)\n","                alpha = v / (1 - iou + v + self.eps)\n","                return iou - (rho2 / c2 + v * alpha)\n","        return iou\n","\n","    def _parse_label(self, label_path, img_size):\n","        # Open label file and parse bounding boxes\n","        # target: tensor([(7, 7, num_anchors, [x, y, w, h, obj, num_classes]) # coarse\n","        #          * (14, 14, num_anchors, [x, y, w, h, obj, num_classes])    # medium\n","        #          * (28, 28, num_anchors, [x, y, w, h, obj, num_classes])])  # fine\n","        with open(label_path, 'r', encoding='utf-8-sig') as f:\n","            reader = csv.reader(f, delimiter=',')\n","            coarse = torch.zeros(7, 7, self.num_anchors, (5 + self.num_classes))\n","            medium = torch.zeros(14, 14, self.num_anchors, (5 + self.num_classes))\n","            fine = torch.zeros(28, 28, self.num_anchors, (5 + self.num_classes))\n","            for row in reader:\n","                x1, y1, x2, y2, x3, y3, x4, y4 = map(int, row[:8])\n","                x1, x2, x3, x4 = x1 / img_size[0], x2 / img_size[0], x3 / img_size[0], x4 / img_size[0]\n","                y1, y2, y3, y4 = y1 / img_size[1], y2 / img_size[1], y3 / img_size[1], y4 / img_size[1]\n","                x, y = (x1 + x2 + x3 + x4) / 4, (y1 + y2 + y3 + y4) / 4\n","                w, h = max(x1, x2, x3, x4) - min(x1, x2, x3, x4), max(y1, y2, y3, y4) - min(y1, y2, y3, y4)\n","                obj, cls = 1.0, 0.0\n","                coarse = self._fill_anchor(coarse, x, y, w, h, obj, cls, (7, 7))\n","                medium = self._fill_anchor(medium, x, y, w, h, obj, cls, (14, 14))\n","                fine = self._fill_anchor(fine, x, y, w, h, obj, cls, (28, 28))\n","        return torch.cat([coarse.flatten(), medium.flatten(), fine.flatten()], dim=0)\n","\n","    def get_batch(self, batch_size, randomized=True):\n","        if randomized:\n","            indices = np.random.choice(len(self), batch_size, replace=False)\n","        else:\n","            indices = np.arange(self.batch_count, self.batch_count + batch_size) % len(self)\n","            self.batch_count += batch_size\n","        batch_images = torch.stack([self[i][0] for i in indices], dim=0)  # Images\n","        batch_labels = torch.stack([self[i][1] for i in indices], dim=0)  # Labels\n","        return batch_images, batch_labels\n","\n","    @staticmethod\n","    def collate_fn(batch):\n","        images, labels = zip(*batch)\n","        images = torch.stack(images, dim=0)\n","        labels = torch.stack(labels, dim=0)\n","        return images, labels"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Loss"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.164779Z","iopub.status.busy":"2024-10-03T21:10:24.164286Z","iopub.status.idle":"2024-10-03T21:10:24.181085Z","shell.execute_reply":"2024-10-03T21:10:24.180180Z","shell.execute_reply.started":"2024-10-03T21:10:24.164720Z"},"trusted":true},"outputs":[],"source":["class YoLoss(nn.Module):\n","    def __init__(self, num_classes=1, num_anchors=3, lambda_coord=1.0, lambda_noobj=2.0,\n","                 lambda_class=1.0, iou_threshold=0.5, focal_alpha=0.25, focal_gamma=2.0, label_smoothing=0.1):\n","        super(YoLoss, self).__init__()\n","        self.num_classes = num_classes\n","        self.num_anchors = num_anchors\n","        self.lambda_coord = lambda_coord\n","        self.lambda_noobj = lambda_noobj\n","        self.lambda_class = lambda_class\n","        self.iou_threshold = iou_threshold\n","        self.focal_alpha = focal_alpha\n","        self.focal_gamma = focal_gamma\n","        self.label_smoothing = label_smoothing\n","        self.anchors = torch.tensor([\n","            [[1.25, 1.625], [2.0, 3.75], [4.125, 2.875]],           # of coarse (7x7 grid)\n","            [[1.875, 3.8125], [3.875, 2.8125], [3.6875, 7.4375]],   # of medium (14x14 grid)\n","            [[3.625, 2.8125], [4.875, 6.1875], [11.65625, 10.1875]] # of fine (28x28 grid)\n","        ])\n","        self.eps = 1e-7\n","    \n","    def gaussian_objectness(self, x, y, sigma=0.3):\n","        return torch.exp(-((x ** 2 + y ** 2) / (2 * sigma ** 2)))\n","\n","    def focal_loss(self, pred, target):\n","        if torch.allclose(pred.float(), target.float(), atol=self.eps):\n","            return torch.zeros_like(pred)\n","        pred_prob = torch.sigmoid(pred)\n","        p_t = target * pred_prob + (1 - target) * (1 - pred_prob)\n","        alpha_factor = target * self.focal_alpha + (1 - target) * (1 - self.focal_alpha)\n","        modulating_factor = (1.0 - p_t).pow(self.focal_gamma)\n","        loss = F.binary_cross_entropy_with_logits(pred, target, reduction='none')\n","        return alpha_factor * modulating_factor * loss\n","\n","    def bbox_iou(self, box1, box2, DIoU=False, CIoU=False):\n","        b1_x1, b1_x2 = box1[..., 0] - box1[..., 2] / 2.0, box1[..., 0] + box1[..., 2] / 2.0\n","        b1_y1, b1_y2 = box1[..., 1] - box1[..., 3] / 2.0, box1[..., 1] + box1[..., 3] / 2.0\n","        b2_x1, b2_x2 = box2[..., 0] - box2[..., 2] / 2.0, box2[..., 0] + box2[..., 2] / 2.0\n","        b2_y1, b2_y2 = box2[..., 1] - box2[..., 3] / 2.0, box2[..., 1] + box2[..., 3] / 2.0\n","\n","        if torch.allclose(box1.float(), box2.float(), atol=self.eps):\n","            return torch.ones_like(box1[..., 0])\n","\n","        inter = torch.clamp((torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)), min=0) * \\\n","                torch.clamp((torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)), min=0)\n","\n","        w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1\n","        w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1\n","\n","        union = (w1 * h1 + w2 * h2 - inter) + self.eps\n","        iou = inter / union\n","\n","        if CIoU or DIoU:\n","            cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)\n","            ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)\n","            c2 = (cw ** 2 + ch ** 2) + self.eps\n","            rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4\n","            if DIoU:\n","                return iou - rho2 / c2\n","            elif CIoU:\n","                v = (4 / (math.pi ** 2)) * torch.pow(torch.atan(w2 / (h2 + self.eps)) - torch.atan(w1 / (h1 + self.eps)), 2)\n","                alpha = v / (1 - iou + v + self.eps)\n","                return iou - (rho2 / c2 + v * alpha)\n","        return iou\n","\n","    def get_box_loss(self, predictions, targets):\n","        box_loss = 0\n","        for pi, ti in zip(predictions, targets):\n","            mask = ti[..., 4] > 0\n","            p_boxes, t_boxes = pi[mask][..., :4], ti[mask][..., :4]\n","            \n","            if p_boxes.numel() > 0:\n","                iou = self.bbox_iou(p_boxes, t_boxes, CIoU=True)\n","                box_loss += torch.mean(1 - iou)\n","                \n","                # Additional L1 loss for better convergence\n","                box_loss += F.l1_loss(p_boxes, t_boxes, reduction='mean')\n","        return box_loss\n","\n","    def get_obj_loss(self, predictions, targets):\n","        obj_loss = 0.0\n","        pos_weight = 8.0\n","        neg_weight = 0.5\n","        smooth_factor = 0.1\n","        for pi, ti in zip(predictions, targets):\n","            pred_obj = pi[..., 4]\n","            target_obj = (1 - smooth_factor) * ti[..., 4] + smooth_factor * 0.5\n","            # Soft objectness targets based on predicted IoU\n","            pred_box = pi[..., :4]\n","            target_box = ti[..., :4]\n","            ious = self.bbox_iou(pred_box, target_box)\n","            soft_target_obj = torch.max(target_obj, ious)\n","            # Calculate binary cross-entropy with Focal Loss\n","            obj_loss_per_anchor = self.focal_loss(pred_obj, soft_target_obj)\n","            # Weight positive and negative samples differently\n","            pos_mask = (soft_target_obj > 0.5).float()\n","            neg_mask = (soft_target_obj <= 0.5).float()\n","            weighted_loss = pos_weight * pos_mask * obj_loss_per_anchor + neg_weight * neg_mask * obj_loss_per_anchor\n","            obj_loss += torch.mean(weighted_loss)\n","        return obj_loss\n","\n","    def get_cls_loss(self, predictions, targets):\n","        cls_loss = 0\n","        if self.num_classes > 1:\n","            for pi, ti in zip(predictions, targets):\n","                pred_cls = pi[..., 5:]\n","                target_cls = ti[..., 5:]\n","                if not torch.allclose(pred_cls.float(), target_cls.float(), atol=self.eps):\n","                    target_cls = (1 - self.label_smoothing) * target_cls + self.label_smoothing / self.num_classes\n","                cls_loss += torch.mean(self.focal_loss(pred_cls, target_cls))\n","        return cls_loss\n","\n","    def forward(self, predictions, targets):\n","        b_size = targets.size(0)\n","        coarse_size = 7 * 7 * self.num_anchors * (5 + self.num_classes)\n","        medium_size = 14 * 14 * self.num_anchors * (5 + self.num_classes)\n","        fine_size = 28 * 28 * self.num_anchors * (5 + self.num_classes)\n","\n","        flat_coarse, flat_medium, flat_fine = torch.split(targets, [coarse_size, medium_size, fine_size], dim=1)\n","        t_coarse = flat_coarse.view(b_size, 7, 7, self.num_anchors, (5 + self.num_classes))\n","        t_medium = flat_medium.view(b_size, 14, 14, self.num_anchors, (5 + self.num_classes))\n","        t_fine = flat_fine.view(b_size, 28, 28, self.num_anchors, (5 + self.num_classes))\n","\n","        targets_split = [t_coarse, t_medium, t_fine]\n","\n","        box_loss = self.get_box_loss(predictions, targets_split)\n","        obj_loss = self.get_obj_loss(predictions, targets_split)\n","        cls_loss = self.get_cls_loss(predictions, targets_split)\n","        \n","        total_loss = self.lambda_coord * box_loss + self.lambda_noobj * obj_loss + self.lambda_class * cls_loss\n","        return total_loss"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Training"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.182684Z","iopub.status.busy":"2024-10-03T21:10:24.182334Z","iopub.status.idle":"2024-10-03T21:10:35.238599Z","shell.execute_reply":"2024-10-03T21:10:35.237680Z","shell.execute_reply.started":"2024-10-03T21:10:24.182648Z"},"trusted":true},"outputs":[],"source":["train_dataset = ICDAR2015(train_path, train_labels, num_classes)\n","train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n","                           num_workers=num_workers, collate_fn=ICDAR2015.collate_fn,\n","                           pin_memory=True)\n","\n","val_dataset = ICDAR2015(test_path, test_labels, num_classes)\n","val_loader  = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n","                         num_workers=num_workers, collate_fn=ICDAR2015.collate_fn,\n","                         pin_memory=True)"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://pytorch.org/tutorials/_images/pinmem.png\" alt=\"why pin_memory\" width=\"350\" height=\"auto\">"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def evaluate(model, criterion, data_loader, device):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():\n","        for images, targets in data_loader:\n","            images = images.to(device, non_blocking=True)\n","            targets = targets.to(device, non_blocking=True)\n","            with torch.amp.autocast(device_type=str(device)):\n","                outputs = model(images)\n","                loss = criterion(outputs, targets)\n","            total_loss += loss.item()\n","    return total_loss / len(data_loader)\n","\n","def adaptive_gradient_clipping(model, clip_factor=0.01, eps=1e-3):\n","    for param in model.parameters():\n","        param_norm = torch.norm(param.grad)\n","        clip_value = clip_factor * (torch.norm(param) + eps)\n","        param.grad = param.grad * (clip_value / (param_norm + eps))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:35.240698Z","iopub.status.busy":"2024-10-03T21:10:35.240174Z","iopub.status.idle":"2024-10-03T21:10:35.530099Z","shell.execute_reply":"2024-10-03T21:10:35.529042Z","shell.execute_reply.started":"2024-10-03T21:10:35.240641Z"},"trusted":true},"outputs":[],"source":["model = MobileYOLOv3(num_classes=num_classes).to(device)\n","criterion = YoLoss()\n","\n","# Adam and decoupling weight decay from gradient update\n","base_optimizer = torch.optim.AdamW([\n","    {'params': model.backbone.parameters(), 'lr': learning_rate * 1.5, 'weight_decay': weight_decay * 0.5},\n","    {'params': model.conv_7.parameters()},\n","    {'params': model.eca_7.parameters()},\n","    {'params': model.det1.parameters()},\n","    {'params': model.r_1024_128.parameters()},\n","    {'params': model.r_48_128.parameters()},\n","    {'params': model.conv_14.parameters()},\n","    {'params': model.eca_14.parameters()},\n","    {'params': model.det2.parameters()},\n","    {'params': model.r_512_64.parameters()},\n","    {'params': model.r_24_64.parameters()},\n","    {'params': model.conv_28.parameters()},\n","    {'params': model.eca_28.parameters()},\n","    {'params': model.det3.parameters()},\n","], lr=learning_rate, weight_decay=weight_decay)\n","\n","# Periodically look ahead, update weights by averaging weight updates at every k steps\n","optimizer = optim.Lookahead(base_optimizer, k=optim_k, alpha=optim_alpha)\n","\n","# Gradually cool down LR over time\n","scheduler = OneCycleLR(optimizer, max_lr=learning_rate, epochs=num_epochs, steps_per_epoch=len(train_loader),\n","                       pct_start=0.3, anneal_strategy='cos', div_factor=25.0, final_div_factor=10000.0)\n","\n","# Avoids numerical underflow/overflow through scaling, helps maintain information in mixed-precision\n","scaler = torch.amp.GradScaler(enabled=(str(device) != 'cpu'))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:35.532217Z","iopub.status.busy":"2024-10-03T21:10:35.531686Z","iopub.status.idle":"2024-10-03T21:22:36.998857Z","shell.execute_reply":"2024-10-03T21:22:36.997495Z","shell.execute_reply.started":"2024-10-03T21:10:35.532134Z"},"trusted":true},"outputs":[],"source":["lossi, losst = [], []\n","lowsi = float('inf')\n","\n","# The fine selection of hyperparameters\n","print(f\"{batch_size} | {learning_rate} | {weight_decay} | {num_epochs} | {lr_warmup} | {optim_k} | {optim_alpha} | {warmup_start} | {scheduler_t0} | {scheduler_tmult} | {prune_amount} | {target_architecture} | {mixup_alpha} | {num_anchors}\")\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_loss = 0\n","    \n","    for files, targets in train_loader:\n","        files = files.to(device, non_blocking=True)\n","        targets = targets.to(device, non_blocking=True)\n","        optimizer.zero_grad()\n","        \n","        with torch.amp.autocast(device_type=str(device)):\n","            logits = model(files)\n","            loss = criterion(logits, targets)\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        scheduler.step()\n","        epoch_loss += loss.item()\n","        \n","    epoch_loss /= len(train_loader)\n","    lossi.append(epoch_loss)\n","\n","    t_loss = evaluate(model, criterion, val_loader, device)\n","    losst.append(t_loss)\n","        \n","    # GPUs aren't infinite\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    # Print training and test loss\n","    print(f'Epoch [{epoch+1:3}/{num_epochs}] | Train: {epoch_loss:8.6f} | Test: {t_loss:8.6f} | LR: {optimizer.param_groups[-1][\"lr\"]:.6f}')"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:22:37.001075Z","iopub.status.busy":"2024-10-03T21:22:37.000644Z","iopub.status.idle":"2024-10-03T21:22:37.315232Z","shell.execute_reply":"2024-10-03T21:22:37.314338Z","shell.execute_reply.started":"2024-10-03T21:22:37.001034Z"},"trusted":true},"outputs":[],"source":["# Save unaltered model\n","torch.save(model.state_dict(), f'solid_{model_path}')\n","\n","# Prune, Quantize\n","pruned_model = prune_model(model, amount=prune_amount)\n","lifted_model = lift_pruning(pruned_model)\n","quantized_model = quantize_model(lifted_model, device)\n","\n","# Save the quantized model\n","torch.save(quantized_model.state_dict(), model_path)"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Loss Graph"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:22:37.316866Z","iopub.status.busy":"2024-10-03T21:22:37.316534Z","iopub.status.idle":"2024-10-03T21:22:37.695052Z","shell.execute_reply":"2024-10-03T21:22:37.693981Z","shell.execute_reply.started":"2024-10-03T21:22:37.316831Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(range(num_epochs), lossi, label=\"Training Loss\", color='blue', marker='o', linestyle='-', markersize=3)\n","plt.plot(range(num_epochs), losst, label=\"Test Loss\", color=\"red\", marker='o', linestyle='-', markersize=3)\n","\n","plt.title('Loss Curves', fontsize=16)\n","plt.xlabel('Epochs', fontsize=14)\n","plt.ylabel('Loss', fontsize=14)\n","plt.grid(True)\n","plt.xticks(range(0, num_epochs, 4))\n","plt.legend(loc='upper right')\n","plt.show();"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Evaluate"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:30:41.780508Z","iopub.status.busy":"2024-10-03T21:30:41.779478Z","iopub.status.idle":"2024-10-03T21:30:41.790096Z","shell.execute_reply":"2024-10-03T21:30:41.788973Z","shell.execute_reply.started":"2024-10-03T21:30:41.780462Z"},"trusted":true},"outputs":[],"source":["def load_model(model_class, num_classes, model_path, device='cpu'):\n","    \"\"\"\n","    Load a PyTorch model for inference on the target device, regardless of where it was originally trained.\n","    \"\"\"\n","    if isinstance(device, str):\n","        device = torch.device(device)\n","\n","    # Load to CPU first\n","    state_dict = torch.load(model_path, map_location=device, weights_only=False)\n","    \n","    if isinstance(state_dict, dict) and 'model_state_dict' in state_dict:\n","        state_dict = state_dict['model_state_dict']\n","\n","    # Remove 'module.' prefix caused by SWA\n","    state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n","\n","    # Remove pruning-related keys\n","    new_state_dict = {}\n","    for key, value in state_dict.items():\n","        if 'weight_mask' not in key:\n","            new_key = key.replace('weight_orig', 'weight')\n","            new_state_dict[new_key] = value\n","        \n","    model = model_class(num_classes)\n","    model.load_state_dict(new_state_dict, strict=False)\n","    model = model.to(device)\n","    model.eval()\n","    return model"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["model = load_model(MobileYOLOv3, num_classes, model_path, device='cuda')\n","test_dataset = ICDAR2015(test_path, test_labels)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def display_with_boxes(img, outputs):\n","    # img (3, 224, 224), \n","    # labels (43218) = flat((7, 7, num_anchors, (5 + num_classes)), (14, 14, num_anchors, (5 + num_classes)), (28, 28, num_anchors, (5 + num_classes)))\n","    _, ax = plt.subplots(1)\n","\n","    img = img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n","    ax.imshow(img)\n","    outputs = [outputs[0].squeeze(0).cpu(), outputs[1].squeeze(0).cpu(), outputs[2].squeeze(0).cpu()]\n","    img_x, img_y = img.shape[0], img.shape[1]\n","\n","    t_coarse = outputs[0]\n","    t_medium = outputs[1]\n","    t_fine = outputs[2]\n","\n","    for target in [t_coarse, t_medium, t_fine]:\n","        for i in range(target.shape[0]):  # Iterate over rows\n","            for j in range(target.shape[1]):  # Iterate over columns\n","                for k in range(num_anchors):  # Iterate over anchors\n","                    box = target[i, j, k, :4] # (x, y, w, h)\n","                    obj = target[i, j, k, 4]\n","                    if box.sum() > 0 and obj > 0.5:\n","                        print(\"Hi\")\n","                        x, y, w, h = box\n","                        x, y, w, h = x * img_x, y * img_y, w * img_x, h * img_y\n","                        rect = patches.Rectangle((x - w / 2, y - h / 2), w, h, linewidth=1, edgecolor='g', facecolor='none')\n","                        ax.add_patch(rect)\n","    \n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","    plt.show()\n","\n","def visualize_inference(model, dataset, num_images=5):\n","    for _ in range(num_images):\n","        img, _ = dataset[random.randint(0, len(dataset))]\n","        with torch.no_grad():\n","            img = img.unsqueeze(0).to(device)\n","            outputs = model(img)\n","            display_with_boxes(img, outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["visualize_inference(model, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1150310,"sourceId":1928836,"sourceType":"datasetVersion"}],"dockerImageVersionId":30775,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"ai","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
