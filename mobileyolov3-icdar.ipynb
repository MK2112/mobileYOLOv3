{"cells":[{"cell_type":"markdown","metadata":{},"source":["# MobileNetV3 YOLOv3 on ICDAR 2015\n","\n","A text detection model with a MobileNetV3 backbone, trained according to the YOLOv3 paradigm on the ICDAR 2015 dataset.<br>\n","One-shot pruned and quantized for deployment on edge-devices."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:21.050641Z","iopub.status.busy":"2024-10-03T21:10:21.050117Z","iopub.status.idle":"2024-10-03T21:10:24.052955Z","shell.execute_reply":"2024-10-03T21:10:24.052036Z","shell.execute_reply.started":"2024-10-03T21:10:21.050583Z"},"trusted":true},"outputs":[],"source":["import gc\n","import csv\n","import math\n","import torch\n","import random\n","import numpy as np\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import torch_optimizer as optim\n","import matplotlib.patches as patches\n","import torch.nn.utils.prune as prune\n","import torchvision.transforms.functional as TF\n","\n","from PIL import Image\n","from pathlib import Path\n","from torchvision import transforms\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.lr_scheduler import OneCycleLR\n","from mobileyolov3 import MobileYOLOv3, DSConv, Resizer\n","\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.054910Z","iopub.status.busy":"2024-10-03T21:10:24.054406Z","iopub.status.idle":"2024-10-03T21:10:24.094925Z","shell.execute_reply":"2024-10-03T21:10:24.093802Z","shell.execute_reply.started":"2024-10-03T21:10:24.054872Z"},"trusted":true},"outputs":[],"source":["torch.manual_seed(42)\n","torch.cuda.manual_seed_all(42)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using training device: {device}\")\n","\n","batch_size = 128\n","num_workers = 4\n","num_classes = 1\n","learning_rate = 5e-4\n","num_epochs = 500\n","lr_warmup = num_epochs * 0.1\n","weight_decay = 1e-5\n","optim_k = 5\n","optim_alpha = 0.3\n","warmup_start = 0.35\n","warmup_epochs = lr_warmup\n","scheduler_t0 = warmup_epochs\n","scheduler_tmult = 2\n","prune_amount = 0.2\n","dropout_rate = 0.3\n","target_architecture = 'cuda'\n","anchors = [(0.28, 0.35), (0.43, 0.58), (0.62, 0.78)]\n","num_anchors = len(anchors)\n","\n","# https://www.kaggle.com/datasets/bestofbests9/icdar2015\n","dataset_path = Path(\"./icdar2015/\")\n","train_path = dataset_path / 'ch4_training_images'\n","train_labels = dataset_path / 'ch4_training_localization_transcription_gt'\n","test_path = dataset_path / 'ch4_test_images'\n","test_labels = dataset_path / 'ch4_test_localization_transcription_gt'\n","\n","model_path = 'mobileyolov3_icdar2015.pth'"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Activation Functions"]},{"cell_type":"markdown","metadata":{},"source":["We will need to incorporate the correct activation functions and place them throughout our model.<br>\n","If you expect a value to only ever be $[0;1]$, then using LeakyReLU might not be as computationally effective (projects into $[0;\\infty]$) as Sigmoid (projects into $[0;1]$).\n","\n","Plotting some activation function candidates helps in making sure that you really select the most fitting activation for your setting:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# This is interesting too: https://pat.chormai.org/blog/2020-relu-softplus (considered, but didn't end up using it)\n","# For Swish: https://arxiv.org/pdf/1710.05941.pdf (esp pp. 5-6) (considered, but didn't end up using it)\n","\n","x = np.linspace(-10, 10, 400)\n","y_relu = np.maximum(0, x)\n","y_leaky_relu_0_1 = np.where(x > 0, x, 0.1 * x)\n","y_leaky_relu_0_02 = np.where(x > 0, x, 0.02 * x)\n","y_sigmoid = 1 / (1 + np.exp(-x))\n","y_softplus_beta_0_5 = (1 / 0.5) * np.log(1 + np.exp(0.5 * x))\n","y_softplus_beta_1 = (1 / 1) * np.log(1 + np.exp(1 * x))\n","y_swish_derivative = (x * y_sigmoid) + y_sigmoid * (1 - (x * y_sigmoid))\n","y_elu = np.where(x > 0, x, np.exp(x) - 1)\n","\n","plt.figure(figsize=(8, 4))\n","plt.plot(x, y_relu, label=\"ReLU\", linewidth=2, color='black')\n","plt.plot(x, y_sigmoid, label=\"Sigmoid\", linewidth=2, color='red')\n","plt.plot(x, y_elu, label=\"ELU\", linewidth=2, linestyle='dashed', color='brown')\n","plt.plot(x, y_leaky_relu_0_1, label=\"Leaky ReLU (α=0.1)\", linewidth=2, linestyle='dotted', color='green')\n","plt.plot(x, y_leaky_relu_0_02, label=\"Leaky ReLU (α=0.02)\", linewidth=2, linestyle='dotted', color='gray')\n","plt.plot(x, y_softplus_beta_0_5, label=\"Softplus (β=0.5)\", linewidth=2, color='blue')\n","plt.plot(x, y_softplus_beta_1, label=\"Softplus (β=1)\", linewidth=2, linestyle='dashed', color='orange')\n","plt.plot(x, y_swish_derivative, label=\"Swish Derivative\", linewidth=2, color='purple')\n","\n","plt.title('Compared Activation Functions', fontsize=14)\n","plt.xlabel('x', fontsize=12)\n","plt.ylabel('activation(x)', fontsize=12)\n","plt.legend(loc='upper left')\n","plt.axhline(0, color='black', linewidth=0.5)\n","plt.axvline(0, color='black', linewidth=0.5)\n","plt.grid(True)\n","plt.xlim(-7, 7)\n","plt.ylim(-1, 3)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Pruning & Quantization Definition"]},{"cell_type":"markdown","metadata":{},"source":["Pruning and quantization will get applied after model training concluded.\n","\n","Pruning picks out tiny/non-contributing weights and removes them from the model structure all together.<br>\n","This frees up memory and processing resources at little to no cost in accuracy. Impact on accuracy varies per use-case though.<br>\n","Quantization is an additional step that, considering weights, doesn't look at the value itself, but the difference between required and actually granted numeric precision.<br>\n","If a weight can be represented little informational loss through a coarser numeric precision, this could decrease memory and computation demands."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.110103Z","iopub.status.busy":"2024-10-03T21:10:24.109729Z","iopub.status.idle":"2024-10-03T21:10:24.125642Z","shell.execute_reply":"2024-10-03T21:10:24.124621Z","shell.execute_reply.started":"2024-10-03T21:10:24.110066Z"},"trusted":true},"outputs":[],"source":["def prune_model(model, amount):\n","    def prune_conv(conv, amount):\n","        prune.ln_structured(conv, name='weight', amount=amount, n=2, dim=0)\n","\n","    for _, module in model.named_modules():\n","        if isinstance(module, DSConv):\n","            prune_conv(module.depthwise, amount)\n","            prune_conv(module.pointwise, amount)\n","        elif isinstance(module, Resizer):\n","            if isinstance(module.conv, DSConv):\n","                prune_conv(module.conv.depthwise, amount)\n","                prune_conv(module.conv.pointwise, amount)\n","            else:\n","                prune_conv(module.conv, amount)\n","        elif isinstance(module, nn.Conv2d):\n","            prune_conv(module, amount)\n","\n","    parameters_to_prune = []\n","    for module in model.modules():\n","        if isinstance(module, nn.Conv2d):\n","            parameters_to_prune.append((module, 'weight'))\n","        elif isinstance(module, DSConv):\n","            parameters_to_prune.extend([(module.depthwise, 'weight'), (module.pointwise, 'weight')])\n","        elif isinstance(module, nn.Linear):\n","            parameters_to_prune.append((module, 'weight'))\n","    prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=amount)\n","    return model\n","\n","def quantize_model(model, device):\n","    \"\"\"\n","    Crunch numeric precision of weights and activations.\n","    \"\"\"\n","    model = model.cpu() # Works only on CPU\n","    quantized_model = torch.quantization.quantize_dynamic(model, {nn.Conv2d, nn.Linear}, dtype=torch.qint8, inplace=True) # inplace=True avoids deepcopy issues\n","    return quantized_model.to(device)\n","\n","def lift_pruning(model):\n","    for module in model.modules():\n","        if isinstance(module, (nn.Conv2d, nn.Linear)):\n","            if hasattr(module, 'weight_mask'):\n","                module.weight.data *= module.weight_mask\n","                prune.remove(module, 'weight')\n","            elif hasattr(module, 'weight_orig'):\n","                # If there's a weight_orig but no mask, it means pruning was applied but the mask was removed\n","                module.weight.data = module.weight_orig.data\n","                delattr(module, 'weight_orig')\n","        elif isinstance(module, DSConv):\n","            for conv in [module.depthwise, module.pointwise]:\n","                if hasattr(conv, 'weight_mask'):\n","                    conv.weight.data *= conv.weight_mask\n","                    prune.remove(conv, 'weight')\n","                elif hasattr(conv, 'weight_orig'):\n","                    conv.weight.data = conv.weight_orig.data\n","                    delattr(conv, 'weight_orig')\n","        elif isinstance(module, Resizer):\n","            if isinstance(module.conv, DSConv):\n","                for conv in [module.conv.depthwise, module.conv.pointwise]:\n","                    if hasattr(conv, 'weight_mask'):\n","                        conv.weight.data *= conv.weight_mask\n","                        prune.remove(conv, 'weight')\n","                    elif hasattr(conv, 'weight_orig'):\n","                        conv.weight.data = conv.weight_orig.data\n","                        delattr(conv, 'weight_orig')\n","            else:\n","                if hasattr(module.conv, 'weight_mask'):\n","                    module.conv.weight.data *= module.conv.weight_mask\n","                    prune.remove(module.conv, 'weight')\n","                elif hasattr(module.conv, 'weight_orig'):\n","                    module.conv.weight.data = module.conv.weight_orig.data\n","                    delattr(module.conv, 'weight_orig')\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Datasets"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.127518Z","iopub.status.busy":"2024-10-03T21:10:24.127141Z","iopub.status.idle":"2024-10-03T21:10:24.162452Z","shell.execute_reply":"2024-10-03T21:10:24.161267Z","shell.execute_reply.started":"2024-10-03T21:10:24.127481Z"},"trusted":true},"outputs":[],"source":["class ICDAR2015(Dataset):\n","    \"\"\"\n","    ICDAR2015 Dataset for YOLOv3 training.\n","    \"\"\"\n","    def __init__(self, input_path, label_path, num_classes=1, num_anchors=3, img_size=(224, 224), img_format='.jpg', anchors=anchors):\n","        self.input_path = Path(input_path)  # Path to images\n","        self.label_path = Path(label_path)  # Path to labels\n","        self.num_classes = num_classes      # Number of associable classes\n","        self.num_anchors = num_anchors      # Number of predictable distinct objects per grid tile\n","        self.img_size = img_size            # Image size\n","        self.batch_count = 0                # Batch counter\n","        self.anchors = anchors\n","        # Encounter same image multiple times, different augmentations each time\n","        self.transform = transforms.Compose([\n","            transforms.Resize(img_size),\n","            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","            transforms.RandomAdjustSharpness(sharpness_factor=3.0, p=0.5),\n","            transforms.ToTensor()\n","        ])\n","        self.files = self._assemble_files(img_format=img_format)\n","        self.labels = [self._parse_label(label, Image.open(img).size) for img, label in self.files]\n","\n","    def _flip_image_and_labels(self, img, labels):\n","        img = TF.hflip(img)\n","        flipped_labels = labels.clone()\n","        grid_sizes = [7, 14, 28]\n","        num_anchors = self.num_anchors\n","        num_classes = self.num_classes\n","        start_idx = 0\n","        for grid_size in grid_sizes:\n","            grid_area = grid_size * grid_size\n","            level_size = grid_area * num_anchors * (5 + num_classes)\n","            level_labels = flipped_labels[start_idx:start_idx+level_size].view(grid_size, grid_size, num_anchors, 5 + num_classes)\n","            level_labels[:, :, :, 0] = 1 - level_labels[:, :, :, 0] - level_labels[:, :, :, 2]\n","            level_labels = torch.flip(level_labels, [1])\n","            flipped_labels[start_idx:start_idx+level_size] = level_labels.view(-1)\n","            start_idx += level_size\n","        return img, flipped_labels\n","\n","    def _assemble_files(self, img_format):\n","        image_files, data = list(self.input_path.glob(f'*{img_format}')), []\n","        for img_file in image_files:\n","            img_id = img_file.stem.split('_')[-1]\n","            label_file = self.label_path / f\"gt_img_{img_id}.txt\"\n","            if label_file.exists():\n","                data.append((img_file, label_file))\n","            else:\n","                print(f\"Warning: No matching label file found for {img_file.name}\")\n","        return data\n","\n","    def __len__(self):\n","        return len(self.files)\n","    \n","    def __getitem__(self, idx):\n","        img_path, _ = self.files[idx]\n","        img = Image.open(img_path)\n","        img = self.transform(img)\n","        label = self.labels[idx]\n","        aug_coin = np.random.rand()\n","        if aug_coin < 0.25:\n","            return self._flip_image_and_labels(img, label)\n","        return img, label\n","    \n","    def __iter__(self):\n","        self.index = 0\n","        return self\n","\n","    def __next__(self):\n","        if self.index >= len(self):\n","            raise StopIteration\n","        item = self[self.index]\n","        self.index += 1\n","        return item\n","\n","    def _calculate_iou(self, box1, box2):\n","        # Calculate intersection\n","        x1 = max(box1[0], box2[0])\n","        y1 = max(box1[1], box2[1])\n","        x2 = min(box1[0] + box1[2], box2[0] + box2[2])\n","        y2 = min(box1[1] + box1[3], box2[1] + box2[3])\n","        inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n","        box1_area, box2_area = box1[2] * box1[3], box2[2] * box2[3]\n","        union_area = box1_area + box2_area - inter_area\n","        return inter_area / union_area if union_area > 0 else 0\n","\n","    def _to_grid(self, grid, box):\n","        grid_h, grid_w = grid.size(0), grid.size(1)  # Switched to height-first indexing\n","        x, y, w, h, obj, _ = box\n","        # Grid cell coordinates\n","        grid_x, grid_y = int(x * grid_w), int(y * grid_h)\n","        # Convert to relative coordinates in grid\n","        x, y = x * grid_w - grid_x, y * grid_h - grid_y\n","        best_iou, best_anchor_idx = 0, -1\n","        for anchor_idx, (anchor_w, anchor_h) in enumerate(self.anchors):\n","            anchor_box = torch.tensor([x, y, w / anchor_w, h / anchor_h])\n","            iou = self._calculate_iou(anchor_box.numpy(), [x, y, w, h])\n","            if iou > best_iou:\n","                best_iou = iou\n","                best_anchor_idx = anchor_idx\n","        if best_anchor_idx >= 0:\n","            anchor_slice = slice(best_anchor_idx * (5 + self.num_classes), (best_anchor_idx + 1) * (5 + self.num_classes))\n","            grid[grid_y, grid_x, anchor_slice][:4] = torch.tensor([x, y, w / self.anchors[best_anchor_idx][0], h / self.anchors[best_anchor_idx][1]])\n","            grid[grid_y, grid_x, anchor_slice][4] = obj\n","\n","    def _parse_label(self, label_path, img_size):\n","        coarse_labels = torch.zeros((7, 7, self.num_anchors * (5 + self.num_classes)))\n","        medium_labels = torch.zeros((14, 14, self.num_anchors * (5 + self.num_classes)))\n","        fine_labels   = torch.zeros((28, 28, self.num_anchors * (5 + self.num_classes)))\n","\n","        with open(label_path, 'r', encoding='utf-8-sig') as file:\n","            reader = csv.reader(file, delimiter=',')\n","            for row in reader:\n","                # Convert bounding box points to tensors\n","                row = torch.tensor([float(i) for i in row[:8]])  # BBox points as (x1, y1, x2, y2, ...)\n","                x, y = (row[0::2].sum() / 4, row[1::2].sum() / 4)  # Compute center (x, y)\n","                w = row[0::2].max() - row[0::2].min()  # BBox width\n","                h = row[1::2].max() - row[1::2].min()  # BBox height\n","                \n","                # Ensure that x, y are within image bounds\n","                x, y = min(x, img_size[0] - 1e-3), min(y, img_size[1] - 1e-3)\n","                w, h = min(w, img_size[0] - 1e-3), min(h, img_size[1] - 1e-3)\n","                \n","                # Normalize the bounding box coordinates\n","                x, y = x / img_size[0], y / img_size[1]\n","                w, h = w / img_size[0], h / img_size[1]\n","                \n","                obj, cls = 1.0, 0.0  # Change latter for accommodating multi-class settings\n","                box = torch.tensor([x, y, w, h, obj, cls])\n","                \n","                # Assign box to (coarse, medium, fine) grids\n","                self._to_grid(coarse_labels, box)\n","                self._to_grid(medium_labels, box)\n","                self._to_grid(fine_labels, box)\n","        \n","        # Flatten and concatenate the labels from all grid levels\n","        return torch.cat([coarse_labels.flatten(), medium_labels.flatten(), fine_labels.flatten()], dim=0)\n","\n","    def get_batch(self, batch_size, randomized=True):\n","        if randomized:\n","            indices = np.random.choice(len(self), batch_size, replace=False)\n","        else:\n","            indices = np.arange(self.batch_count, self.batch_count + batch_size) % len(self)\n","            self.batch_count += batch_size\n","        batch_images = torch.stack([self[i][0] for i in indices], dim=0) # Images\n","        batch_labels = torch.stack([self[i][1] for i in indices], dim=0) # Labels\n","        return batch_images, batch_labels\n","\n","    @staticmethod\n","    def collate_fn(batch):\n","        images, labels = zip(*batch)\n","        images = torch.stack(images, dim=0)\n","        labels = torch.stack(labels, dim=0)\n","        return images, labels"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Loss"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.164779Z","iopub.status.busy":"2024-10-03T21:10:24.164286Z","iopub.status.idle":"2024-10-03T21:10:24.181085Z","shell.execute_reply":"2024-10-03T21:10:24.180180Z","shell.execute_reply.started":"2024-10-03T21:10:24.164720Z"},"trusted":true},"outputs":[],"source":["class YoLoss(nn.Module):\n","    def __init__(self, num_classes=1, num_anchors=3, lambda_coord=7.0, lambda_noobj=1.0,\n","                 lambda_class=1.0, iou_threshold=0.5, focal_alpha=0.25, focal_gamma=2.0, label_smoothing=0.1, anchors=anchors):\n","        super(YoLoss, self).__init__()\n","        self.num_classes = num_classes\n","        self.num_anchors = num_anchors\n","        self.lambda_coord = lambda_coord\n","        self.lambda_noobj = lambda_noobj\n","        self.lambda_class = lambda_class\n","        self.iou_threshold = iou_threshold\n","        self.focal_alpha = focal_alpha\n","        self.focal_gamma = focal_gamma\n","        self.label_smoothing = label_smoothing\n","        self.anchors = torch.tensor(anchors)\n","        self.eps = 1e-7\n","    \n","    def gaussian_objectness(self, x, y, sigma=0.3):\n","        return torch.exp(-((x ** 2 + y ** 2) / (2 * sigma ** 2)))\n","\n","    def focal_loss(self, pred, target):\n","        if torch.allclose(pred.float(), target.float(), atol=self.eps):\n","            return torch.zeros_like(pred)\n","        pred_prob = torch.clamp(torch.sigmoid(pred), self.eps, 1 - self.eps)\n","        p_t = target * pred_prob + (1 - target) * (1 - pred_prob)\n","        p_t = torch.clamp(p_t, self.eps, 1 - self.eps)\n","        alpha_factor = target * self.focal_alpha + (1 - target) * (1 - self.focal_alpha)\n","        modulating_factor = (1.0 - p_t).pow(self.focal_gamma)\n","        loss = F.binary_cross_entropy_with_logits(pred, target, reduction='none')\n","        return alpha_factor * modulating_factor * loss\n","\n","    def bbox_iou(self, box1, box2, DIoU=False, CIoU=False):\n","        b1_x1, b1_x2 = box1[..., 0] - box1[..., 2] / 2.0, box1[..., 0] + box1[..., 2] / 2.0\n","        b1_y1, b1_y2 = box1[..., 1] - box1[..., 3] / 2.0, box1[..., 1] + box1[..., 3] / 2.0\n","        b2_x1, b2_x2 = box2[..., 0] - box2[..., 2] / 2.0, box2[..., 0] + box2[..., 2] / 2.0\n","        b2_y1, b2_y2 = box2[..., 1] - box2[..., 3] / 2.0, box2[..., 1] + box2[..., 3] / 2.0\n","        if torch.allclose(box1.float(), box2.float(), atol=self.eps):\n","            return torch.ones_like(box1[..., 0])\n","        inter = torch.clamp((torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)), min=0) * \\\n","                torch.clamp((torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)), min=0)\n","        inter = torch.clamp(inter, min=0)\n","        w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1\n","        w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1\n","        union = (w1 * h1 + w2 * h2 - inter) + self.eps\n","        iou = inter / torch.clamp(union, min=self.eps)\n","        if CIoU or DIoU:\n","            cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)\n","            ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)\n","            c2 = (cw ** 2 + ch ** 2) + self.eps\n","            rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4\n","            if DIoU:\n","                return iou - rho2 / c2\n","            elif CIoU:\n","                v = (4 / (math.pi ** 2)) * torch.pow(torch.atan(w2 / (h2 + self.eps)) - torch.atan(w1 / (h1 + self.eps)), 2)\n","                alpha = v / (1 - iou + v + self.eps)\n","                return iou - (rho2 / c2 + v * alpha)\n","        return iou\n","\n","    def get_box_loss(self, predictions, targets):\n","        box_loss = 0\n","        for pi, ti in zip(predictions, targets):\n","            mask = ti[..., 4] > 0\n","            p_boxes, t_boxes = pi[mask][..., :4], ti[mask][..., :4]\n","            if p_boxes.numel() > 0:\n","                iou = self.bbox_iou(p_boxes, t_boxes, CIoU=True)\n","                box_loss += torch.mean(1.0 - iou)\n","                box_loss += F.l1_loss(p_boxes, t_boxes, reduction='mean')\n","        return box_loss\n","\n","    def get_obj_loss(self, predictions, targets):\n","        obj_loss = 0.0\n","        pos_weight = 8.0\n","        neg_weight = 0.5\n","        smooth_factor = 0.1\n","        for pi, ti in zip(predictions, targets):\n","            pred_obj = pi[..., 4]\n","            target_obj = (1 - smooth_factor) * ti[..., 4] + smooth_factor * 0.5\n","            # Soft objectness targets based on predicted IoU\n","            pred_box = pi[..., :4]\n","            target_box = ti[..., :4]\n","            ious = self.bbox_iou(pred_box, target_box)\n","            soft_target_obj = torch.where(ious > self.iou_threshold, torch.ones_like(ious), target_obj)\n","            # Calculate binary cross-entropy with Focal Loss\n","            obj_loss_per_anchor = self.focal_loss(pred_obj, soft_target_obj)\n","            # Weight positive and negative samples differently\n","            pos_mask = (soft_target_obj > 0.5).float()\n","            neg_mask = (soft_target_obj <= 0.5).float()\n","            weighted_loss = pos_weight * pos_mask * obj_loss_per_anchor + neg_weight * neg_mask * obj_loss_per_anchor\n","            obj_loss += torch.mean(weighted_loss)\n","        return obj_loss\n","\n","    def get_cls_loss(self, predictions, targets):\n","        cls_loss = 0\n","        if self.num_classes > 1:\n","            for pi, ti in zip(predictions, targets):\n","                pred_cls = pi[..., 5:]\n","                target_cls = ti[..., 5:]\n","                if not torch.allclose(pred_cls.float(), target_cls.float(), atol=self.eps):\n","                    target_cls = (1 - self.label_smoothing) * target_cls + self.label_smoothing / self.num_classes\n","                cls_loss += torch.mean(self.focal_loss(pred_cls, target_cls))\n","        return cls_loss\n","\n","    def forward(self, predictions, targets):\n","        b_size = targets.size(0)\n","        coarse_size = 7 * 7 * self.num_anchors * (5 + self.num_classes)\n","        medium_size = 14 * 14 * self.num_anchors * (5 + self.num_classes)\n","        fine_size = 28 * 28 * self.num_anchors * (5 + self.num_classes)\n","\n","        flat_coarse, flat_medium, flat_fine = torch.split(targets, [coarse_size, medium_size, fine_size], dim=1)\n","        t_coarse = flat_coarse.view(b_size, 7, 7, self.num_anchors, (5 + self.num_classes))\n","        t_medium = flat_medium.view(b_size, 14, 14, self.num_anchors, (5 + self.num_classes))\n","        t_fine = flat_fine.view(b_size, 28, 28, self.num_anchors, (5 + self.num_classes))\n","\n","        targets_split = [t_coarse, t_medium, t_fine]\n","        \n","        box_loss = self.get_box_loss(predictions, targets_split)\n","        obj_loss = self.get_obj_loss(predictions, targets_split)\n","        cls_loss = self.get_cls_loss(predictions, targets_split)\n","        \n","        total_loss = self.lambda_coord * box_loss + self.lambda_noobj * obj_loss + self.lambda_class * cls_loss\n","\n","        if torch.isnan(total_loss):\n","            print(f'total_loss={total_loss} box_loss={box_loss} obj_loss={obj_loss} cls_loss={cls_loss}')\n","\n","        return total_loss"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","### Data + Loss Sanity Check"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["sanity_dataset = ICDAR2015(train_path, train_labels, num_classes)\n","sanity_criterion = YoLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Assuming train_path, train_labels, and num_classes are already defined\n","img, label = sanity_dataset[random.randint(0, len(sanity_dataset) - 1)]\n","_, labem = sanity_dataset[1]\n","imgb, labelb = sanity_dataset.get_batch(2)\n","\n","print('Image:', img.shape, '\\t\\tLabel:', label.shape)\n","print('Image Batch:', imgb.shape, '\\tLabel:', labelb.shape)\n","\n","def get_loss(label_a, label_b, title='', grid_sizes=[(7, 7), (14, 14), (28, 28)]):\n","    coarse_size = grid_sizes[0][0] * grid_sizes[0][1] * num_anchors * (5 + num_classes)\n","    medium_size = grid_sizes[1][0] * grid_sizes[1][1] * num_anchors * (5 + num_classes)\n","    fine_size = grid_sizes[2][0] * grid_sizes[2][1] * num_anchors * (5 + num_classes)\n","    coarse_flat, medium_flat, fine_flat = torch.split(label_a, [coarse_size, medium_size, fine_size], dim=0)\n","    coarse = coarse_flat.view(grid_sizes[0][0], grid_sizes[0][1], num_anchors, (5 + num_classes))\n","    medium = medium_flat.view(grid_sizes[1][0], grid_sizes[1][1], num_anchors, (5 + num_classes))\n","    fine = fine_flat.view(grid_sizes[2][0], grid_sizes[2][1], num_anchors, (5 + num_classes))\n","    predictions = [coarse.unsqueeze(0), medium.unsqueeze(0), fine.unsqueeze(0)]\n","    print(title, sanity_criterion(predictions, label_b.unsqueeze(0)), '\\n', '-' * 50)\n","\n","def calculate_iou(box1, box2):\n","    # Calculate intersection\n","    x1 = max(box1[0], box2[0])\n","    y1 = max(box1[1], box2[1])\n","    x2 = min(box1[0] + box1[2], box2[0] + box2[2])\n","    y2 = min(box1[1] + box1[3], box2[1] + box2[3])\n","    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n","    box1_area, box2_area = box1[2] * box1[3], box2[2] * box2[3]\n","    union_area = box1_area + box2_area - inter_area\n","    return inter_area / union_area if union_area > 0 else 0\n","\n","def show_image_with_bboxes(img, label, num_anchors=3, num_classes=1, grid_sizes=[(7, 7), (14, 14), (28, 28)], anchors=anchors):\n","    img_np = TF.to_pil_image(img)\n","    _, ax = plt.subplots(1)\n","    ax.imshow(img_np)\n","    ax.set_xticks([])\n","    ax.set_yticks([])\n","    coarse_size = grid_sizes[0][0] * grid_sizes[0][1] * num_anchors * (5 + num_classes)\n","    medium_size = grid_sizes[1][0] * grid_sizes[1][1] * num_anchors * (5 + num_classes)\n","    fine_size = grid_sizes[2][0] * grid_sizes[2][1] * num_anchors * (5 + num_classes)\n","    coarse_flat, medium_flat, fine_flat = torch.split(label, [coarse_size, medium_size, fine_size], dim=0)\n","    coarse = coarse_flat.view(grid_sizes[0][0], grid_sizes[0][1], num_anchors, (5 + num_classes))\n","    medium = medium_flat.view(grid_sizes[1][0], grid_sizes[1][1], num_anchors, (5 + num_classes))\n","    fine = fine_flat.view(grid_sizes[2][0], grid_sizes[2][1], num_anchors, (5 + num_classes))\n","    def draw_boxes(grid, grid_w, grid_h, anchors):\n","        for y in range(grid_h):\n","            for x in range(grid_w):\n","                best_iou, best_anchor_idx = 0, -1\n","                for a in range(num_anchors):\n","                    box = grid[y, x, a]\n","                    if box[4] > 0:\n","                        anchor_w, anchor_h = anchors[a]\n","                        w = box[2].item() * anchor_w * img_np.width  # Width\n","                        h = box[3].item() * anchor_h * img_np.height # Height\n","                        cx = (box[0].item() + x) / grid_w * img_np.width   # Center x\n","                        cy = (box[1].item() + y) / grid_h * img_np.height  # Center y\n","                        # Create a normalized bounding box for IoU calculation\n","                        normalized_box = torch.tensor([cx - w / 2, cy - h / 2, w, h])\n","                        # Calculate IoU with the current anchor\n","                        iou = calculate_iou(normalized_box.numpy(), [cx - w / 2, cy - h / 2, w, h])\n","                        # Update best IoU and corresponding anchor index\n","                        if iou > best_iou:\n","                            best_iou = iou\n","                            best_anchor_idx = a\n","                if best_anchor_idx != -1:\n","                    # Draw the bounding box with the best anchor\n","                    best_box = grid[y, x, best_anchor_idx]\n","                    anchor_w, anchor_h = anchors[best_anchor_idx]\n","                    w_best = best_box[2].item() * anchor_w * img_np.width  # Width using best anchor\n","                    h_best = best_box[3].item() * anchor_h * img_np.height # Height using best anchor\n","                    cx_best = (best_box[0].item() + x) / grid_w * img_np.width   # Center x using best anchor\n","                    cy_best = (best_box[1].item() + y) / grid_h * img_np.height  # Center y using best anchor\n","                    rect = patches.Rectangle((cx_best - w_best / 2, cy_best - h_best / 2), w_best, h_best,\n","                                             linewidth=1, edgecolor='g', facecolor='none')\n","                    ax.add_patch(rect)\n","\n","    draw_boxes(coarse, grid_sizes[0][0], grid_sizes[0][1], anchors)\n","    draw_boxes(medium, grid_sizes[1][0], grid_sizes[1][1], anchors)\n","    draw_boxes(fine, grid_sizes[2][0], grid_sizes[2][1], anchors)\n","    plt.show()\n","\n","show_image_with_bboxes(img, label)\n","get_loss(label, label, 'Self Loss:')\n","get_loss(label, labem, 'Random Label Comparison Loss:')"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Training"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.182684Z","iopub.status.busy":"2024-10-03T21:10:24.182334Z","iopub.status.idle":"2024-10-03T21:10:35.238599Z","shell.execute_reply":"2024-10-03T21:10:35.237680Z","shell.execute_reply.started":"2024-10-03T21:10:24.182648Z"},"trusted":true},"outputs":[],"source":["sanity_dataset = ICDAR2015(train_path, train_labels, num_classes)\n","train_loader  = DataLoader(sanity_dataset, batch_size=batch_size, shuffle=True, \n","                           num_workers=num_workers, collate_fn=ICDAR2015.collate_fn,\n","                           pin_memory=True)\n","\n","val_dataset = ICDAR2015(test_path, test_labels, num_classes)\n","val_loader  = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n","                         num_workers=num_workers, collate_fn=ICDAR2015.collate_fn,\n","                         pin_memory=True)"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://pytorch.org/tutorials/_images/pinmem.png\" alt=\"why pin_memory\" width=\"350\" height=\"auto\">"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def evaluate(model, criterion, data_loader, device):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():\n","        for images, targets in data_loader:\n","            images = images.to(device, non_blocking=True)\n","            targets = targets.to(device, non_blocking=True)\n","            with torch.amp.autocast(device_type=str(device)):\n","                outputs = model(images)\n","                loss = criterion(outputs, targets)\n","            total_loss += loss.item()\n","    return total_loss / len(data_loader)\n","\n","def adaptive_gradient_clipping(model, clip_factor=0.01, eps=1e-3):\n","    for param in model.parameters():\n","        param_norm = torch.norm(param.grad)\n","        clip_value = clip_factor * (torch.norm(param) + eps)\n","        param.grad = param.grad * (clip_value / (param_norm + eps))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:35.240698Z","iopub.status.busy":"2024-10-03T21:10:35.240174Z","iopub.status.idle":"2024-10-03T21:10:35.530099Z","shell.execute_reply":"2024-10-03T21:10:35.529042Z","shell.execute_reply.started":"2024-10-03T21:10:35.240641Z"},"trusted":true},"outputs":[],"source":["model = MobileYOLOv3(num_classes=num_classes, dropout_rate=dropout_rate, anchors=torch.tensor(anchors, dtype=torch.float32, device=device)).to(device)\n","criterion = YoLoss()\n","\n","# Adam and decoupling weight decay from gradient update\n","base_optimizer = torch.optim.AdamW([\n","    {'params': model.backbone.parameters(), 'lr': learning_rate * 0.8, 'weight_decay': weight_decay * 0.5},\n","    {'params': model.conv_7.parameters()},\n","    {'params': model.eca_7.parameters()},\n","    {'params': model.det1.parameters()},\n","    {'params': model.r_1024_128.parameters()},\n","    {'params': model.r_48_128.parameters()},\n","    {'params': model.conv_14.parameters()},\n","    {'params': model.eca_14.parameters()},\n","    {'params': model.det2.parameters()},\n","    {'params': model.r_512_64.parameters()},\n","    {'params': model.r_24_64.parameters()},\n","    {'params': model.conv_28.parameters()},\n","    {'params': model.eca_28.parameters()},\n","    {'params': model.det3.parameters()},\n","], lr=learning_rate, weight_decay=weight_decay)\n","\n","# Periodically look ahead, update weights by averaging weight updates at every k steps\n","optimizer = optim.Lookahead(base_optimizer, k=optim_k, alpha=optim_alpha)\n","\n","# Gradually cool down LR over time\n","scheduler = OneCycleLR(optimizer, max_lr=learning_rate*2, epochs=num_epochs, steps_per_epoch=len(train_loader),\n","                       pct_start=0.3, anneal_strategy='cos', div_factor=10.0, final_div_factor=10000.0)\n","\n","# Avoids numerical underflow/overflow through scaling, helps maintain information in mixed-precision\n","scaler = torch.amp.GradScaler(enabled=(str(device) != 'cpu'))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:35.532217Z","iopub.status.busy":"2024-10-03T21:10:35.531686Z","iopub.status.idle":"2024-10-03T21:22:36.998857Z","shell.execute_reply":"2024-10-03T21:22:36.997495Z","shell.execute_reply.started":"2024-10-03T21:10:35.532134Z"},"trusted":true},"outputs":[],"source":["# Expect this to take ~10 minutes on a standard 3060\n","lossi, losst = [], []\n","lowsi = float('inf')\n","\n","# The fine selection of hyperparameters\n","print(f\"{batch_size} | {learning_rate} | {weight_decay} | {num_epochs} | {lr_warmup} | {optim_k} | {optim_alpha} | {warmup_start} | {scheduler_t0} | {scheduler_tmult} | {prune_amount} | {target_architecture} | {num_anchors} | {dropout_rate}\")\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_loss = 0\n","    \n","    for files, targets in train_loader:\n","        files = files.to(device, non_blocking=True)\n","        targets = targets.to(device, non_blocking=True)\n","        optimizer.zero_grad()\n","        \n","        with torch.amp.autocast(device_type=str(device)):\n","            logits = model(files)\n","            loss = criterion(logits, targets)\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n","        optimizer.step()\n","        scheduler.step()\n","        epoch_loss += loss.item()\n","        \n","    epoch_loss /= len(train_loader)\n","    lossi.append(epoch_loss)\n","\n","    t_loss = evaluate(model, criterion, val_loader, device)\n","    losst.append(t_loss)\n","    \n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    # Print training and test loss\n","    print(f'Epoch [{epoch+1:3}/{num_epochs}] | Train: {epoch_loss:8.6f} | Test: {t_loss:8.6f} | LR: {optimizer.param_groups[-1][\"lr\"]:.6f}')"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:22:37.001075Z","iopub.status.busy":"2024-10-03T21:22:37.000644Z","iopub.status.idle":"2024-10-03T21:22:37.315232Z","shell.execute_reply":"2024-10-03T21:22:37.314338Z","shell.execute_reply.started":"2024-10-03T21:22:37.001034Z"},"trusted":true},"outputs":[],"source":["# Save unaltered model\n","torch.save(model.state_dict(), f'solid_{model_path}')\n","\n","# Prune, Quantize\n","pruned_model = prune_model(model, amount=prune_amount)\n","lifted_model = lift_pruning(pruned_model)\n","quantized_model = quantize_model(lifted_model, device)\n","\n","# Save the quantized model\n","torch.save(quantized_model.state_dict(), model_path)"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Loss Graph"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:22:37.316866Z","iopub.status.busy":"2024-10-03T21:22:37.316534Z","iopub.status.idle":"2024-10-03T21:22:37.695052Z","shell.execute_reply":"2024-10-03T21:22:37.693981Z","shell.execute_reply.started":"2024-10-03T21:22:37.316831Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(range(num_epochs), lossi, label=\"Training Loss\", color='blue', marker='o', linestyle='-', markersize=3)\n","plt.plot(range(num_epochs), losst, label=\"Test Loss\", color=\"red\", marker='o', linestyle='-', markersize=3)\n","\n","plt.title('Loss Curves', fontsize=16)\n","plt.xlabel('Epochs', fontsize=14)\n","plt.ylabel('Loss', fontsize=14)\n","plt.grid(True)\n","plt.xticks(range(0, num_epochs, 4))\n","plt.legend(loc='upper right')\n","plt.show();"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Evaluate"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:30:41.780508Z","iopub.status.busy":"2024-10-03T21:30:41.779478Z","iopub.status.idle":"2024-10-03T21:30:41.790096Z","shell.execute_reply":"2024-10-03T21:30:41.788973Z","shell.execute_reply.started":"2024-10-03T21:30:41.780462Z"},"trusted":true},"outputs":[],"source":["def load_model(model_class, num_classes, model_path, device='cpu'):\n","    \"\"\"\n","    Load a PyTorch model for inference on the target device, regardless of where it was originally trained.\n","    \"\"\"\n","    if isinstance(device, str):\n","        device = torch.device(device)\n","\n","    # Load to CPU first\n","    state_dict = torch.load(model_path, map_location=device, weights_only=False)\n","    \n","    if isinstance(state_dict, dict) and 'model_state_dict' in state_dict:\n","        state_dict = state_dict['model_state_dict']\n","\n","    # Remove 'module.' prefix caused by SWA\n","    state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n","\n","    # Remove pruning-related keys\n","    new_state_dict = {}\n","    for key, value in state_dict.items():\n","        if 'weight_mask' not in key:\n","            new_key = key.replace('weight_orig', 'weight')\n","            new_state_dict[new_key] = value\n","        \n","    model = model_class(num_classes)\n","    model.load_state_dict(new_state_dict, strict=False)\n","    model = model.to(device)\n","    model.eval()\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = load_model(MobileYOLOv3, num_classes, model_path, device='cuda')\n","test_dataset = ICDAR2015(test_path, test_labels)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def display_with_boxes(img, outputs):\n","    # img (3, 224, 224), \n","    # labels (43218) = flat((7, 7, num_anchors, (5 + num_classes)), (14, 14, num_anchors, (5 + num_classes)), (28, 28, num_anchors, (5 + num_classes)))\n","    _, ax = plt.subplots(1)\n","\n","    img = img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n","    ax.imshow(img)\n","    outputs = [outputs[0].squeeze(0).cpu(), outputs[1].squeeze(0).cpu(), outputs[2].squeeze(0).cpu()]\n","    img_x, img_y = img.shape[0], img.shape[1]\n","\n","    t_coarse = outputs[0]\n","    t_medium = outputs[1]\n","    t_fine = outputs[2]\n","\n","    for target in [t_coarse, t_medium, t_fine]:\n","        for i in range(target.shape[0]):  # Iterate over rows\n","            for j in range(target.shape[1]):  # Iterate over columns\n","                for k in range(num_anchors):  # Iterate over anchors\n","                    box = target[i, j, k, :4] # (x, y, w, h)\n","                    obj = target[i, j, k, 4]\n","                    if box.sum() > 0 and obj >= 0.2:\n","                        x, y, w, h = box\n","                        x, y, w, h = x * img_x, y * img_y, w * img_x, h * img_y\n","                        rect = patches.Rectangle((x - w / 2, y - h / 2), w, h, linewidth=1, edgecolor='g', facecolor='none')\n","                        ax.add_patch(rect)\n","    \n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","    plt.show()\n","\n","def visualize_inference(model, dataset, num_images=5):\n","    for _ in range(num_images):\n","        img, _ = dataset[random.randint(0, len(dataset))]\n","        with torch.no_grad():\n","            img = img.unsqueeze(0).to(device)\n","            outputs = model(img)\n","            display_with_boxes(img, outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["visualize_inference(model, test_dataset)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1150310,"sourceId":1928836,"sourceType":"datasetVersion"}],"dockerImageVersionId":30775,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"ai","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
