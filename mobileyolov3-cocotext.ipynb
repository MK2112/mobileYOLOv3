{"cells":[{"cell_type":"markdown","metadata":{},"source":["# MobileNetV3-YOLOv3 COCO-Text Detection"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:21.050641Z","iopub.status.busy":"2024-10-03T21:10:21.050117Z","iopub.status.idle":"2024-10-03T21:10:24.052955Z","shell.execute_reply":"2024-10-03T21:10:24.052036Z","shell.execute_reply.started":"2024-10-03T21:10:21.050583Z"},"trusted":true},"outputs":[],"source":["import gc\n","import os\n","import math\n","import torch\n","import random\n","import numpy as np\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import torch_optimizer as optim\n","import matplotlib.patches as patches\n","import torch.nn.utils.prune as prune\n","import torchvision.transforms.functional as TF\n","\n","from PIL import Image\n","from pathlib import Path\n","from datasets import load_dataset\n","from torchvision import transforms\n","from torch.nn import functional as F\n","from huggingface_hub import snapshot_download\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.lr_scheduler import OneCycleLR\n","from mobileyolov3 import MobileYOLOv3, DSConv, Resizer\n","\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Hyperparameters"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.054910Z","iopub.status.busy":"2024-10-03T21:10:24.054406Z","iopub.status.idle":"2024-10-03T21:10:24.094925Z","shell.execute_reply":"2024-10-03T21:10:24.093802Z","shell.execute_reply.started":"2024-10-03T21:10:24.054872Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using training device: cuda\n"]}],"source":["# CPU Seed for Reproducibility\n","torch.manual_seed(42)\n","torch.cuda.manual_seed_all(42)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# GPU Seed for Reproducibility\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(42)\n","\n","print(f\"Using training device: {device}\")\n","\n","batch_size = 128\n","num_workers = 4\n","num_classes = 1\n","learning_rate = 4e-4\n","num_epochs = 400\n","lr_warmup = num_epochs * 0.1\n","weight_decay = 1e-4\n","optim_k = 5\n","optim_alpha = 0.3\n","prune_amount = 0.2\n","dropout_rate = 0.4\n","t_arch = str(device)\n","anchors = [(0.28, 0.35), (0.43, 0.58), (0.62, 0.78)]\n","num_anchors = len(anchors)\n","model_path = 'mobileyolov3_coco-text.pth'"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Dataset"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset already exists at /mnt/data/COCO-Text\n"]}],"source":["# load_dataset by default loads to system drive and caches to home dir\n","# this is more flexible\n","\n","down_dir = Path(\"/mnt/data/\")\n","target_path = down_dir / \"COCO-Text\"\n","cache_path = down_dir / \"COCO-Text_Cache\"\n","\n","if not os.path.exists(target_path):\n","    os.makedirs(target_path)\n","    os.makedirs(cache_path, exist_ok=True)\n","\n","    dataset_id = \"howard-hou/COCO-Text\"\n","\n","    print(f\"Downloading {dataset_id}...\")\n","\n","    while True:\n","        try:\n","            snapshot_download(dataset_id, repo_type=\"dataset\", cache_dir=str(cache_path), local_dir=str(target_path))\n","            break\n","        except Exception as _:\n","            continue\n","else:\n","    print(f\"Dataset already exists at {target_path}\")"]},{"cell_type":"code","execution_count":78,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.127518Z","iopub.status.busy":"2024-10-03T21:10:24.127141Z","iopub.status.idle":"2024-10-03T21:10:24.162452Z","shell.execute_reply":"2024-10-03T21:10:24.161267Z","shell.execute_reply.started":"2024-10-03T21:10:24.127481Z"},"trusted":true},"outputs":[],"source":["class COCOText(Dataset):\n","    \"\"\"\n","    COCO-Text Dataset adapter for YOLOv3 training, loading from a local directory.\n","    \"\"\"\n","    def __init__(self, num_classes=1, num_anchors=3, img_size=(224, 224), anchors=anchors, split=\"train\"):\n","        self.num_classes = num_classes\n","        self.num_anchors = num_anchors\n","        self.img_size = img_size\n","        self.anchors = anchors\n","        self.batch_count = 0\n","        self.coarse = (7, 7, self.num_anchors * (5 + self.num_classes))\n","        self.medium = (14, 14, self.num_anchors * (5 + self.num_classes))\n","        self.fine = (28, 28, self.num_anchors * (5 + self.num_classes))\n","        self.transform = transforms.Compose([\n","            transforms.Resize(img_size),\n","            transforms.ColorJitter(brightness=0.1, contrast=0.3, saturation=0.3, hue=0.1),\n","            transforms.RandomAdjustSharpness(sharpness_factor=3.0, p=0.9),\n","            transforms.ToTensor()\n","        ])\n","\n","        # Load the dataset from the local directory \n","        # akin to https://stackoverflow.com/questions/77020278/how-to-load-a-huggingface-dataset-from-local-path\n","        # I named the split exactly like the original, you can change it to your liking\n","        self.dataset = load_dataset(\n","            \"parquet\",\n","            data_files={\n","                \"train\": str(target_path / \"data\" / \"train-*.parquet\"),\n","                \"validation\": str(target_path / \"data\" / \"validation-*.parquet\"),\n","            },\n","            cache_dir=str(cache_path),\n","            split=split,\n","        )\n","\n","    def _calculate_iou(self, box1, box2):\n","        # Intersection over Union (IoU) between two bounding boxes, range of returns is [0, 1]\n","        x1, y1 = max(box1[0], box2[0]), max(box1[1], box2[1])\n","        x2, y2 = min(box1[0] + box1[2], box2[0] + box2[2]), min(box1[1] + box1[3], box2[1] + box2[3])\n","        inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n","        box1_area, box2_area = box1[2] * box1[3], box2[2] * box2[3]\n","        union_area = box1_area + box2_area - inter_area\n","        return inter_area / union_area if union_area > 0 else 0 # Avoiding zero division\n","    \n","    def _to_grid(self, grid, box):\n","        # Convert bounding boxes to grid coordinates\n","        # BBox format: [x, y, w, h, obj, cls]\n","        # Anchor idxs == num_anchors: 0, 1, 2 \n","        # Assign BBox to the anchor (and its idx in grid coords therefore) with highest IoU\n","        grid_h, grid_w = grid.size(0), grid.size(1)\n","        x, y, w, h, obj, _ = box\n","        grid_x, grid_y = int(x * grid_w), int(y * grid_h)\n","        x, y = x * grid_w - grid_x, y * grid_h - grid_y\n","        best_iou, best_anchor_idx = 0, -1\n","        for anchor_idx, (anchor_w, anchor_h) in enumerate(self.anchors):\n","            anchor_box = torch.tensor([x, y, w / anchor_w, h / anchor_h])\n","            iou = self._calculate_iou(anchor_box.numpy(), [x, y, w, h])\n","            if iou > best_iou:\n","                best_iou = iou\n","                best_anchor_idx = anchor_idx\n","        if best_anchor_idx >= 0:\n","            anchor_slice = slice(best_anchor_idx * (5 + self.num_classes), (best_anchor_idx + 1) * (5 + self.num_classes))\n","            grid[grid_y, grid_x, anchor_slice][:4] = torch.tensor([x, y, w / self.anchors[best_anchor_idx][0], h / self.anchors[best_anchor_idx][1]])\n","            grid[grid_y, grid_x, anchor_slice][4] = obj\n","\n","    def _parse_label(self, label):\n","        # Read Label data, parse it, apply it to (7, 14, 28) grid, resolutions,\n","        # Concatenate them, flatten and return them as a single tensor\n","        coarse_labels = torch.zeros(self.coarse)\n","        medium_labels = torch.zeros(self.medium)\n","        fine_labels   = torch.zeros(self.fine)\n","        boxes = [[word[\"bounding_box\"][\"top_left_x\"], word[\"bounding_box\"][\"top_left_y\"], word[\"bounding_box\"][\"width\"], word[\"bounding_box\"][\"height\"]] for word in label]\n","        centers = [[box[0] + box[2] / 2, box[1] + box[3] / 2] for box in boxes] # these are all normalized already, so no need to divide by image size\n","        for i in range(len(boxes)):\n","            x, y = centers[i]\n","            w, h = boxes[i][2], boxes[i][3]\n","            obj, cls = 1.0, 0.0\n","            box = torch.tensor([x, y, w, h, obj, cls])\n","            self._to_grid(coarse_labels, box)\n","            self._to_grid(medium_labels, box)\n","            self._to_grid(fine_labels, box)\n","        return torch.cat([coarse_labels.flatten(), medium_labels.flatten(), fine_labels.flatten()], dim=0)\n","    \n","    def __len__(self):\n","        return len(self.dataset)\n","    \n","    def __getitem__(self, idx):\n","        item = self.dataset[idx]\n","        img = self.transform(item[\"image\"])\n","        label = self._parse_label(item[\"ocr_info\"])\n","        return img, label\n","    \n","    def __iter__(self):\n","        self.index = 0\n","        return self\n","\n","    def __next__(self):\n","        if self.index >= len(self):\n","            raise StopIteration\n","        item = self[self.index]\n","        self.index += 1\n","        return item\n","    \n","    def get_batch(self, batch_size, randomized=True):\n","        # returns pairs of (batch_size, 3, 224, 224) image with (batch_size, 18522) label\n","        # 18522 = (7*7*3*6) + (14*14*3*6) + (28*28*3*6)\n","        if randomized:\n","            indices = np.random.choice(len(self), batch_size, replace=False)\n","        else:\n","            indices = np.arange(self.batch_count, self.batch_count + batch_size) % len(self)\n","            self.batch_count += batch_size\n","        indices = [int(i) for i in indices]\n","        batch_images = torch.stack([self[i][0] for i in indices], dim=0)\n","        batch_labels = torch.stack([self[i][1] for i in indices], dim=0)\n","        return batch_images, batch_labels\n","\n","    @staticmethod\n","    def collate_fn(batch):\n","        # collate_fn purpose is to convert list of (image, label) pairs into a single tensor\n","        # Gets used internally by DataLoader to prepare batches for training\n","        images, labels = zip(*batch)\n","        images = torch.stack(images, dim=0)\n","        labels = torch.stack(labels, dim=0)\n","        return images, labels"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Length:\t\t3074\n","Input Shape:\ttorch.Size([3, 224, 224])\n","Output Shape:\ttorch.Size([18522])\n","Input Batch Shape:\t torch.Size([2, 3, 224, 224])\n","Output Batch Shape:\t torch.Size([2, 18522])\n"]}],"source":["# Small little sanity check\n","test_dataset = COCOText(split=\"validation\")\n","# Expect (3, 224, 224) input and (18522,) label tensors\n","print(f'Length:\\t\\t{len(test_dataset)}\\nInput Shape:\\t{test_dataset[0][0].shape}\\nOutput Shape:\\t{test_dataset[0][1].shape}')\n","\n","test_batch = test_dataset.get_batch(2, randomized=False)\n","# Expect (2, 3, 224, 224) input and (2, 18522) label batch tensors\n","print(f'Input Batch Shape:\\t {test_batch[0].shape}\\nOutput Batch Shape:\\t {test_batch[1].shape}')"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Loss"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["class YoLoss(nn.Module):\n","    \"\"\"\n","    YOLOv3 Custom Loss Function.\n","    \"\"\"\n","    def __init__(self, num_classes=num_classes, num_anchors=num_anchors, lambda_coord=1.0, lambda_obj=3.0, lambda_noobj=0.1,\n","                 lambda_class=2.0, iou_threshold=0.5, focal_alpha=0.5, focal_gamma=1.0, label_smoothing=0.1, anchors=anchors):\n","        super(YoLoss, self).__init__()\n","        self.num_classes = num_classes      # Amount of associable classes\n","        self.num_anchors = num_anchors      # Count of distinctly predictable objects per grid tile\n","        self.lambda_coord = lambda_coord    # BBox Coord Loss Weight\n","        self.lambda_obj = lambda_obj        # Objectness Loss Weight\n","        self.lambda_noobj = lambda_noobj    # Non-Objectness Loss Weight\n","        self.lambda_class = lambda_class    # Classification Loss Weight\n","        self.iou_threshold = iou_threshold  # Intersection over Union Threshold\n","        self.focal_alpha = focal_alpha      # Focal Loss weight of the positive class\n","        self.focal_gamma = focal_gamma      # Focal Loss weight of the negative class\n","        self.label_smoothing = label_smoothing  # Percentage of noise to add to the labels\n","        self.anchors = torch.tensor(anchors)    # Anchor boxes\n","        self.eps = 1e-7                     # Imprecision Avoidance Factor\n","        self.bce_loss = nn.BCEWithLogitsLoss(reduction='none')\n","    \n","    def focal_loss(self, pred, target):\n","        pred_prob = torch.sigmoid(pred)\n","        p_t = target * pred_prob + (1 - target) * (1 - pred_prob)\n","        alpha_factor = target * self.focal_alpha + (1 - target) * (1 - self.focal_alpha)\n","        modulating_factor = (1.0 - p_t).pow(self.focal_gamma)\n","        loss = self.bce_loss(pred, target)\n","        weight = torch.where(target == 1, torch.tensor(60.0).to(pred.device), torch.tensor(1.0).to(pred.device))\n","        return weight * (alpha_factor * modulating_factor * loss)\n","\n","    def bbox_iou(self, box1, box2, xywh=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-7):\n","        if xywh:\n","            (x1, y1, w1, h1), (x2, y2, w2, h2) = box1.chunk(4, -1), box2.chunk(4, -1)\n","            w1_, h1_, w2_, h2_ = w1 / 2, h1 / 2, w2 / 2, h2 / 2\n","            b1_x1, b1_x2, b1_y1, b1_y2 = x1 - w1_, x1 + w1_, y1 - h1_, y1 + h1_\n","            b2_x1, b2_x2, b2_y1, b2_y2 = x2 - w2_, x2 + w2_, y2 - h2_, y2 + h2_\n","        else:\n","            b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n","            b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n","            w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1\n","            w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1\n","        # Area of Bbox Intersection\n","        inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n","                (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n","        # Union Area\n","        union = (w1 * h1 + w2 * h2 - inter) + eps\n","        iou = inter / union\n","        if GIoU or DIoU or CIoU:\n","            cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)\n","            ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)\n","            if CIoU or DIoU:\n","                c2 = (cw ** 2 + ch ** 2) + eps\n","                rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4\n","                if DIoU:\n","                    return iou - rho2 / c2\n","                elif CIoU:\n","                    v = (4 / math.pi ** 2) * torch.pow(torch.atan(w2 / (h2 + eps)) - torch.atan(w1 / (h1 + eps)), 2)\n","                    with torch.no_grad():\n","                        alpha = v / (v - iou + (1 + eps))\n","                    return iou - (rho2 / c2 + v * alpha)\n","            else:\n","                c_area = cw * ch + eps\n","                return iou - (c_area - union) / c_area\n","        else:\n","            return iou\n","\n","    def get_box_loss(self, predictions, targets):\n","        box_loss = 0\n","        for pi, ti in zip(predictions, targets):\n","            mask = ti[..., 4] > 0\n","            p_boxes, t_boxes = pi[mask][..., :4], ti[mask][..., :4]\n","            if p_boxes.numel() > 0:\n","                iou = self.bbox_iou(p_boxes, t_boxes, CIoU=True)\n","                box_loss += torch.mean(1.0 - iou)\n","                box_loss += F.smooth_l1_loss(p_boxes, t_boxes, reduction='mean')\n","        return box_loss\n","\n","    def get_obj_loss(self, predictions, targets):\n","        obj_loss = 0.0\n","        noobj_loss = 0.0\n","        for pi, ti in zip(predictions, targets):\n","            pred_obj = pi[..., 4]\n","            target_obj = ti[..., 4]\n","            obj_loss += torch.mean(self.focal_loss(pred_obj, target_obj))\n","            noobj_mask = target_obj == 0\n","            if noobj_mask.any():\n","                noobj_loss += torch.mean(self.bce_loss(pred_obj[noobj_mask], target_obj[noobj_mask]))\n","        return self.lambda_obj * obj_loss + self.lambda_noobj * noobj_loss\n","\n","    def get_cls_loss(self, predictions, targets):\n","        cls_loss = 0\n","        if self.num_classes > 1:\n","            for pi, ti in zip(predictions, targets):\n","                pred_cls = pi[..., 5:]\n","                target_cls = ti[..., 5:]\n","                target_cls = (1 - self.label_smoothing) * target_cls + self.label_smoothing / self.num_classes\n","                cls_loss += torch.mean(self.focal_loss(pred_cls, target_cls))\n","        return cls_loss\n","\n","    def forward(self, predictions, targets):\n","        b_size = targets.size(0) # Batch size can't be expected to be static\n","        coarse_size = 7 * 7 * self.num_anchors * (5 + self.num_classes)\n","        medium_size = 14 * 14 * self.num_anchors * (5 + self.num_classes)\n","        fine_size = 28 * 28 * self.num_anchors * (5 + self.num_classes)\n","\n","        flat_coarse, flat_medium, flat_fine = torch.split(targets, [coarse_size, medium_size, fine_size], dim=1)\n","        t_coarse = flat_coarse.view(b_size, 7, 7, self.num_anchors, (5 + self.num_classes))\n","        t_medium = flat_medium.view(b_size, 14, 14, self.num_anchors, (5 + self.num_classes))\n","        t_fine = flat_fine.view(b_size, 28, 28, self.num_anchors, (5 + self.num_classes))\n","\n","        targets_split = [t_coarse, t_medium, t_fine]\n","        \n","        box_loss = self.get_box_loss(predictions, targets_split)\n","        obj_loss = self.get_obj_loss(predictions, targets_split)\n","        cls_loss = self.get_cls_loss(predictions, targets_split)\n","                \n","        total_loss = self.lambda_coord * box_loss * obj_loss + self.lambda_class * cls_loss\n","\n","        if torch.isnan(total_loss):\n","            # Debugging purposes (I saw all of these go NaN at some point, fun times)\n","            print(f'box_loss={box_loss}, obj_loss={obj_loss}, cls_loss={cls_loss}')\n","            total_loss = torch.where(torch.isnan(total_loss), torch.zeros_like(total_loss), total_loss)\n","\n","        return total_loss"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","### Data + Loss Sanity Check"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["train_dataset = COCOText(split=\"train\")\n","sanity_criterion = YoLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sanity_batch = 5\n","imgb, labelb = train_dataset.get_batch(sanity_batch)\n","\n","print('Image Batch:', imgb.shape, '\\tLabel:', labelb.shape)\n","print('Image:', imgb[0].shape, '\\tLabel:', labelb[0].shape)\n","\n","def get_loss(label_a, label_b, title='', grid_sizes=[(7, 7), (14, 14), (28, 28)]):\n","    coarse_size = grid_sizes[0][0] * grid_sizes[0][1] * num_anchors * (5 + num_classes)\n","    medium_size = grid_sizes[1][0] * grid_sizes[1][1] * num_anchors * (5 + num_classes)\n","    fine_size = grid_sizes[2][0] * grid_sizes[2][1] * num_anchors * (5 + num_classes)\n","    coarse_flat, medium_flat, fine_flat = torch.split(label_a, [coarse_size, medium_size, fine_size], dim=0)\n","    coarse = coarse_flat.view(grid_sizes[0][0], grid_sizes[0][1], num_anchors, (5 + num_classes))\n","    medium = medium_flat.view(grid_sizes[1][0], grid_sizes[1][1], num_anchors, (5 + num_classes))\n","    fine = fine_flat.view(grid_sizes[2][0], grid_sizes[2][1], num_anchors, (5 + num_classes))\n","    predictions = [coarse.unsqueeze(0), medium.unsqueeze(0), fine.unsqueeze(0)]\n","    print(title, sanity_criterion(predictions, label_b.unsqueeze(0)), '\\n', '-' * 50)\n","\n","def calculate_iou(box1, box2):\n","    x1 = max(box1[0], box2[0])\n","    y1 = max(box1[1], box2[1])\n","    x2 = min(box1[0] + box1[2], box2[0] + box2[2])\n","    y2 = min(box1[1] + box1[3], box2[1] + box2[3])\n","    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n","    box1_area, box2_area = box1[2] * box1[3], box2[2] * box2[3]\n","    union_area = box1_area + box2_area - inter_area\n","    return inter_area / union_area if union_area > 0 else 0\n","\n","def show_image_with_bboxes(img, label, num_anchors=3, num_classes=1, grid_sizes=[(7, 7), (14, 14), (28, 28)], anchors=anchors):\n","    img_np = TF.to_pil_image(img)\n","    _, ax = plt.subplots(1)\n","    ax.imshow(img_np)\n","    ax.set_xticks([])\n","    ax.set_yticks([])\n","    coarse_size = grid_sizes[0][0] * grid_sizes[0][1] * num_anchors * (5 + num_classes)\n","    medium_size = grid_sizes[1][0] * grid_sizes[1][1] * num_anchors * (5 + num_classes)\n","    fine_size = grid_sizes[2][0] * grid_sizes[2][1] * num_anchors * (5 + num_classes)\n","    coarse_flat, medium_flat, fine_flat = torch.split(label, [coarse_size, medium_size, fine_size], dim=0)\n","    coarse = coarse_flat.view(grid_sizes[0][0], grid_sizes[0][1], num_anchors, (5 + num_classes))\n","    medium = medium_flat.view(grid_sizes[1][0], grid_sizes[1][1], num_anchors, (5 + num_classes))\n","    fine = fine_flat.view(grid_sizes[2][0], grid_sizes[2][1], num_anchors, (5 + num_classes))\n","    def draw_boxes(grid, grid_w, grid_h, anchors):\n","        for y in range(grid_h):\n","            for x in range(grid_w):\n","                best_iou, best_anchor_idx = 0, -1\n","                for a in range(num_anchors):\n","                    box = grid[y, x, a]\n","                    if box[4] > 0:\n","                        anchor_w, anchor_h = anchors[a]\n","                        w = box[2].item() * anchor_w * img_np.width\n","                        h = box[3].item() * anchor_h * img_np.height\n","                        cx = (box[0].item() + x) / grid_w * img_np.width\n","                        cy = (box[1].item() + y) / grid_h * img_np.height\n","                        normalized_box = torch.tensor([cx - w / 2, cy - h / 2, w, h])\n","                        iou = calculate_iou(normalized_box.numpy(), [cx - w / 2, cy - h / 2, w, h])\n","                        if iou > best_iou:\n","                            best_iou = iou\n","                            best_anchor_idx = a\n","                if best_anchor_idx != -1:\n","                    best_box = grid[y, x, best_anchor_idx]\n","                    anchor_w, anchor_h = anchors[best_anchor_idx]\n","                    w_best = best_box[2].item() * anchor_w * img_np.width\n","                    h_best = best_box[3].item() * anchor_h * img_np.height\n","                    cx_best = (best_box[0].item() + x) / grid_w * img_np.width\n","                    cy_best = (best_box[1].item() + y) / grid_h * img_np.height\n","                    rect = patches.Rectangle((cx_best - w_best / 2, cy_best - h_best / 2), w_best, h_best,\n","                                             linewidth=1, edgecolor='g', facecolor='none')\n","                    ax.add_patch(rect)\n","    draw_boxes(coarse, grid_sizes[0][0], grid_sizes[0][1], anchors)\n","    draw_boxes(medium, grid_sizes[1][0], grid_sizes[1][1], anchors)\n","    draw_boxes(fine, grid_sizes[2][0], grid_sizes[2][1], anchors)\n","    plt.show()\n","\n","for i in range(imgb.shape[0]):\n","    show_image_with_bboxes(imgb[i], labelb[i])\n","    get_loss(labelb[i], labelb[i], title='Self Loss:')\n","    another_idx = random.choice([num for num in range(sanity_batch) if num != i])\n","    get_loss(labelb[i], labelb[another_idx], title='Random Comparison')"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","# Model\n"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Training"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.182684Z","iopub.status.busy":"2024-10-03T21:10:24.182334Z","iopub.status.idle":"2024-10-03T21:10:35.238599Z","shell.execute_reply":"2024-10-03T21:10:35.237680Z","shell.execute_reply.started":"2024-10-03T21:10:24.182648Z"},"trusted":true},"outputs":[],"source":["train_dataset = ICDAR2015(train_path, train_labels, num_classes)\n","train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n","                           num_workers=num_workers, collate_fn=ICDAR2015.collate_fn,\n","                           pin_memory=True)\n","\n","val_dataset = ICDAR2015(test_path, test_labels, num_classes)\n","val_loader  = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n","                         num_workers=num_workers, collate_fn=ICDAR2015.collate_fn,\n","                         pin_memory=True)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def evaluate(model, criterion, data_loader, device):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():\n","        for images, targets in data_loader:\n","            images = images.to(device, non_blocking=True)\n","            targets = targets.to(device, non_blocking=True)\n","            with torch.amp.autocast(device_type=str(device)):\n","                outputs = model(images)\n","                loss = criterion(outputs, targets)\n","            total_loss += loss.item()\n","    return total_loss / len(data_loader)\n","\n","def mixup_data(x, y, alpha=0.1):\n","    # Intentionally mess up image-to-label relationship for alpha percentage of batch entries\n","    # Helps to avoid overfitting (when not overdone)\n","    if alpha > 0:\n","        lam = np.random.beta(alpha, alpha)\n","        batch_size = x.size(0)\n","        index = torch.randperm(batch_size).to(x.device)\n","        mixed_x = lam * x + (1 - lam) * x[index, :]\n","        y_a, y_b = y, y[index]\n","        return mixed_x, y_a, y_b, lam\n","    else:\n","        return x, y, y, 1"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:35.240698Z","iopub.status.busy":"2024-10-03T21:10:35.240174Z","iopub.status.idle":"2024-10-03T21:10:35.530099Z","shell.execute_reply":"2024-10-03T21:10:35.529042Z","shell.execute_reply.started":"2024-10-03T21:10:35.240641Z"},"trusted":true},"outputs":[],"source":["model = MobileYOLOv3(num_classes=num_classes, dropout_rate=dropout_rate, anchors=torch.tensor(anchors, dtype=torch.float32, device=device)).to(device)\n","criterion = YoLoss()\n","\n","# Adam with decoupled weight decay from gradient update\n","base_optimizer = torch.optim.AdamW([\n","    {'params': model.backbone.parameters(), 'lr': learning_rate * 0.8, 'weight_decay': weight_decay * 0.6},\n","    {'params': model.conv_7.parameters()},\n","    {'params': model.eca_7.parameters()},\n","    {'params': model.det1.parameters()},\n","    {'params': model.r_1024_128.parameters()},\n","    {'params': model.r_48_128.parameters()},\n","    {'params': model.conv_14.parameters()},\n","    {'params': model.eca_14.parameters()},\n","    {'params': model.det2.parameters()},\n","    {'params': model.r_512_64.parameters()},\n","    {'params': model.r_24_64.parameters()},\n","    {'params': model.conv_28.parameters()},\n","    {'params': model.eca_28.parameters()},\n","    {'params': model.det3.parameters()},\n","], lr=learning_rate, weight_decay=weight_decay)\n","\n","# Switching between providing 'fast weights' and 'slow weights' for AdamW optimizer update calculations\n","optimizer = optim.Lookahead(base_optimizer, k=optim_k, alpha=optim_alpha)\n","\n","# Gradually warm and then cool down LR over time\n","scheduler = OneCycleLR(optimizer, max_lr=learning_rate*2, epochs=num_epochs, steps_per_epoch=len(train_loader),\n","                       pct_start=0.3, anneal_strategy='cos', div_factor=10.0, final_div_factor=10000.0)\n","\n","# Helps avoiding numerical underflow/overflow through assigning mixed-precision by necessity rather than default\n","scaler = torch.amp.GradScaler(enabled=(str(device) != 'cpu'))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:35.532217Z","iopub.status.busy":"2024-10-03T21:10:35.531686Z","iopub.status.idle":"2024-10-03T21:22:36.998857Z","shell.execute_reply":"2024-10-03T21:22:36.997495Z","shell.execute_reply.started":"2024-10-03T21:10:35.532134Z"},"trusted":true},"outputs":[],"source":["# 800 epochs, ~200 Minutes on 1x3060\n","\n","lossi, losst = [], []\n","lowsi = float('inf')\n","\n","# Portraying the fine selection of hyperparameters\n","print(f'Batch Size:\\t {batch_size}\\n',\n","      f'Learning Rate:\\t {learning_rate}\\n',\n","      f'Weight Decay:\\t {weight_decay}\\n',\n","      f'Num Epochs:\\t {num_epochs}\\n',\n","      f'LR Warmup:\\t {lr_warmup}\\n',\n","      f'Optim K:\\t {optim_k}\\n',\n","      f'Optim Alpha:\\t {optim_alpha}\\n',\n","      f'Prune Amount:\\t {prune_amount}\\n',\n","      f'Target Architecture:\\t {t_arch}\\n',\n","      f'Num Anchors:\\t {num_anchors}\\n',\n","      f'Dropout Rate:\\t {dropout_rate}\\n\\n')\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_loss = 0\n","    \n","    for files, targets in train_loader:\n","        files = files.to(device, non_blocking=True)\n","        targets = targets.to(device, non_blocking=True)\n","        mixed_files, targets_a, targets_b, lam = mixup_data(files, targets)\n","        optimizer.zero_grad()\n","        \n","        with torch.amp.autocast(device_type=str(device)):\n","            logits = model(mixed_files)\n","            loss_a = criterion(logits, targets_a)\n","            loss_b = criterion(logits, targets_b)\n","            loss = lam * loss_a + (1 - lam) * loss_b # Mixup Loss Calculation\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","        epoch_loss += loss.item()\n","        \n","    epoch_loss /= len(train_loader)\n","    lossi.append(epoch_loss)\n","    t_loss = evaluate(model, criterion, val_loader, device)\n","    losst.append(t_loss)\n","\n","    if 'cuda' == str(device):\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","    \n","    print(f'Epoch [{epoch+1:3}/{num_epochs}] | Train: {epoch_loss:8.6f} | Test: {t_loss:8.6f} | LR: {optimizer.param_groups[-1][\"lr\"]:.6f}')"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:22:37.001075Z","iopub.status.busy":"2024-10-03T21:22:37.000644Z","iopub.status.idle":"2024-10-03T21:22:37.315232Z","shell.execute_reply":"2024-10-03T21:22:37.314338Z","shell.execute_reply.started":"2024-10-03T21:22:37.001034Z"},"trusted":true},"outputs":[],"source":["# Saving the unaltered model\n","torch.save(model.state_dict(), f'solid_{model_path}')"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Loss Graph"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:22:37.316866Z","iopub.status.busy":"2024-10-03T21:22:37.316534Z","iopub.status.idle":"2024-10-03T21:22:37.695052Z","shell.execute_reply":"2024-10-03T21:22:37.693981Z","shell.execute_reply.started":"2024-10-03T21:22:37.316831Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(range(num_epochs), lossi, label=\"Training Loss\", color='blue', marker='o', linestyle='-', markersize=3)\n","plt.plot(range(num_epochs), losst, label=\"Test Loss\", color=\"red\", marker='o', linestyle='-', markersize=3)\n","\n","plt.title('Loss Curves', fontsize=16)\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.grid(True)\n","plt.xticks(range(0, num_epochs, 50))\n","plt.legend(loc='upper right')\n","plt.show();"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Pruning & Quantization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def prune_model(model, amount):\n","    def prune_conv(conv, amount):\n","        prune.ln_structured(conv, name='weight', amount=amount, n=2, dim=0)\n","\n","    for _, module in model.named_modules():\n","        if isinstance(module, DSConv):\n","            prune_conv(module.depthwise, amount)\n","            prune_conv(module.pointwise, amount)\n","        elif isinstance(module, Resizer):\n","            if isinstance(module.conv, DSConv):\n","                prune_conv(module.conv.depthwise, amount)\n","                prune_conv(module.conv.pointwise, amount)\n","            else:\n","                prune_conv(module.conv, amount)\n","        elif isinstance(module, nn.Conv2d):\n","            prune_conv(module, amount)\n","\n","    parameters_to_prune = []\n","    for module in model.modules():\n","        if isinstance(module, nn.Conv2d):\n","            parameters_to_prune.append((module, 'weight'))\n","        elif isinstance(module, DSConv):\n","            parameters_to_prune.extend([(module.depthwise, 'weight'), (module.pointwise, 'weight')])\n","        elif isinstance(module, nn.Linear):\n","            parameters_to_prune.append((module, 'weight'))\n","    prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=amount)\n","    return model\n","\n","def quantize_model(model, device):\n","    \"\"\"\n","    Crunch numeric precision of weights and activations.\n","    \"\"\"\n","    model = model.cpu() # Works only on CPU\n","    quantized_model = torch.quantization.quantize_dynamic(model, {nn.Conv2d, nn.Linear}, dtype=torch.qint8, inplace=True) # inplace=True avoids deepcopy issues\n","    return quantized_model.to(device)\n","\n","def lift_prune_mask(model):\n","    for module in model.modules():\n","        if isinstance(module, (nn.Conv2d, nn.Linear)):\n","            if hasattr(module, 'weight_mask'):\n","                module.weight.data *= module.weight_mask\n","                prune.remove(module, 'weight')\n","            elif hasattr(module, 'weight_orig'):\n","                # If there's a weight_orig but no mask, it means pruning was applied but the mask was removed\n","                module.weight.data = module.weight_orig.data\n","                delattr(module, 'weight_orig')\n","        elif isinstance(module, DSConv):\n","            for conv in [module.depthwise, module.pointwise]:\n","                if hasattr(conv, 'weight_mask'):\n","                    conv.weight.data *= conv.weight_mask\n","                    prune.remove(conv, 'weight')\n","                elif hasattr(conv, 'weight_orig'):\n","                    conv.weight.data = conv.weight_orig.data\n","                    delattr(conv, 'weight_orig')\n","        elif isinstance(module, Resizer):\n","            if isinstance(module.conv, DSConv):\n","                for conv in [module.conv.depthwise, module.conv.pointwise]:\n","                    if hasattr(conv, 'weight_mask'):\n","                        conv.weight.data *= conv.weight_mask\n","                        prune.remove(conv, 'weight')\n","                    elif hasattr(conv, 'weight_orig'):\n","                        conv.weight.data = conv.weight_orig.data\n","                        delattr(conv, 'weight_orig')\n","            else:\n","                if hasattr(module.conv, 'weight_mask'):\n","                    module.conv.weight.data *= module.conv.weight_mask\n","                    prune.remove(module.conv, 'weight')\n","                elif hasattr(module.conv, 'weight_orig'):\n","                    module.conv.weight.data = module.conv.weight_orig.data\n","                    delattr(module.conv, 'weight_orig')\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Prune, Quantize\n","pruned_model = prune_model(model, amount=prune_amount)\n","lifted_model = lift_prune_mask(pruned_model)\n","quantized_model = quantize_model(lifted_model, device)\n","\n","# Save the quantized model\n","torch.save(quantized_model.state_dict(), model_path)"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Inference"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:30:41.780508Z","iopub.status.busy":"2024-10-03T21:30:41.779478Z","iopub.status.idle":"2024-10-03T21:30:41.790096Z","shell.execute_reply":"2024-10-03T21:30:41.788973Z","shell.execute_reply.started":"2024-10-03T21:30:41.780462Z"},"trusted":true},"outputs":[],"source":["def load_model(model_class, num_classes, model_path, device='cpu'):\n","    \"\"\"\n","    Load a PyTorch model for inference on the target device, regardless of where it was originally trained.\n","    \"\"\"\n","    if isinstance(device, str):\n","        device = torch.device(device)\n","\n","    # Load to CPU first\n","    state_dict = torch.load(model_path, map_location=device, weights_only=False)\n","    \n","    if isinstance(state_dict, dict) and 'model_state_dict' in state_dict:\n","        state_dict = state_dict['model_state_dict']\n","\n","    # Remove 'module.' prefix caused by SWA\n","    state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n","\n","    # Remove pruning-related keys\n","    new_state_dict = {}\n","    for key, value in state_dict.items():\n","        if 'weight_mask' not in key:\n","            new_key = key.replace('weight_orig', 'weight')\n","            new_state_dict[new_key] = value\n","    \n","    model = model_class(num_classes=num_classes, dropout_rate=dropout_rate, anchors=torch.tensor(anchors, dtype=torch.float32, device=device)).to(device)\n","    model.load_state_dict(new_state_dict, strict=False)\n","    model.eval()\n","    return model"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["model = load_model(MobileYOLOv3, num_classes, model_path, device='cuda')"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["def visualize_inference(model, dataset, num_anchors=3, num_classes=1, grid_sizes=[(7, 7), (14, 14), (28, 28)], anchors=anchors):\n","    img, _ = dataset[random.randint(0, len(dataset) - 1)]\n","    prediction = model(img.unsqueeze(0).to(device))\n","    prediction = [p.squeeze(0) for p in prediction]\n","    _, ax = plt.subplots(1)\n","    img_np = TF.to_pil_image(img)\n","    ax.imshow(img_np)    \n","    ax.set_xticks([])\n","    ax.set_yticks([])\n","\n","    def draw_boxes(grid, grid_w, grid_h, anchors):\n","        for y in range(grid_h):\n","            for x in range(grid_w):\n","                best_iou, best_anchor_idx = 0, -1\n","                for a in range(num_anchors):\n","                    box = grid[y, x, a]\n","                    if box[4] > 0.9:\n","                        anchor_w, anchor_h = anchors[a]\n","                        w = box[2].item() * anchor_w * img_np.width  # Width\n","                        h = box[3].item() * anchor_h * img_np.height # Height\n","                        cx = (box[0].item() + x) / grid_w * img_np.width   # Center x\n","                        cy = (box[1].item() + y) / grid_h * img_np.height  # Center y\n","                        # Create a normalized bounding box for IoU calculation\n","                        normalized_box = torch.tensor([cx - w / 2, cy - h / 2, w, h])\n","                        # Calculate IoU with the current anchor\n","                        iou = calculate_iou(normalized_box.numpy(), [cx - w / 2, cy - h / 2, w, h])\n","                        # Update best IoU and corresponding anchor index\n","                        if iou > best_iou:\n","                            best_iou = iou\n","                            best_anchor_idx = a\n","                if best_anchor_idx != -1:\n","                    # Draw the bounding box with the best anchor\n","                    best_box = grid[y, x, best_anchor_idx]\n","                    anchor_w, anchor_h = anchors[best_anchor_idx]\n","                    w_best = best_box[2].item() * anchor_w * img_np.width  # Width using best anchor\n","                    h_best = best_box[3].item() * anchor_h * img_np.height # Height using best anchor\n","                    cx_best = (best_box[0].item() + x) / grid_w * img_np.width   # Center x using best anchor\n","                    cy_best = (best_box[1].item() + y) / grid_h * img_np.height  # Center y using best anchor\n","                    rect = patches.Rectangle((cx_best - w_best / 2, cy_best - h_best / 2), w_best, h_best,\n","                                             linewidth=1, edgecolor='g', facecolor='none')\n","                    ax.add_patch(rect)\n","\n","    draw_boxes(prediction[0], grid_sizes[0][0], grid_sizes[0][1], anchors)\n","    draw_boxes(prediction[1], grid_sizes[1][0], grid_sizes[1][1], anchors)\n","    draw_boxes(prediction[2], grid_sizes[2][0], grid_sizes[2][1], anchors)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["visualize_inference(model, train_dataset)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1150310,"sourceId":1928836,"sourceType":"datasetVersion"}],"dockerImageVersionId":30775,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"ai","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
