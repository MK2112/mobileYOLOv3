{"cells":[{"cell_type":"markdown","metadata":{},"source":["# MobileNetV3 YOLOv3 for Text Detection\n","\n","A text detection model based on MobileNetV3 and YOLOv3.<br>\n","Pruned and quantized for deployment on edge devices.\n","\n","- [x] Pretrained MobileNetV2 backbone\n","- [x] YOLOv3 top end\n","- [x] Basic Pruning, Quantization integration\n","- [x] Training pipeline (for ICDAR 2015)\n","- [x] Switch backbone to MobileNetV3\n","- [x] Mixed Precision Training\n","- [x] Advanced Pruning and quantization\n","\n","- [ ] Basic Inference\n","- [ ] Performance Evaluation\n","- [ ] Deflate Jupyter Notebook into file structure\n","- [ ] Advanced training pipeline (COCO-Text dataset, batch augmentation, etc.)\n","- [ ] Live Image-Feed Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:07.786952Z","iopub.status.busy":"2024-10-03T21:10:07.786546Z","iopub.status.idle":"2024-10-03T21:10:21.048258Z","shell.execute_reply":"2024-10-03T21:10:21.047060Z","shell.execute_reply.started":"2024-10-03T21:10:07.786910Z"},"trusted":true},"outputs":[],"source":["!pip install torch-optimizer"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:21.050641Z","iopub.status.busy":"2024-10-03T21:10:21.050117Z","iopub.status.idle":"2024-10-03T21:10:24.052955Z","shell.execute_reply":"2024-10-03T21:10:24.052036Z","shell.execute_reply.started":"2024-10-03T21:10:21.050583Z"},"trusted":true},"outputs":[],"source":["import gc\n","import csv\n","import torch\n","import random\n","import numpy as np\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import torch_optimizer as optim\n","import torchvision.models as models\n","import matplotlib.patches as patches\n","import torch.nn.utils.prune as prune\n","\n","from PIL import Image\n","from pathlib import Path\n","from torchvision import transforms\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.swa_utils import AveragedModel, SWALR\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.054910Z","iopub.status.busy":"2024-10-03T21:10:24.054406Z","iopub.status.idle":"2024-10-03T21:10:24.094925Z","shell.execute_reply":"2024-10-03T21:10:24.093802Z","shell.execute_reply.started":"2024-10-03T21:10:24.054872Z"},"trusted":true},"outputs":[],"source":["torch.manual_seed(42)\n","torch.cuda.manual_seed_all(42);\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using training device: {device}\")\n","\n","batch_size = 32\n","num_workers = 4\n","num_classes = 1\n","learning_rate = 1e-4\n","num_epochs = 50\n","warmup_epochs = 12\n","weight_decay = 2e-4\n","swa_kickoff = 0.75\n","optim_k = 5\n","optim_alpha = 0.5\n","warmup_start = 0.3\n","scheduler_t0 = 2\n","scheduler_tmult = 2\n","swa_lr = 1e-3\n","swa_anneal_sched = 5\n","prune_amount = 0.2\n","smoothing_factor = 0.05\n","target_architecture = 'cuda'\n","dropout_rate = 0.2\n","mixup_alpha = 0.3\n","\n","# https://www.kaggle.com/datasets/bestofbests9/icdar2015\n","dataset_path = Path(\"./icdar2015/\")\n","train_path = dataset_path / 'ch4_training_images'\n","train_labels = dataset_path / 'ch4_training_localization_transcription_gt'\n","test_path = dataset_path / 'ch4_test_images'\n","test_labels = dataset_path / 'ch4_test_localization_transcription_gt'\n","\n","model_path = 'pq_mobileyolov3.pth'"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Model Definition"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.098325Z","iopub.status.busy":"2024-10-03T21:10:24.097934Z","iopub.status.idle":"2024-10-03T21:10:24.108350Z","shell.execute_reply":"2024-10-03T21:10:24.107213Z","shell.execute_reply.started":"2024-10-03T21:10:24.098286Z"},"trusted":true},"outputs":[],"source":["class DSConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n","        super(DSConv, self).__init__()\n","        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, \n","                                   stride=stride, padding=kernel_size//2, groups=in_channels, bias=False)\n","        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.activation = nn.LeakyReLU(0.1)\n","\n","    def forward(self, x):\n","        x = self.depthwise(x)\n","        x = self.pointwise(x)\n","        x = self.bn(x)\n","        return self.activation(x)\n","\n","class MobileYOLOv3(nn.Module):\n","    \"\"\"\n","    In:  (batch_size, 3, 224, 224)\n","    Out: [(batch_size, 7, 7, num_anchors * (5 + num_classes)),\n","          (batch_size, 14, 14, num_anchors * (5 + num_classes))]\n","    5 + num_classes = 4 (x,y,w,h) + 1 (objectness) + num_classes (class probabilities)\n","    \"\"\"\n","    def __init__(self, num_classes=1, num_anchors=3):\n","        super(MobileYOLOv3, self).__init__()\n","        self.num_classes = num_classes\n","        self.num_anchors = num_anchors\n","\n","        self.mobilenet = nn.Sequential(*list(models.mobilenet_v3_small(weights='IMAGENET1K_V1').features))\n","        self.conv2 = DSConv(576, 192, kernel_size=1)\n","        self.conv3 = DSConv(192, 384, kernel_size=3)\n","        self.det1 = nn.Conv2d(384, num_anchors * (5 + num_classes), kernel_size=1)\n","        \n","        self.conv4 = DSConv(384, 128, kernel_size=1)\n","        self.upsample = nn.ConvTranspose2d(128, 128, kernel_size=2, stride=2)\n","        self.conv5 = DSConv(128 + 48, 128, kernel_size=3)\n","        self.det2 = nn.Conv2d(128, num_anchors * (5 + num_classes), kernel_size=1)\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, x):\n","        skip_out = None\n","        for i, layer in enumerate(self.mobilenet):\n","            x = layer(x)\n","            # Using 8th MobileNetV3 layer as skip connection\n","            if i == 8:\n","                skip_out = x # (batch_size, 48, 14, 14)\n","\n","        # 'Coarse' detection\n","        x = self.conv2(x)   # (batch_size, 256, 7, 7)\n","        x = self.conv3(x)   # (batch_size, 512, 7, 7)\n","        det1 = self.det1(self.dropout(x)).permute(0, 2, 3, 1).contiguous() # (batch_size, 7, 7, num_anchors * (5 + num_classes))\n","\n","        # Skip connection\n","        x = self.conv4(x)     # (batch_size, 128, 7, 7)\n","        x = self.upsample(x)  # (batch_size, 128, 14, 14)\n","        x = torch.cat([x, skip_out], dim=1)  # (batch_size, 128 + 48, 14, 14)\n","\n","        # 'Fine' detection\n","        x = self.conv5(x)  # (batch_size, 128, 14, 14)\n","        det2 = self.det2(self.dropout(x)).permute(0, 2, 3, 1).contiguous() # (batch_size, 14, 14, num_anchors * (5 + num_classes))\n","\n","        # [(batch_size, 7, 7, num_anchors * (5 + num_classes)),\n","        #  (batch_size, 14, 14, num_anchors * (5 + num_classes))]\n","        return [det1, det2]"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Pruning & Quantization Definition"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.110103Z","iopub.status.busy":"2024-10-03T21:10:24.109729Z","iopub.status.idle":"2024-10-03T21:10:24.125642Z","shell.execute_reply":"2024-10-03T21:10:24.124621Z","shell.execute_reply.started":"2024-10-03T21:10:24.110066Z"},"trusted":true},"outputs":[],"source":["def prune_model(model, amount=0.2):\n","    \"\"\"\n","    Weed out amount% weakest model weights.\n","    \"\"\"\n","    for _, module in model.named_modules():\n","        if isinstance(module, DSConv):\n","            # Depthwise convolution pruning\n","            prune.l1_unstructured(module.depthwise, name='weight', amount=amount)\n","            prune.ln_structured(module.depthwise, name='weight', amount=amount, n=2, dim=0)\n","            prune.remove(module.depthwise, 'weight')            \n","            # Pointwise convolution pruning\n","            prune.l1_unstructured(module.pointwise, name='weight', amount=amount)\n","            prune.ln_structured(module.pointwise, name='weight', amount=amount, n=2, dim=0)\n","            prune.remove(module.pointwise, 'weight')\n","    parameters_to_prune = [(module, 'weight') for module in model.modules() if isinstance(module, (nn.Conv2d, nn.Linear))]\n","    prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=amount / 2)\n","    return model\n","\n","def quantize_model(model, device):\n","    \"\"\"\n","    Reduce numeric precision of weights and activations.\n","    \"\"\"\n","    model = model.cpu()\n","    quantized_model = torch.quantization.quantize_dynamic(model, {nn.Conv2d, nn.Linear}, dtype=torch.qint8, inplace=True)\n","    # inplace=True avoids deepcopy issues\n","    return quantized_model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.127518Z","iopub.status.busy":"2024-10-03T21:10:24.127141Z","iopub.status.idle":"2024-10-03T21:10:24.162452Z","shell.execute_reply":"2024-10-03T21:10:24.161267Z","shell.execute_reply.started":"2024-10-03T21:10:24.127481Z"},"trusted":true},"outputs":[],"source":["class ICDAR2015(Dataset):\n","    \"\"\"\n","    ICDAR2015 Dataset for YOLOv3 training.\n","    \"\"\"\n","    def __init__(self, input_path, label_path, num_classes=1, num_anchors=3, img_size=(224, 224), img_format='.jpg', anchors=None):\n","        self.input_path = Path(input_path)  # Path to images\n","        self.label_path = Path(label_path)  # Path to labels\n","        self.num_classes = num_classes      # Number of associable classes\n","        self.num_anchors = num_anchors      # Number of predictable distinct objects per grid tile\n","        self.img_size = img_size            # Image size\n","        self.batch_count = 0                # Batch counter\n","        self.anchors = anchors if anchors is not None else [(0.28, 0.35), (0.43, 0.58), (0.62, 0.78)]\n","        # Encounter same image multiple times, different augmentations each time\n","        self.transform = transforms.Compose([\n","            transforms.Resize(img_size),\n","            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","            transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n","            transforms.ToTensor()\n","        ])\n","        self.files = self._assemble_files(img_format=img_format)\n","        self.labels = [self._parse_label(label, Image.open(img).size) for img, label in self.files]\n","\n","    def _assemble_files(self, img_format):\n","        image_files, data = list(self.input_path.glob(f'*{img_format}')), []\n","        for img_file in image_files:\n","            img_id = img_file.stem.split('_')[-1]\n","            label_file = self.label_path / f\"gt_img_{img_id}.txt\"\n","            if label_file.exists():\n","                data.append((img_file, label_file))\n","            else:\n","                print(f\"Warning: No matching label file found for {img_file.name}\")\n","        return data\n","\n","    def __len__(self):\n","        return len(self.files)\n","    \n","    def __getitem__(self, idx):\n","        img_path, _ = self.files[idx]\n","        img = Image.open(img_path)\n","        img = self.transform(img)\n","        label = self.labels[idx]\n","        return img, label\n","    \n","    def __iter__(self):\n","        self.index = 0\n","        return self\n","\n","    def __next__(self):\n","        if self.index >= len(self):\n","            raise StopIteration\n","        item = self[self.index]\n","        self.index += 1\n","        return item\n","    \n","    def _to_grid(self, grid, box, grid_dim):\n","        grid_w, grid_h = grid_dim\n","        x, y, w, h, obj, _ = box\n","        # Grid cell coordinates\n","        grid_x, grid_y = int(x * grid_w), int(y * grid_h)\n","        # Convert to relative coordinates in grid\n","        x, y = x * grid_w - grid_x, y * grid_h - grid_y\n","        for anchor_idx, (anchor_w, anchor_h) in enumerate(self.anchors):\n","            anchor_slice = slice(anchor_idx * (5 + self.num_classes), (anchor_idx + 1) * (5 + self.num_classes))\n","            if grid[grid_x, grid_y, anchor_slice].sum() == 0: # Empty grid cell\n","                grid[grid_x, grid_y, anchor_slice][:4] = torch.tensor([x, y, w / anchor_w, h / anchor_h])\n","                grid[grid_x, grid_y, anchor_slice][4] = obj # Objectness\n","                break  # Assign to one anchor only\n","\n","    def _parse_label(self, label_path, img_size):\n","        coarse_labels = torch.zeros((7, 7, self.num_anchors * (5 + self.num_classes)))\n","        fine_labels   = torch.zeros((14, 14, self.num_anchors * (5 + self.num_classes)))\n","        with open(label_path, 'r', encoding='utf-8-sig') as file:\n","            reader = csv.reader(file, delimiter=',')\n","            for row in reader:\n","                row = torch.tensor([float(i) for i in row[:8]])   # BBox Points\n","                x, y = (row[0::2].sum() / 4, row[1::2].sum() / 4) # Center Point\n","                w = row[0::2].max() - row[0::2].min()   # BBox Width\n","                h = row[1::2].max() - row[1::2].min()   # BBox Height\n","                # Ground Truth Labels might be logically overreaching\n","                x, y = min(x, img_size[0] - 1e-3), min(y, img_size[1] - 1e-3)\n","                w, h = min(w, img_size[0] - 1e-3), min(h, img_size[1] - 1e-3)\n","                # Normalize\n","                x, y = x / img_size[0], y / img_size[1]\n","                w, h = w / img_size[0], h / img_size[1]\n","                # Objectness, Class Probability (ignore for single-class usecases)\n","                obj = 1.0\n","                cls = torch.zeros(self.num_classes) if self.num_classes > 1 else 0.0\n","                # 4 (x,y,w,h) + 1 (objectness) + num_classes (class probabilities)\n","                box = torch.tensor([x, y, w, h, obj, cls] if self.num_classes == 1 else [x, y, w, h, obj] + cls.tolist())\n","                # Assign in coarse grid\n","                self._to_grid(coarse_labels, box, (7, 7))\n","                # Assign in fine grid\n","                self._to_grid(fine_labels, box, (14, 14))\n","        # Would've loved to separate the labels more distinctly, but GPU memory rules this out\n","        cat = torch.cat([coarse_labels.flatten(), fine_labels.flatten()], dim=0) # (4410)\n","        return cat\n","\n","    def get_batch(self, batch_size, randomized=True):\n","        if randomized:\n","            indices = np.random.choice(len(self), batch_size, replace=False)\n","        else:\n","            indices = np.arange(self.batch_count, self.batch_count + batch_size) % len(self)\n","            self.batch_count += batch_size\n","        batch_images = torch.stack([self[i][0] for i in indices], dim=0) # Images\n","        batch_labels = torch.stack([self[i][1] for i in indices], dim=0) # Labels\n","        # (batch_size, 3, 224, 224), (batch_size, flat((7, 7, num_anchors * (5 + num_classes)) + (14, 14, num_anchors * (5 + num_classes))))\n","        return batch_images, batch_labels\n","\n","    @staticmethod\n","    def collate_fn(batch):\n","        images, labels = zip(*batch)\n","        images = torch.stack(images, dim=0)\n","        labels = torch.stack(labels, dim=0)\n","        # (batch_size, 3, 224, 224), (batch_size, flat((7, 7, num_anchors * (5 + num_classes)), (14, 14, num_anchors * (5 + num_classes))))\n","        return images, labels"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Loss"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.164779Z","iopub.status.busy":"2024-10-03T21:10:24.164286Z","iopub.status.idle":"2024-10-03T21:10:24.181085Z","shell.execute_reply":"2024-10-03T21:10:24.180180Z","shell.execute_reply.started":"2024-10-03T21:10:24.164720Z"},"trusted":true},"outputs":[],"source":["class YoLoss(nn.Module):\n","    \"\"\"\n","    Custom YOLOv3-based loss function.\n","    Returns a scalar loss value, normalized over batch size, based on:\n","    - Coordinate loss for boxes with IoU > threshold and IoU < threshold\n","    - Objectness loss\n","    - No-objectness loss\n","    - Class loss (only for objects)\n","    \"\"\"\n","    def __init__(self, num_classes=1, num_anchors=3, lambda_coord=10, lambda_noobj=0.5, \n","                 lambda_class=1, iou_threshold=0.5, focal_alpha=0.25, focal_gamma=2.0):\n","        super(YoLoss, self).__init__()\n","        self.num_classes = num_classes      # Expecting this many different classes\n","        self.num_anchors = num_anchors      # Amount of boxes each grid cell is capable of predicting\n","        self.lambda_coord = lambda_coord    # Penalty factor, coordinate misalignment\n","        self.lambda_noobj = lambda_noobj    # Another penalty factor, objectness misidentification\n","        self.lambda_class = lambda_class    # Final penalty factor, class misattribution\n","        self.iou_threshold = iou_threshold  # If IoU > threshold -> objectness = 1 -> Spacial box attribution guide\n","        self.focal_alpha = focal_alpha\n","        self.focal_gamma = focal_gamma\n","\n","    def focal_loss(self, pred, target, alpha, gamma):\n","        BCE_loss = F.binary_cross_entropy_with_logits(pred, target, reduction='none')\n","        pt = torch.exp(-BCE_loss)\n","        return alpha * (1 - pt) ** gamma * BCE_loss\n","        \n","    def compute_loss(self, predictions, targets):\n","        \"\"\"\n","        predictions, targets: (batch_size, grid_size_x, grid_size_y, num_anchors * (5 + num_classes))\n","        out -> scalar loss value\n","        \"\"\"\n","        b_size, grid_size_x, grid_size_y, _ = predictions.size()\n","\n","        # Mend to (batch_size, grid_size_x, grid_size_y, num_anchors, 5 + num_classes)\n","        # Thankfully this stripes the tensor into the desired shape (num_anchors many breaks in the last dimension)\n","        predictions = predictions.view(b_size, grid_size_x, grid_size_y, self.num_anchors, 5 + self.num_classes)\n","        targets     = targets.view(b_size, grid_size_x, grid_size_y, self.num_anchors, 5 + self.num_classes)\n","\n","        # Split into components\n","        p_xy    = predictions[..., :2]\n","        p_wh    = predictions[..., 2:4]\n","        p_obj   = predictions[..., 4]\n","        p_class = predictions[..., 5:]\n","\n","        t_xy    = targets[..., :2]\n","        t_wh    = targets[..., 2:4]\n","        t_obj   = targets[..., 4]\n","        t_class = targets[..., 5:]\n","\n","        # Coordinate Loss (Center + Width-Height Deviation) for cells containing an object\n","        coord_loss = self.lambda_coord * t_obj * (\n","            F.mse_loss(p_xy, t_xy, reduction='none').sum(-1) + \n","            F.mse_loss(p_wh, t_wh, reduction='none').sum(-1)\n","        )\n","        \n","        # Penalize incorrect objectness predictions + non-objectness predictions\n","        obj_loss = self.focal_loss(p_obj, t_obj, self.focal_alpha, self.focal_gamma)\n","        no_obj_loss = self.lambda_noobj * (1 - t_obj) * obj_loss\n","\n","        # 4. Class Loss with Focal Loss (if applicable)\n","        if self.num_classes > 1:\n","            # Penalize incorrect class predictions\n","            class_loss = self.lambda_class * t_obj * self.focal_loss(p_class, t_class, self.focal_alpha, self.focal_gamma).sum(-1)\n","        else:\n","            # No class-related loss calculation if only one class anyway\n","            class_loss = torch.zeros_like(coord_loss)\n","\n","        # Total loss: sum of all components, divided by batch size\n","        total_loss = (coord_loss + obj_loss + no_obj_loss + class_loss).sum() / b_size\n","\n","        return total_loss\n","    \n","    def forward(self, predictions, targets):\n","        b_size = predictions[0].shape[0]\n","        coarse_size = 7 * 7 * self.num_anchors * (5 + self.num_classes)\n","        fine_size = 14 * 14 * self.num_anchors * (5 + self.num_classes)\n","        flat_coarse, flat_fine = torch.split(targets, [coarse_size, fine_size], dim=1)\n","        t_coarse = flat_coarse.view(b_size, 7, 7, self.num_anchors * (5 + self.num_classes))\n","        t_fine = flat_fine.view(b_size, 14, 14, self.num_anchors * (5 + self.num_classes))\n","        p_coarse, p_fine = predictions\n","        return self.compute_loss(p_coarse, t_coarse) + self.compute_loss(p_fine, t_fine)"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Training"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:24.182684Z","iopub.status.busy":"2024-10-03T21:10:24.182334Z","iopub.status.idle":"2024-10-03T21:10:35.238599Z","shell.execute_reply":"2024-10-03T21:10:35.237680Z","shell.execute_reply.started":"2024-10-03T21:10:24.182648Z"},"trusted":true},"outputs":[],"source":["train_dataset = ICDAR2015(train_path, train_labels, num_classes)\n","train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n","                           num_workers=num_workers, collate_fn=ICDAR2015.collate_fn,\n","                           pin_memory=True)\n","\n","val_dataset = ICDAR2015(test_path, test_labels, num_classes)\n","val_loader  = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n","                         num_workers=num_workers, collate_fn=ICDAR2015.collate_fn,\n","                         pin_memory=True)\n","\n","def evaluate(model, criterion, data_loader, device):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():\n","        for images, targets in data_loader:\n","            images = images.to(device, non_blocking=True)\n","            targets = targets.to(device, non_blocking=True)\n","            with torch.amp.autocast(device_type=str(device)):\n","                outputs = model(images)\n","                loss = criterion(outputs, targets)\n","            total_loss += loss.item()\n","    return total_loss / len(data_loader)\n","\n","def adaptive_gradient_clipping(model, clip_factor=0.01, eps=1e-3):\n","    for param in model.parameters():\n","        param_norm = torch.norm(param.grad)\n","        clip_value = clip_factor * (torch.norm(param) + eps)\n","        param.grad = param.grad * (clip_value / (param_norm + eps))\n","\n","def mixup_data(x, y, alpha=mixup_alpha):\n","    '''Returns mixed inputs, i.e. messing with the image-label relationship'''\n","    if alpha > 0:\n","        lam = np.random.beta(alpha, alpha)\n","    else:\n","        lam = 1\n","\n","    batch_size = x.size()[0]\n","    index = torch.randperm(batch_size).to(x.device)\n","    mixed_x = lam * x + (1 - lam) * x[index, :]\n","    y_a, y_b = y, y[index]\n","    return mixed_x, y_a, y_b, lam"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:35.240698Z","iopub.status.busy":"2024-10-03T21:10:35.240174Z","iopub.status.idle":"2024-10-03T21:10:35.530099Z","shell.execute_reply":"2024-10-03T21:10:35.529042Z","shell.execute_reply.started":"2024-10-03T21:10:35.240641Z"},"trusted":true},"outputs":[],"source":["model = MobileYOLOv3(num_classes=num_classes).to(device)\n","criterion = YoLoss()\n","\n","# Adam, but decoupling weight decay from the gradient update\n","base_optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","\n","# Periodically look ahead, update weights by averaging weight updates at every k steps\n","optimizer = optim.Lookahead(base_optimizer, k=optim_k, alpha=optim_alpha)\n","\n","# Increase learning rate gradually, prevent harsh updates early on\n","warmup_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=warmup_start, total_iters=warmup_epochs)\n","\n","# Periodically run original learning rate, decaying it following a cosine curve over T_0 epochs\n","scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=scheduler_t0, T_mult=scheduler_tmult)\n","\n","# Avoids numerical underflow/overflow through scaling, helps maintain information in mixed-precision\n","scaler = torch.amp.GradScaler(enabled=(str(device) != 'cpu'))\n","\n","# Running model param average, stabilizes training\n","swa_model = AveragedModel(model)\n","\n","# Learning rate scheduler for SWA phase\n","swa_scheduler = SWALR(optimizer, swa_lr=swa_lr, anneal_strategy=\"cos\", anneal_epochs=swa_anneal_sched)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:10:35.532217Z","iopub.status.busy":"2024-10-03T21:10:35.531686Z","iopub.status.idle":"2024-10-03T21:22:36.998857Z","shell.execute_reply":"2024-10-03T21:22:36.997495Z","shell.execute_reply.started":"2024-10-03T21:10:35.532134Z"},"trusted":true},"outputs":[],"source":["lossi, losst = [], []\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_loss = 0\n","    \n","    for files, targets in train_loader:\n","        files = files.to(device, non_blocking=True)\n","        targets = targets.to(device, non_blocking=True)\n","        mixed_files, targets_a, targets_b, lam = mixup_data(files, targets)\n","        optimizer.zero_grad()\n","        \n","        with torch.amp.autocast(device_type=str(device)):\n","            outputs = model(mixed_files)\n","            loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n","\n","        scaler.scale(loss).backward()\n","        adaptive_gradient_clipping(model)\n","        scaler.step(optimizer)\n","        scaler.update()\n","        epoch_loss += loss.item()\n","        \n","    epoch_loss /= len(train_loader)\n","    lossi.append(epoch_loss)\n","    \n","    t_loss = evaluate(model, criterion, val_loader, device)\n","    losst.append(t_loss)\n","\n","    # SWA starts averaging model weights\n","    if epoch > num_epochs * swa_kickoff:\n","        swa_model.update_parameters(model)\n","        swa_scheduler.step()\n","    else:\n","        scheduler.step()\n","        \n","    # GPUs are expensive\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    # Print training and test loss\n","    print(f'Epoch [{epoch+1:3}/{num_epochs}] | Train: {epoch_loss:8.3f} | Test: {t_loss:8.3f} | LR: {optimizer.param_groups[0][\"lr\"]:.6f}')"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:22:37.001075Z","iopub.status.busy":"2024-10-03T21:22:37.000644Z","iopub.status.idle":"2024-10-03T21:22:37.315232Z","shell.execute_reply":"2024-10-03T21:22:37.314338Z","shell.execute_reply.started":"2024-10-03T21:22:37.001034Z"},"trusted":true},"outputs":[],"source":["# Use the 'swa_model' from here on\n","torch.optim.swa_utils.update_bn(train_loader, swa_model, device=device)\n","\n","# Prune, Quantize\n","pruned_model = prune_model(swa_model, amount=prune_amount)\n","quantized_model = quantize_model(pruned_model, device)\n","\n","# Save the quantized model\n","torch.save(quantized_model.state_dict(), model_path)"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:22:37.316866Z","iopub.status.busy":"2024-10-03T21:22:37.316534Z","iopub.status.idle":"2024-10-03T21:22:37.695052Z","shell.execute_reply":"2024-10-03T21:22:37.693981Z","shell.execute_reply.started":"2024-10-03T21:22:37.316831Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(range(num_epochs), lossi, label=\"Training Loss\", color='blue', marker='o', linestyle='-', markersize=3)\n","plt.plot(range(num_epochs), losst, label=\"Test Loss\", color=\"red\", marker='o', linestyle='-', markersize=3)\n","\n","plt.title('Training + Test Loss Over Epochs', fontsize=16)\n","plt.xlabel('Epochs', fontsize=14)\n","plt.ylabel('Loss', fontsize=14)\n","plt.grid(True)\n","plt.legend(loc='upper right')\n","plt.show();"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:30:41.780508Z","iopub.status.busy":"2024-10-03T21:30:41.779478Z","iopub.status.idle":"2024-10-03T21:30:41.790096Z","shell.execute_reply":"2024-10-03T21:30:41.788973Z","shell.execute_reply.started":"2024-10-03T21:30:41.780462Z"},"trusted":true},"outputs":[],"source":["def load_model(model_class, num_classes, model_path, target_device='cpu'):\n","    \"\"\"\n","    Load a PyTorch model for inference on the target device, regardless of where it was originally trained.\n","    \"\"\"\n","    if isinstance(target_device, str):\n","        target_device = torch.device(target_device)\n","\n","    # Load to CPU first\n","    state_dict = torch.load(model_path, map_location=target_device, weights_only=False)\n","    \n","    if isinstance(state_dict, dict) and 'model_state_dict' in state_dict:\n","        state_dict = state_dict['model_state_dict']\n","\n","    # Remove 'module.' prefix caused by SWA\n","    state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n","\n","    # Remove pruning-related keys\n","    new_state_dict = {}\n","    for key, value in state_dict.items():\n","        if 'weight_mask' not in key:\n","            new_key = key.replace('weight_orig', 'weight')\n","            new_state_dict[new_key] = value\n","        \n","    model = model_class(num_classes)\n","    model.load_state_dict(new_state_dict, strict=False)\n","    model = model.to(target_device)\n","    model.eval()\n","    return model"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def to_edge_coords(predictions, original_sizes, conf_threshold=0.5, num_anchors=3):\n","    batch_size = predictions[0].shape[0]\n","    bboxes_per_image = []\n","    for img_idx in range(batch_size):\n","        boxes = []\n","        orig_w, orig_h = original_sizes[img_idx]\n","        for pred_idx, pred in enumerate(predictions):\n","            pred_img = pred[img_idx]\n","            grid_h, grid_w = (7, 7) if pred_idx == 0 else (14, 14)\n","            for y in range(grid_h):\n","                for x in range(grid_w):\n","                    for anchor in range(num_anchors):\n","                        start_idx = anchor * (5 + 1)  # Assuming num_classes is 1\n","                        obj_conf = pred_img[y, x, start_idx + 4].sigmoid()\n","                        if obj_conf > conf_threshold:\n","                            box = pred_img[y, x, start_idx:start_idx + 4].sigmoid()\n","                            cx, cy, w, h = box\n","                            # Convert to absolute coordinates\n","                            cx_abs = (x + cx) / grid_w * orig_w\n","                            cy_abs = (y + cy) / grid_h * orig_h\n","                            w_abs = w * orig_w\n","                            h_abs = h * orig_h\n","                            # Calculate corner coordinates\n","                            x_min = cx_abs - w_abs / 2\n","                            y_min = cy_abs - h_abs / 2\n","                            x_max = cx_abs + w_abs / 2\n","                            y_max = cy_abs + h_abs / 2\n","                            # Convert to four-point bounding box\n","                            box = [\n","                                x_min, y_min,  # Top-left\n","                                x_max, y_min,  # Top-right\n","                                x_max, y_max,  # Bottom-right\n","                                x_min, y_max   # Bottom-left\n","                            ]\n","                            boxes.append(box)\n","        bboxes_per_image.append(boxes)\n","    return bboxes_per_image\n","\n","def process_batch(model, dataset, device, num_images=10, conf_threshold=0.5):\n","    model.eval()\n","    sampled_indices = random.sample(range(len(dataset)), num_images)\n","    original_imgs, original_sizes, batch_imgs = [], [], []\n","    for idx in sampled_indices:\n","        img, _ = dataset[idx]\n","        original_size = img.shape[2], img.shape[1]  # (width, height)\n","        original_imgs.append(img.permute(1, 2, 0).cpu().numpy())  # Store as HWC numpy array\n","        original_sizes.append(original_size)\n","        batch_imgs.append(img)\n","    batch_tensor = torch.stack(batch_imgs).to(device)\n","    with torch.no_grad():\n","        predictions = model(batch_tensor)\n","        predictions = [pred.cpu() for pred in predictions]\n","        # Reshape predictions to match ICDAR2015 format\n","        coarse_pred = predictions[0].view(batch_tensor.shape[0], 7, 7, -1)\n","        fine_pred = predictions[1].view(batch_tensor.shape[0], 14, 14, -1)\n","        predictions = [coarse_pred, fine_pred]\n","    return to_edge_coords(predictions, original_sizes, conf_threshold), original_imgs\n","\n","def visualize_predictions(model, dataset, device, num_images=10, conf_threshold=0.5):\n","    predicted_boxes, original_images = process_batch(model, dataset, device, num_images, conf_threshold)\n","    _, axes = plt.subplots(2, 5, figsize=(20, 8))\n","    axes = axes.flatten()\n","    for img_idx, ax in enumerate(axes):\n","        img = original_images[img_idx]\n","        ax.imshow(img)\n","        boxes = predicted_boxes[img_idx]\n","        for box in boxes:\n","            x1, y1, x2, y2, x3, y3, x4, y4 = box\n","            poly = patches.Polygon(np.array([[x1, y1], [x2, y2], [x3, y3], [x4, y4]]), \n","                                   closed=True, fill=False, edgecolor='red', linewidth=1)\n","            ax.add_patch(poly)\n","        ax.axis('off')\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T21:30:48.674934Z","iopub.status.busy":"2024-10-03T21:30:48.673852Z","iopub.status.idle":"2024-10-03T21:30:58.312034Z","shell.execute_reply":"2024-10-03T21:30:58.310415Z","shell.execute_reply.started":"2024-10-03T21:30:48.674887Z"},"trusted":true},"outputs":[],"source":["model = load_model(MobileYOLOv3, num_classes, model_path, 'cuda')\n","dataset = ICDAR2015(train_path, train_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["visualize_predictions(model, dataset, device, num_images=10, conf_threshold=0.347) # I mean, it's getting there."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1150310,"sourceId":1928836,"sourceType":"datasetVersion"}],"dockerImageVersionId":30775,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"ai","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
