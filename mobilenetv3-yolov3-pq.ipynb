{"cells":[{"cell_type":"markdown","metadata":{},"source":["# MobileNetV3 YOLOv3 for Text Detection\n","\n","A text detection model based on MobileNetV3 and YOLOv3.<br>\n","Pruned and quantized for deployment on edge devices.\n","\n","- [x] Pretrained MobileNetV2 backbone\n","- [x] YOLOv3 top end\n","- [x] Basic Pruning, Quantization integration\n","- [x] Training pipeline (for ICDAR 2015)\n","- [x] Switch backbone to MobileNetV3\n","- [x] Mixed Precision Training\n","- [x] Advanced Pruning and quantization\n","\n","- [ ] Basic Inference\n","- [ ] Performance Evaluation\n","- [ ] Deflate Jupyter Notebook into file structure\n","- [ ] Advanced training pipeline (COCO-Text dataset, batch augmentation, etc.)\n","- [ ] Live Image-Feed Inference"]},{"cell_type":"code","execution_count":14,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-01T13:54:05.139904Z","iopub.status.busy":"2024-10-01T13:54:05.139450Z","iopub.status.idle":"2024-10-01T13:54:11.345824Z","shell.execute_reply":"2024-10-01T13:54:11.344676Z","shell.execute_reply.started":"2024-10-01T13:54:05.139860Z"},"trusted":true},"outputs":[],"source":["import os\n","import csv\n","import torch\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch.nn as nn\n","import torch.nn.init as init\n","import torchvision.ops as ops\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import torchvision.models as models\n","import torch.nn.utils.prune as prune\n","import torch.quantization.quantize_fx as quantize_fx\n","\n","from PIL import Image, ImageDraw\n","from pathlib import Path\n","from torch.nn import functional as F\n","from torchvision import transforms\n","from torch.cuda.amp import autocast\n","from torch.quantization import quantize_dynamic\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T13:54:11.348744Z","iopub.status.busy":"2024-10-01T13:54:11.348005Z","iopub.status.idle":"2024-10-01T13:54:11.365021Z","shell.execute_reply":"2024-10-01T13:54:11.363773Z","shell.execute_reply.started":"2024-10-01T13:54:11.348683Z"},"trusted":true},"outputs":[],"source":["torch.manual_seed(42)\n","torch.cuda.manual_seed_all(42);"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Model Definition"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T13:58:40.452049Z","iopub.status.busy":"2024-10-01T13:58:40.451569Z","iopub.status.idle":"2024-10-01T13:58:40.460404Z","shell.execute_reply":"2024-10-01T13:58:40.459196Z","shell.execute_reply.started":"2024-10-01T13:58:40.452007Z"},"trusted":true},"outputs":[],"source":["class MobileYOLOv3(nn.Module):\n","    \"\"\"\n","    In:  (batch_size, 3, 448, 224)\n","    Out: (batch_size, 7, 7, num_anchors * (5 + num_classes))\n","          5 + num_classes = 4 (x,y,w,h) \n","                            + 1 (objectness) \n","                            + num_classes (class probabilities)\n","    Making for 7x7=49 grid tiles with num_anchors many (5 + num_classes) values each.\n","    YoloV3 originally starts 13x13, but I deviate for a smaller model.\n","    \"\"\"\n","\n","    def __init__(self, num_classes=1, num_anchors=3):\n","        super(MobileYOLOv3, self).__init__()\n","        self.num_classes = num_classes  # 1, but keep this flexible\n","        self.num_anchors = num_anchors  # 3, like the original YOLOv3\n","        self.conv1 = nn.Conv2d(3, 3, kernel_size=3, stride=2, padding=1)\n","        self.mobilenet = models.mobilenet_v3_small(weights='IMAGENET1K_V1').features\n","        self.conv2 = nn.Conv2d(576, num_anchors * (5 + num_classes), kernel_size=1, stride=1, padding=0)\n","\n","    def forward(self, x):           # Input shape: (batch_size, 3, 448, 224)\n","        x = F.relu(self.conv1(x))   # (batch_size, 3, 224, 112)\n","        x = self.mobilenet(x)       # (batch_size, 576, 7, 7)\n","        x = F.relu(self.conv2(x))   # (batch_size, num_anchors * (5 + num_classes), 7, 7)\n","        return x.permute(0, 2, 3, 1).contiguous()   # (batch_size, 7, 7, num_anchors * (5 + num_classes))"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Pruning & Quantization Definition"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T13:51:32.480761Z","iopub.status.busy":"2024-10-01T13:51:32.480432Z","iopub.status.idle":"2024-10-01T13:51:32.492700Z","shell.execute_reply":"2024-10-01T13:51:32.491813Z","shell.execute_reply.started":"2024-10-01T13:51:32.480730Z"},"trusted":true},"outputs":[],"source":["def prune_model(model, amount=0.3):\n","    for _, module in model.named_modules():\n","        if isinstance(module, nn.Conv2d):\n","            prune.l1_unstructured(module, name='weight', amount=amount)\n","            prune.ln_structured(module, name='weight', amount=amount, n=2, dim=0)\n","            prune.remove(module, 'weight')\n","            if module.bias is not None:\n","                prune.l1_unstructured(module, name='bias', amount=amount/2)\n","                prune.remove(module, 'bias')\n","    parameters_to_prune = [(module, 'weight') for module in model.modules() if isinstance(module, (nn.Conv2d, nn.Linear))]\n","    prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=amount/2)\n","    return model\n","\n","def quantize_model(model, device):\n","    model = model.cpu()  # Quantization happens only on CPU\n","    quantized_model = torch.quantization.quantize_dynamic(\n","        model, {nn.Conv2d, nn.Linear}, dtype=torch.qint8, inplace=True  # Use inplace=True against deepcopy\n","    )\n","    return quantized_model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ICDAR2015(Dataset):\n","    \"\"\"\n","    ICDAR2015 Dataset for YOLOv3 training.\n","    Required Inputs: input_path, label_path,\n","    Output: (batch_size, 3, 448, 224), (batch_size, 7, 7, num_anchors * (5 + num_classes))\n","    (5 = 4 (x,y,w,h) + 1 (objectness) + num_classes (class probabilities))\n","    \"\"\"\n","    def __init__(self, input_path, label_path, num_classes=1, num_anchors=3, transform=(448, 224), grid_size=(7, 7), device=None):\n","        self.input_path = Path(input_path)\n","        self.label_path = Path(label_path)\n","        self.num_classes = num_classes\n","        self.num_anchors = num_anchors\n","        self.image_size = transform\n","        self.grid_size = grid_size\n","        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.batch_count = 0\n","        \n","        self.transform = transforms.Compose([\n","            transforms.Resize(transform),\n","            transforms.ToTensor()\n","        ])\n","        \n","        self.data = [(img, label) for img, label in zip(list(self.input_path.glob('*.jpg')), list(self.label_path.glob('*.txt')))]\n","        self.labels = [self._parse_label(label, Image.open(img).size) for img, label in self.data]\n","\n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, idx):\n","        img_path, _ = self.data[idx]\n","        img = Image.open(img_path)\n","        img = self.transform(img).to(self.device)\n","        label = self.labels[idx]\n","        return img, label\n","    \n","    def __iter__(self):\n","        self.index = 0\n","        return self\n","\n","    def __next__(self):\n","        if self.index >= len(self):\n","            raise StopIteration\n","        item = self[self.index]\n","        self.index += 1\n","        return item\n","    \n","    def _parse_label(self, label_path, img_size):\n","        labels = torch.zeros((self.grid_size[0], self.grid_size[1], self.num_anchors * (5 + self.num_classes)), device=self.device)\n","        with open(label_path, 'r') as file:\n","            reader = csv.reader(file, delimiter=',')\n","            for row in reader:\n","                row = torch.tensor([float(i) for i in row[:-1]], device=self.device)\n","                x, y = (row[0::2].sum() / 4, row[1::2].sum() / 4)\n","                w = row[0::2].max() - row[0::2].min()\n","                h = row[1::2].max() - row[1::2].min()\n","                \n","                x = min(x, img_size[0] - 1e-3)\n","                y = min(y, img_size[1] - 1e-3)\n","                w = min(w, img_size[0] - 1e-3)\n","                h = min(h, img_size[1] - 1e-3)\n","\n","                x, y = x / img_size[0] * self.grid_size[0], y / img_size[1] * self.grid_size[1]\n","                \n","                grid_x, grid_y = min(max(int(x), 0), self.grid_size[0] - 1), min(max(int(y), 0), self.grid_size[1] - 1)\n","                x, y = x - grid_x, y - grid_y\n","                \n","                w, h = w / img_size[0], h / img_size[1]\n","\n","                obj = 1.0   # Objectness\n","                cls = 0.0 if self.num_classes == 1 else torch.zeros(self.num_classes, device=self.device) # Class probabilities\n","\n","                # 4 (x,y,w,h) + 1 (objectness) + num_classes (class probabilities)\n","                box = torch.tensor([x, y, w, h, obj, cls] if self.num_classes == 1 else [x, y, w, h, obj] + cls.tolist(), device=self.device)\n","                center_distance = torch.sqrt((box[2] - 0.5)**2 + (box[3] - 0.5)**2).item()\n","                \n","                for anchor in range(self.num_anchors):\n","                    anchor_slice = slice(anchor * (5 + self.num_classes), (anchor + 1) * (5 + self.num_classes))\n","                    if labels[grid_y, grid_x, anchor_slice].sum() == 0:\n","                        labels[grid_y, grid_x, anchor_slice] = box\n","                        break\n","                    elif labels[grid_y, grid_x, anchor * (5 + self.num_classes) + 4] == 1.0:\n","                        c_box = labels[grid_y, grid_x, anchor_slice]\n","                        c_distance = torch.sqrt((c_box[2] - 0.5)**2 + (c_box[3] - 0.5)**2).item()\n","                        if center_distance < c_distance:\n","                            labels[grid_y, grid_x, anchor_slice] = box\n","                            break\n","        return labels\n","\n","    def next_batch(self, batch_size, randomized=True):\n","        if randomized:\n","            indices = np.random.choice(len(self), batch_size, replace=False)\n","        else:\n","            indices = np.arange(self.batch_count, self.batch_count + batch_size) % len(self)\n","            self.batch_count += batch_size\n","        \n","        batch_images = torch.stack([self[i][0] for i in indices]) # Images\n","        batch_labels = torch.stack([self.labels[i] for i in indices]) # Labels\n","        return batch_images.to(self.device), batch_labels.to(self.device)\n","\n","    @staticmethod\n","    def collate_fn(batch):\n","        images, labels = zip(*batch)\n","        images = torch.stack(images)\n","        labels = torch.stack(labels)\n","        return images, labels"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Loss"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class YoLoss(nn.Module):\n","    def __init__(self, num_classes=1, num_anchors=3, lambda_coord=5, lambda_noobj=0.5):\n","        super(YoLoss, self).__init__()\n","        self.num_classes = num_classes\n","        self.num_anchors = num_anchors\n","        self.lambda_coord = lambda_coord\n","        self.lambda_noobj = lambda_noobj\n","\n","    def forward(self, predictions, targets):\n","        batch_size = predictions.size(0)\n","        grid_size = predictions.size(1)\n","\n","        # TODO. Maybe this works?!?\n","        predictions = predictions.view(batch_size, grid_size, grid_size, self.num_anchors, 5 + self.num_classes)\n","        \n","        # Separate components of the predictions\n","        pred_x     = predictions[..., 0]\n","        pred_y     = predictions[..., 1]\n","        pred_w     = predictions[..., 2]\n","        pred_h     = predictions[..., 3]\n","        pred_obj   = predictions[..., 4]\n","        pred_class = predictions[..., 5:]\n","\n","        # Separate components of the targets\n","        target_x     = targets[..., 0]\n","        target_y     = targets[..., 1]\n","        target_w     = targets[..., 2]\n","        target_h     = targets[..., 3]\n","        target_obj   = targets[..., 4]\n","        target_class = targets[..., 5:]\n","\n","        # Create object mask\n","        obj_mask = target_obj.bool()\n","        noobj_mask = ~obj_mask\n","\n","        # Coordinate loss\n","        coord_loss = self.lambda_coord * obj_mask * (\n","            F.mse_loss(pred_x, target_x, reduction='sum') +\n","            F.mse_loss(pred_y, target_y, reduction='sum') +\n","            F.mse_loss(torch.sqrt(pred_w), torch.sqrt(target_w), reduction='sum') +\n","            F.mse_loss(torch.sqrt(pred_h), torch.sqrt(target_h), reduction='sum')\n","        )\n","\n","        # Objectness loss\n","        obj_loss = obj_mask * F.binary_cross_entropy(pred_obj, target_obj, reduction='sum')\n","        noobj_loss = self.lambda_noobj * noobj_mask * F.binary_cross_entropy(pred_obj, target_obj, reduction='sum')\n","\n","        # Class loss (only if num_classes > 1)\n","        if self.num_classes > 1:\n","            class_loss = obj_mask * F.binary_cross_entropy(pred_class, target_class, reduction='sum')\n","        else:\n","            class_loss = 0\n","\n","        # Total loss\n","        total_loss = (coord_loss + obj_loss + noobj_loss + class_loss) / batch_size\n","\n","        return total_loss"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T13:51:32.494149Z","iopub.status.busy":"2024-10-01T13:51:32.493865Z","iopub.status.idle":"2024-10-01T13:51:32.538394Z","shell.execute_reply":"2024-10-01T13:51:32.537484Z","shell.execute_reply.started":"2024-10-01T13:51:32.494119Z"},"trusted":true},"outputs":[],"source":["batch_size = 32\n","num_workers = 4\n","num_classes = 1\n","learning_rate = 1e-3\n","num_epochs = 25\n","target_architecture = 'cuda' # else 'cpu'\n","\n","# https://www.kaggle.com/datasets/bestofbests9/icdar2015\n","dataset_path = Path('/kaggle/input/icdar2015')\n","train_path = dataset_path / 'ch4_training_images'\n","train_labels = dataset_path / 'ch4_training_localization_transcription_gt'\n","test_path = dataset_path / 'ch4_test_images'\n","test_labels = dataset_path / 'ch4_test_localization_transcription_gt'\n","\n","model_path = 'pq_yolov3_mobilenetv3.pth'\n","\n","train_dataset = ICDAR2015(train_path, train_labels, num_classes)\n","test_dataset = ICDAR2015(test_path, test_labels, num_classes)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using training device: {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T13:51:32.539808Z","iopub.status.busy":"2024-10-01T13:51:32.539492Z","iopub.status.idle":"2024-10-01T13:51:36.278165Z","shell.execute_reply":"2024-10-01T13:51:36.276289Z","shell.execute_reply.started":"2024-10-01T13:51:32.539776Z"},"trusted":true},"outputs":[],"source":["model = MobileYOLOv3(num_classes=num_classes).to(device)\n","criterion = YoLoss()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=2e-4)\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n","scaler = torch.amp.GradScaler(str(device))\n","\n","lossi = []\n","losst = []\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_loss = 0\n","    \n","    for batch_idx, (data, targets) in enumerate(train_loader):        \n","        data = data.to(device, non_blocking=True)\n","        targets = targets.to(device, non_blocking=True)\n","\n","        optimizer.zero_grad()\n","\n","        with torch.amp.autocast(device_type=str(device)):\n","            outputs = model(data)\n","            loss = criterion(outputs, targets)\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        epoch_loss += loss.item()\n","        \n","    epoch_loss = epoch_loss / len(train_loader)\n","    lossi.append(epoch_loss)\n","    \n","    t_loss = test_loss(model, test_loader, criterion, device)\n","    losst.append(t_loss)\n","\n","    scheduler.step(epoch_loss)\n","\n","    # Print training and test loss\n","    print(f'Epoch [{epoch+1:3}/{num_epochs}] | Train Loss: {epoch_loss:8.3f} | Test Loss: {t_loss:8.3f} | LR: {optimizer.param_groups[0][\"lr\"]:.6f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-01T13:51:36.279610Z","iopub.status.idle":"2024-10-01T13:51:36.280216Z","shell.execute_reply":"2024-10-01T13:51:36.279975Z","shell.execute_reply.started":"2024-10-01T13:51:36.279948Z"},"trusted":true},"outputs":[],"source":["# Prune, Quantize\n","pruned_model = prune_model(model)\n","quantized_model = quantize_model(pruned_model, device)\n","\n","# Save the quantized model\n","torch.save(quantized_model.state_dict(), model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-01T13:51:36.282198Z","iopub.status.idle":"2024-10-01T13:51:36.282753Z","shell.execute_reply":"2024-10-01T13:51:36.282500Z","shell.execute_reply.started":"2024-10-01T13:51:36.282473Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(range(num_epochs), lossi, label=\"Training Loss\", color='blue', marker='o', linestyle='-', markersize=3)\n","plt.plot(range(num_epochs), losst, label=\"Test Loss\", color=\"red\", marker='o', linestyle='-', markersize=3)\n","\n","plt.title('Training + Test Loss Over Epochs', fontsize=16)\n","plt.xlabel('Epochs', fontsize=14)\n","plt.ylabel('Loss', fontsize=14)\n","plt.grid(True)\n","plt.legend(loc='upper right')\n","\n","plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-01T13:51:36.284261Z","iopub.status.idle":"2024-10-01T13:51:36.284663Z","shell.execute_reply":"2024-10-01T13:51:36.284492Z","shell.execute_reply.started":"2024-10-01T13:51:36.284471Z"},"trusted":true},"outputs":[],"source":["def load_model(model_class, num_classes, model_path, target_device='cpu'):\n","    \"\"\"\n","    Load a PyTorch model for inference on the target device, regardless of where it was originally trained.\n","    \"\"\"\n","    if not os.path.exists(model_path):\n","        raise FileNotFoundError(f\"No file at {model_path}\")\n","\n","    if isinstance(target_device, str):\n","        target_device = torch.device(target_device)\n","\n","    # Load the state dict to CPU first\n","    state_dict = torch.load(model_path, map_location=target_device, weights_only=False)\n","    \n","    # If it's a full checkpoint, extract just the model state dict\n","    if isinstance(state_dict, dict) and 'model_state_dict' in state_dict:\n","        state_dict = state_dict['model_state_dict']\n","\n","    # Remove pruning-related keys\n","    new_state_dict = {}\n","    for key, value in state_dict.items():\n","        if 'weight_mask' not in key:\n","            new_key = key.replace('weight_orig', 'weight')\n","            new_state_dict[new_key] = value\n","        \n","    model = model_class(num_classes)\n","    model.load_state_dict(new_state_dict)\n","    model = model.to(target_device)\n","    model.eval()\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-01T13:51:36.285876Z","iopub.status.idle":"2024-10-01T13:51:36.286238Z","shell.execute_reply":"2024-10-01T13:51:36.286080Z","shell.execute_reply.started":"2024-10-01T13:51:36.286061Z"},"trusted":true},"outputs":[],"source":["def yolo_to_icdar2015(predictions, original_sizes, conf_threshold=0.5, grid_size=(8, 14), input_size=(256, 448)):\n","    batch_size, _, grid_h, grid_w = predictions.shape\n","    predictions = predictions.view(batch_size, 5, -1, grid_h, grid_w).permute(0, 1, 3, 4, 2).contiguous()\n","\n","    input_w, input_h = input_size  # Resized width and height\n","    bboxes_per_image = []\n","\n","    for img_idx in range(batch_size):\n","        boxes = []\n","        orig_w, orig_h = original_sizes[img_idx]  # Get the original size of the current image\n","\n","        # Calculate the scaling factors between the resized input and the original image size\n","        scale_w, scale_h = orig_w / input_w, orig_h / input_h\n","\n","        for i in range(5):  # Iterate over the 5 anchor boxes\n","            for y in range(grid_h):\n","                for x in range(grid_w):\n","                    pred = predictions[img_idx, i, y, x]\n","                    obj_conf = pred[4].sigmoid()  # Objectness confidence\n","                    if obj_conf > conf_threshold:\n","                        # Extract bounding box (cx, cy, w, h)\n","                        cx, cy, w, h = pred[:4].sigmoid()  # Apply sigmoid to bounding box dimensions\n","\n","                        # Convert relative to absolute pixel coordinates (in the resized image's grid space)\n","                        cx_abs = (x + cx) * (input_w / grid_w)  # Center x relative to the resized image\n","                        cy_abs = (y + cy) * (input_h / grid_h)  # Center y relative to the resized image\n","                        w_abs = w * input_w  # Width relative to the resized image\n","                        h_abs = h * input_h  # Height relative to the resized image\n","\n","                        # Calculate corner coordinates from (cx, cy, w, h) in the resized image space\n","                        x_min = cx_abs - w_abs / 2\n","                        y_min = cy_abs - h_abs / 2\n","                        x_max = cx_abs + w_abs / 2\n","                        y_max = cy_abs + h_abs / 2\n","\n","                        # Scale the coordinates back to the original image size\n","                        x_min *= scale_w\n","                        x_max *= scale_w\n","                        y_min *= scale_h\n","                        y_max *= scale_h\n","\n","                        # Convert to four-point bounding box (assuming upright box for simplicity)\n","                        box = [\n","                            x_min, y_min,  # Top-left corner\n","                            x_max, y_min,  # Top-right corner\n","                            x_max, y_max,  # Bottom-right corner\n","                            x_min, y_max   # Bottom-left corner\n","                        ]\n","                        boxes.append(box)\n","        \n","        bboxes_per_image.append(boxes)\n","    return bboxes_per_image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-01T13:51:36.287203Z","iopub.status.idle":"2024-10-01T13:51:36.287585Z","shell.execute_reply":"2024-10-01T13:51:36.287395Z","shell.execute_reply.started":"2024-10-01T13:51:36.287377Z"},"trusted":true},"outputs":[],"source":["def process_batch(model, dataset, device, num_images=10, conf_threshold=0.5):\n","    model.eval()\n","    sampled_indices = random.sample(range(len(dataset)), num_images)\n","    \n","    original_images = []\n","    original_sizes = []  # List to hold original sizes\n","    transformed_images = []\n","\n","    for idx in sampled_indices:\n","        image, _ = dataset[idx]\n","        original_size = (image.shape[2], image.shape[1])  # (width, height)\n","\n","        original_images.append(image)\n","        original_sizes.append(original_size)  # Append original size\n","        transformed_images.append(image.unsqueeze(0))  # Add batch dimension\n","\n","    transformed_images = torch.cat(transformed_images).to(device)\n","\n","    with torch.no_grad():\n","        predictions = model(transformed_images)\n","\n","    return yolo_to_icdar2015(predictions, original_sizes, conf_threshold), original_images"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-01T13:51:36.288474Z","iopub.status.idle":"2024-10-01T13:51:36.288828Z","shell.execute_reply":"2024-10-01T13:51:36.288664Z","shell.execute_reply.started":"2024-10-01T13:51:36.288646Z"},"trusted":true},"outputs":[],"source":["def visualize_predictions(model, dataset, device, num_images=10, conf_threshold=0.5):\n","    predicted_boxes, original_images = process_batch(model, dataset, device, num_images, conf_threshold)\n","\n","    for img_idx in range(num_images):\n","        img = original_images[img_idx]\n","        img_disp = img.permute(1, 2, 0).cpu().numpy()  # Convert to HWC format for display\n","\n","        fig, ax = plt.subplots(1)\n","        ax.imshow(img_disp)\n","        boxes = predicted_boxes[img_idx]\n","        \n","        for box in boxes:\n","            x1, y1, x2, y2, x3, y3, x4, y4 = box\n","            rect = patches.Rectangle((x1, y1), x3 - x1, y3 - y1, linewidth=2, edgecolor='red', facecolor='none')\n","            ax.add_patch(rect)\n","        \n","        plt.axis('off')  # Hide axes\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-01T13:51:36.290222Z","iopub.status.idle":"2024-10-01T13:51:36.290607Z","shell.execute_reply":"2024-10-01T13:51:36.290419Z","shell.execute_reply.started":"2024-10-01T13:51:36.290401Z"},"trusted":true},"outputs":[],"source":["model = load_model(MobileYOLOv3, num_classes, model_path, 'cpu')\n","dataset = ICDAR2015(train_path, train_labels)\n","visualize_predictions(model, dataset, 'cpu', num_images=10, conf_threshold=0.973) # Still horrible"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1150310,"sourceId":1928836,"sourceType":"datasetVersion"}],"dockerImageVersionId":30775,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
